{"results": {"arc_challenge_generate": {"alias": "arc_challenge_generate", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "gsm8k": {"alias": "gsm8k", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.009855951478392721, "exact_match_stderr,flexible-extract": 0.0027210765770416642}, "hellaswag_generate": {"alias": "hellaswag_generate", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0008962358095996813, "exact_match_stderr,flexible-extract": 0.00029862623598599867}, "mmlu_generative": {"exact_match,none": 0.0, "exact_match_stderr,none": 0.0, "alias": "mmlu (generative)"}, "humanities": {"alias": " - humanities"}, "mmlu_formal_logic_generative": {"alias": "  - formal_logic", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_high_school_european_history_generative": {"alias": "  - high_school_european_history", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_high_school_us_history_generative": {"alias": "  - high_school_us_history", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_high_school_world_history_generative": {"alias": "  - high_school_world_history", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_international_law_generative": {"alias": "  - international_law", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_jurisprudence_generative": {"alias": "  - jurisprudence", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_logical_fallacies_generative": {"alias": "  - logical_fallacies", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_moral_disputes_generative": {"alias": "  - moral_disputes", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_moral_scenarios_generative": {"alias": "  - moral_scenarios", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_philosophy_generative": {"alias": "  - philosophy", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_prehistory_generative": {"alias": "  - prehistory", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_professional_law_generative": {"alias": "  - professional_law", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_world_religions_generative": {"alias": "  - world_religions", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "other": {"alias": " - other"}, "mmlu_business_ethics_generative": {"alias": "  - business_ethics", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_clinical_knowledge_generative": {"alias": "  - clinical_knowledge", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_college_medicine_generative": {"alias": "  - college_medicine", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_global_facts_generative": {"alias": "  - global_facts", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_human_aging_generative": {"alias": "  - human_aging", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_management_generative": {"alias": "  - management", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_marketing_generative": {"alias": "  - marketing", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_medical_genetics_generative": {"alias": "  - medical_genetics", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_miscellaneous_generative": {"alias": "  - miscellaneous", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_nutrition_generative": {"alias": "  - nutrition", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_professional_accounting_generative": {"alias": "  - professional_accounting", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_professional_medicine_generative": {"alias": "  - professional_medicine", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_virology_generative": {"alias": "  - virology", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "social sciences": {"alias": " - social sciences"}, "mmlu_econometrics_generative": {"alias": "  - econometrics", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_high_school_geography_generative": {"alias": "  - high_school_geography", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_high_school_government_and_politics_generative": {"alias": "  - high_school_government_and_politics", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_high_school_macroeconomics_generative": {"alias": "  - high_school_macroeconomics", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_high_school_microeconomics_generative": {"alias": "  - high_school_microeconomics", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_high_school_psychology_generative": {"alias": "  - high_school_psychology", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_human_sexuality_generative": {"alias": "  - human_sexuality", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_professional_psychology_generative": {"alias": "  - professional_psychology", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_public_relations_generative": {"alias": "  - public_relations", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_security_studies_generative": {"alias": "  - security_studies", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_sociology_generative": {"alias": "  - sociology", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_us_foreign_policy_generative": {"alias": "  - us_foreign_policy", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "stem": {"alias": " - stem"}, "mmlu_abstract_algebra_generative": {"alias": "  - abstract_algebra", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_anatomy_generative": {"alias": "  - anatomy", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_astronomy_generative": {"alias": "  - astronomy", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_college_biology_generative": {"alias": "  - college_biology", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_college_chemistry_generative": {"alias": "  - college_chemistry", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_college_computer_science_generative": {"alias": "  - college_computer_science", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_college_mathematics_generative": {"alias": "  - college_mathematics", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_college_physics_generative": {"alias": "  - college_physics", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_computer_security_generative": {"alias": "  - computer_security", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_conceptual_physics_generative": {"alias": "  - conceptual_physics", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_electrical_engineering_generative": {"alias": "  - electrical_engineering", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_elementary_mathematics_generative": {"alias": "  - elementary_mathematics", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_high_school_biology_generative": {"alias": "  - high_school_biology", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_high_school_chemistry_generative": {"alias": "  - high_school_chemistry", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_high_school_computer_science_generative": {"alias": "  - high_school_computer_science", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_high_school_mathematics_generative": {"alias": "  - high_school_mathematics", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_high_school_physics_generative": {"alias": "  - high_school_physics", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_high_school_statistics_generative": {"alias": "  - high_school_statistics", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "mmlu_machine_learning_generative": {"alias": "  - machine_learning", "exact_match,none": 0.0, "exact_match_stderr,none": 0.0}, "truthfulqa_generative": {"alias": "truthfulqa_generative", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "winogrande_generate": {"alias": "winogrande_generate", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "xwinograd_generate": {"alias": "xwinograd_generate"}, "xwinograd_generate_en": {"alias": " - xwinograd_generate_en", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "xwinograd_generate_fr": {"alias": " - xwinograd_generate_fr", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "xwinograd_generate_jp": {"alias": " - xwinograd_generate_jp", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "xwinograd_generate_pt": {"alias": " - xwinograd_generate_pt", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "xwinograd_generate_ru": {"alias": " - xwinograd_generate_ru", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "xwinograd_generate_zh": {"alias": " - xwinograd_generate_zh", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}}, "groups": {"mmlu_generative": {"exact_match,none": 0.0, "exact_match_stderr,none": 0.0, "alias": "mmlu (generative)"}, "humanities": {"alias": " - humanities"}, "other": {"alias": " - other"}, "social sciences": {"alias": " - social sciences"}, "stem": {"alias": " - stem"}, "xwinograd_generate": {"alias": "xwinograd_generate"}}, "group_subtasks": {"hellaswag_generate": [], "arc_challenge_generate": [], "truthfulqa_generative": [], "xwinograd_generate": ["xwinograd_generate_en", "xwinograd_generate_fr", "xwinograd_generate_jp", "xwinograd_generate_pt", "xwinograd_generate_ru", "xwinograd_generate_zh"], "winogrande_generate": [], "gsm8k": [], "humanities": ["mmlu_professional_law_generative", "mmlu_philosophy_generative", "mmlu_prehistory_generative", "mmlu_jurisprudence_generative", "mmlu_high_school_us_history_generative", "mmlu_formal_logic_generative", "mmlu_high_school_european_history_generative", "mmlu_moral_scenarios_generative", "mmlu_moral_disputes_generative", "mmlu_international_law_generative", "mmlu_logical_fallacies_generative", "mmlu_high_school_world_history_generative", "mmlu_world_religions_generative"], "social sciences": ["mmlu_security_studies_generative", "mmlu_high_school_macroeconomics_generative", "mmlu_professional_psychology_generative", "mmlu_high_school_psychology_generative", "mmlu_sociology_generative", "mmlu_high_school_microeconomics_generative", "mmlu_high_school_government_and_politics_generative", "mmlu_econometrics_generative", "mmlu_human_sexuality_generative", "mmlu_us_foreign_policy_generative", "mmlu_high_school_geography_generative", "mmlu_public_relations_generative"], "other": ["mmlu_human_aging_generative", "mmlu_marketing_generative", "mmlu_miscellaneous_generative", "mmlu_nutrition_generative", "mmlu_management_generative", "mmlu_professional_medicine_generative", "mmlu_college_medicine_generative", "mmlu_clinical_knowledge_generative", "mmlu_virology_generative", "mmlu_medical_genetics_generative", "mmlu_global_facts_generative", "mmlu_business_ethics_generative", "mmlu_professional_accounting_generative"], "stem": ["mmlu_anatomy_generative", "mmlu_abstract_algebra_generative", "mmlu_high_school_physics_generative", "mmlu_high_school_computer_science_generative", "mmlu_electrical_engineering_generative", "mmlu_astronomy_generative", "mmlu_high_school_statistics_generative", "mmlu_conceptual_physics_generative", "mmlu_high_school_biology_generative", "mmlu_elementary_mathematics_generative", "mmlu_college_biology_generative", "mmlu_college_mathematics_generative", "mmlu_college_computer_science_generative", "mmlu_college_chemistry_generative", "mmlu_machine_learning_generative", "mmlu_computer_security_generative", "mmlu_college_physics_generative", "mmlu_high_school_mathematics_generative", "mmlu_high_school_chemistry_generative"], "mmlu_generative": ["stem", "other", "social sciences", "humanities"]}}