{"results": {"arc_challenge": {"alias": "arc_challenge", "acc,none": 0.24061433447098976, "acc_stderr,none": 0.012491468532390571, "acc_norm,none": 0.26791808873720135, "acc_norm_stderr,none": 0.012942030195136433}, "gsm8k": {"alias": "gsm8k", "exact_match,strict-match": 0.015163002274450341, "exact_match_stderr,strict-match": 0.003366022949726359, "exact_match,flexible-extract": 0.019711902956785442, "exact_match_stderr,flexible-extract": 0.0038289829787357173}, "hellaswag": {"alias": "hellaswag", "acc,none": 0.3855805616411073, "acc_stderr,none": 0.004857374133246891, "acc_norm,none": 0.49083847839075884, "acc_norm_stderr,none": 0.004988943721711195}, "mmlu": {"acc,none": 0.2545933627688364, "acc_stderr,none": 0.0036735518638093714, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.24845908607863976, "acc_stderr,none": 0.006297803332220711, "alias": " - humanities"}, "mmlu_formal_logic": {"alias": "  - formal_logic", "acc,none": 0.3492063492063492, "acc_stderr,none": 0.04263906892795131}, "mmlu_high_school_european_history": {"alias": "  - high_school_european_history", "acc,none": 0.24242424242424243, "acc_stderr,none": 0.03346409881055953}, "mmlu_high_school_us_history": {"alias": "  - high_school_us_history", "acc,none": 0.28431372549019607, "acc_stderr,none": 0.03166009679399812}, "mmlu_high_school_world_history": {"alias": "  - high_school_world_history", "acc,none": 0.28270042194092826, "acc_stderr,none": 0.02931281415395592}, "mmlu_international_law": {"alias": "  - international_law", "acc,none": 0.19834710743801653, "acc_stderr,none": 0.036401182719909456}, "mmlu_jurisprudence": {"alias": "  - jurisprudence", "acc,none": 0.23148148148148148, "acc_stderr,none": 0.04077494709252626}, "mmlu_logical_fallacies": {"alias": "  - logical_fallacies", "acc,none": 0.26380368098159507, "acc_stderr,none": 0.03462419931615624}, "mmlu_moral_disputes": {"alias": "  - moral_disputes", "acc,none": 0.21098265895953758, "acc_stderr,none": 0.021966309947043103}, "mmlu_moral_scenarios": {"alias": "  - moral_scenarios", "acc,none": 0.2424581005586592, "acc_stderr,none": 0.014333522059217892}, "mmlu_philosophy": {"alias": "  - philosophy", "acc,none": 0.24437299035369775, "acc_stderr,none": 0.024406162094668903}, "mmlu_prehistory": {"alias": "  - prehistory", "acc,none": 0.22839506172839505, "acc_stderr,none": 0.023358211840626267}, "mmlu_professional_law": {"alias": "  - professional_law", "acc,none": 0.24837027379400262, "acc_stderr,none": 0.011035212598034487}, "mmlu_world_religions": {"alias": "  - world_religions", "acc,none": 0.27485380116959063, "acc_stderr,none": 0.03424042924691583}, "mmlu_other": {"acc,none": 0.25040231734792406, "acc_stderr,none": 0.007766598626625425, "alias": " - other"}, "mmlu_business_ethics": {"alias": "  - business_ethics", "acc,none": 0.21, "acc_stderr,none": 0.040936018074033256}, "mmlu_clinical_knowledge": {"alias": "  - clinical_knowledge", "acc,none": 0.3018867924528302, "acc_stderr,none": 0.02825420034443865}, "mmlu_college_medicine": {"alias": "  - college_medicine", "acc,none": 0.30057803468208094, "acc_stderr,none": 0.03496101481191179}, "mmlu_global_facts": {"alias": "  - global_facts", "acc,none": 0.33, "acc_stderr,none": 0.047258156262526045}, "mmlu_human_aging": {"alias": "  - human_aging", "acc,none": 0.23318385650224216, "acc_stderr,none": 0.028380391147094727}, "mmlu_management": {"alias": "  - management", "acc,none": 0.23300970873786409, "acc_stderr,none": 0.04185832598928315}, "mmlu_marketing": {"alias": "  - marketing", "acc,none": 0.19230769230769232, "acc_stderr,none": 0.02581923325648375}, "mmlu_medical_genetics": {"alias": "  - medical_genetics", "acc,none": 0.25, "acc_stderr,none": 0.04351941398892446}, "mmlu_miscellaneous": {"alias": "  - miscellaneous", "acc,none": 0.24776500638569604, "acc_stderr,none": 0.015438083080568958}, "mmlu_nutrition": {"alias": "  - nutrition", "acc,none": 0.24836601307189543, "acc_stderr,none": 0.02473998135511359}, "mmlu_professional_accounting": {"alias": "  - professional_accounting", "acc,none": 0.25177304964539005, "acc_stderr,none": 0.0258921511567094}, "mmlu_professional_medicine": {"alias": "  - professional_medicine", "acc,none": 0.2647058823529412, "acc_stderr,none": 0.026799562024887667}, "mmlu_virology": {"alias": "  - virology", "acc,none": 0.19879518072289157, "acc_stderr,none": 0.031069390260789427}, "mmlu_social_sciences": {"acc,none": 0.25999350016249595, "acc_stderr,none": 0.007896478172486033, "alias": " - social sciences"}, "mmlu_econometrics": {"alias": "  - econometrics", "acc,none": 0.21052631578947367, "acc_stderr,none": 0.038351539543994194}, "mmlu_high_school_geography": {"alias": "  - high_school_geography", "acc,none": 0.24242424242424243, "acc_stderr,none": 0.030532892233932026}, "mmlu_high_school_government_and_politics": {"alias": "  - high_school_government_and_politics", "acc,none": 0.29015544041450775, "acc_stderr,none": 0.03275264467791516}, "mmlu_high_school_macroeconomics": {"alias": "  - high_school_macroeconomics", "acc,none": 0.26153846153846155, "acc_stderr,none": 0.02228214120420442}, "mmlu_high_school_microeconomics": {"alias": "  - high_school_microeconomics", "acc,none": 0.2647058823529412, "acc_stderr,none": 0.028657491285071973}, "mmlu_high_school_psychology": {"alias": "  - high_school_psychology", "acc,none": 0.24220183486238533, "acc_stderr,none": 0.01836817630659862}, "mmlu_human_sexuality": {"alias": "  - human_sexuality", "acc,none": 0.25190839694656486, "acc_stderr,none": 0.03807387116306086}, "mmlu_professional_psychology": {"alias": "  - professional_psychology", "acc,none": 0.26633986928104575, "acc_stderr,none": 0.017883188134667206}, "mmlu_public_relations": {"alias": "  - public_relations", "acc,none": 0.2727272727272727, "acc_stderr,none": 0.04265792110940589}, "mmlu_security_studies": {"alias": "  - security_studies", "acc,none": 0.3551020408163265, "acc_stderr,none": 0.030635655150387638}, "mmlu_sociology": {"alias": "  - sociology", "acc,none": 0.21890547263681592, "acc_stderr,none": 0.029239174636647}, "mmlu_us_foreign_policy": {"alias": "  - us_foreign_policy", "acc,none": 0.18, "acc_stderr,none": 0.038612291966536955}, "mmlu_stem": {"acc,none": 0.2626070409134158, "acc_stderr,none": 0.007835044144722636, "alias": " - stem"}, "mmlu_abstract_algebra": {"alias": "  - abstract_algebra", "acc,none": 0.25, "acc_stderr,none": 0.04351941398892446}, "mmlu_anatomy": {"alias": "  - anatomy", "acc,none": 0.2518518518518518, "acc_stderr,none": 0.03749850709174023}, "mmlu_astronomy": {"alias": "  - astronomy", "acc,none": 0.23684210526315788, "acc_stderr,none": 0.034597776068105365}, "mmlu_college_biology": {"alias": "  - college_biology", "acc,none": 0.2361111111111111, "acc_stderr,none": 0.03551446610810826}, "mmlu_college_chemistry": {"alias": "  - college_chemistry", "acc,none": 0.28, "acc_stderr,none": 0.04512608598542128}, "mmlu_college_computer_science": {"alias": "  - college_computer_science", "acc,none": 0.23, "acc_stderr,none": 0.04229525846816508}, "mmlu_college_mathematics": {"alias": "  - college_mathematics", "acc,none": 0.22, "acc_stderr,none": 0.041633319989322695}, "mmlu_college_physics": {"alias": "  - college_physics", "acc,none": 0.22549019607843138, "acc_stderr,none": 0.04158307533083286}, "mmlu_computer_security": {"alias": "  - computer_security", "acc,none": 0.22, "acc_stderr,none": 0.04163331998932268}, "mmlu_conceptual_physics": {"alias": "  - conceptual_physics", "acc,none": 0.2851063829787234, "acc_stderr,none": 0.02951319662553935}, "mmlu_electrical_engineering": {"alias": "  - electrical_engineering", "acc,none": 0.2827586206896552, "acc_stderr,none": 0.03752833958003337}, "mmlu_elementary_mathematics": {"alias": "  - elementary_mathematics", "acc,none": 0.30158730158730157, "acc_stderr,none": 0.023636975996101796}, "mmlu_high_school_biology": {"alias": "  - high_school_biology", "acc,none": 0.29354838709677417, "acc_stderr,none": 0.025906087021319295}, "mmlu_high_school_chemistry": {"alias": "  - high_school_chemistry", "acc,none": 0.23645320197044334, "acc_stderr,none": 0.029896114291733552}, "mmlu_high_school_computer_science": {"alias": "  - high_school_computer_science", "acc,none": 0.27, "acc_stderr,none": 0.04461960433384739}, "mmlu_high_school_mathematics": {"alias": "  - high_school_mathematics", "acc,none": 0.2740740740740741, "acc_stderr,none": 0.027195934804085622}, "mmlu_high_school_physics": {"alias": "  - high_school_physics", "acc,none": 0.2582781456953642, "acc_stderr,none": 0.035737053147634576}, "mmlu_high_school_statistics": {"alias": "  - high_school_statistics", "acc,none": 0.2962962962962963, "acc_stderr,none": 0.031141447823536027}, "mmlu_machine_learning": {"alias": "  - machine_learning", "acc,none": 0.14285714285714285, "acc_stderr,none": 0.033213611069662696}, "truthfulqa_mc1": {"alias": "truthfulqa_mc1", "acc,none": 0.2178702570379437, "acc_stderr,none": 0.014450846714123883}, "winogrande": {"alias": "winogrande", "acc,none": 0.5445935280189423, "acc_stderr,none": 0.013996485037729774}, "xwinograd": {"acc,none": 0.6124971903798606, "acc_stderr,none": 0.007201172073480197, "alias": "xwinograd"}, "xwinograd_en": {"alias": " - xwinograd_en", "acc,none": 0.6898924731182796, "acc_stderr,none": 0.009594642452545135}, "xwinograd_fr": {"alias": " - xwinograd_fr", "acc,none": 0.5301204819277109, "acc_stderr,none": 0.05511548370029596}, "xwinograd_jp": {"alias": " - xwinograd_jp", "acc,none": 0.5067778936392076, "acc_stderr,none": 0.01615278242665904}, "xwinograd_pt": {"alias": " - xwinograd_pt", "acc,none": 0.5133079847908745, "acc_stderr,none": 0.030879159795851496}, "xwinograd_ru": {"alias": " - xwinograd_ru", "acc,none": 0.5238095238095238, "acc_stderr,none": 0.028184622595998444}, "xwinograd_zh": {"alias": " - xwinograd_zh", "acc,none": 0.5773809523809523, "acc_stderr,none": 0.0220252966893008}}, "groups": {"mmlu": {"acc,none": 0.2545933627688364, "acc_stderr,none": 0.0036735518638093714, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.24845908607863976, "acc_stderr,none": 0.006297803332220711, "alias": " - humanities"}, "mmlu_other": {"acc,none": 0.25040231734792406, "acc_stderr,none": 0.007766598626625425, "alias": " - other"}, "mmlu_social_sciences": {"acc,none": 0.25999350016249595, "acc_stderr,none": 0.007896478172486033, "alias": " - social sciences"}, "mmlu_stem": {"acc,none": 0.2626070409134158, "acc_stderr,none": 0.007835044144722636, "alias": " - stem"}, "xwinograd": {"acc,none": 0.6124971903798606, "acc_stderr,none": 0.007201172073480197, "alias": "xwinograd"}}, "group_subtasks": {"mmlu_humanities": ["mmlu_high_school_world_history", "mmlu_high_school_european_history", "mmlu_prehistory", "mmlu_philosophy", "mmlu_high_school_us_history", "mmlu_logical_fallacies", "mmlu_moral_disputes", "mmlu_moral_scenarios", "mmlu_formal_logic", "mmlu_international_law", "mmlu_world_religions", "mmlu_jurisprudence", "mmlu_professional_law"], "mmlu_social_sciences": ["mmlu_security_studies", "mmlu_sociology", "mmlu_high_school_government_and_politics", "mmlu_human_sexuality", "mmlu_econometrics", "mmlu_public_relations", "mmlu_high_school_microeconomics", "mmlu_high_school_psychology", "mmlu_us_foreign_policy", "mmlu_professional_psychology", "mmlu_high_school_geography", "mmlu_high_school_macroeconomics"], "mmlu_other": ["mmlu_nutrition", "mmlu_professional_medicine", "mmlu_global_facts", "mmlu_virology", "mmlu_human_aging", "mmlu_clinical_knowledge", "mmlu_professional_accounting", "mmlu_miscellaneous", "mmlu_college_medicine", "mmlu_marketing", "mmlu_management", "mmlu_medical_genetics", "mmlu_business_ethics"], "mmlu_stem": ["mmlu_college_physics", "mmlu_college_mathematics", "mmlu_college_chemistry", "mmlu_abstract_algebra", "mmlu_high_school_chemistry", "mmlu_anatomy", "mmlu_electrical_engineering", "mmlu_high_school_computer_science", "mmlu_elementary_mathematics", "mmlu_college_biology", "mmlu_conceptual_physics", "mmlu_high_school_physics", "mmlu_computer_security", "mmlu_high_school_statistics", "mmlu_machine_learning", "mmlu_high_school_mathematics", "mmlu_astronomy", "mmlu_college_computer_science", "mmlu_high_school_biology"], "mmlu": ["mmlu_stem", "mmlu_other", "mmlu_social_sciences", "mmlu_humanities"], "hellaswag": [], "xwinograd": ["xwinograd_en", "xwinograd_fr", "xwinograd_jp", "xwinograd_pt", "xwinograd_ru", "xwinograd_zh"], "winogrande": [], "truthfulqa_mc1": [], "arc_challenge": [], "gsm8k": []}}