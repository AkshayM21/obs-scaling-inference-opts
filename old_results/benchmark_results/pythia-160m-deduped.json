{"results": {"arc_challenge": {"alias": "arc_challenge", "acc,none": 0.20051194539249148, "acc_stderr,none": 0.011700318050499382, "acc_norm,none": 0.2440273037542662, "acc_norm_stderr,none": 0.012551447627856259}, "gsm8k": {"alias": "gsm8k", "exact_match,strict-match": 0.003032600454890068, "exact_match_stderr,strict-match": 0.0015145735612245373, "exact_match,flexible-extract": 0.01288855193328279, "exact_match_stderr,flexible-extract": 0.0031069012664996505}, "hellaswag": {"alias": "hellaswag", "acc,none": 0.29067914758016333, "acc_stderr,none": 0.004531477407589642, "acc_norm,none": 0.31318462457677754, "acc_norm_stderr,none": 0.0046284090842187665}, "mmlu": {"acc,none": 0.22931206380857427, "acc_stderr,none": 0.003542461179876229, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.24208289054197663, "acc_stderr,none": 0.00624337585685226, "alias": " - humanities"}, "mmlu_formal_logic": {"alias": "  - formal_logic", "acc,none": 0.2857142857142857, "acc_stderr,none": 0.04040610178208841}, "mmlu_high_school_european_history": {"alias": "  - high_school_european_history", "acc,none": 0.22424242424242424, "acc_stderr,none": 0.03256866661681102}, "mmlu_high_school_us_history": {"alias": "  - high_school_us_history", "acc,none": 0.25, "acc_stderr,none": 0.03039153369274154}, "mmlu_high_school_world_history": {"alias": "  - high_school_world_history", "acc,none": 0.270042194092827, "acc_stderr,none": 0.028900721906293426}, "mmlu_international_law": {"alias": "  - international_law", "acc,none": 0.2396694214876033, "acc_stderr,none": 0.03896878985070417}, "mmlu_jurisprudence": {"alias": "  - jurisprudence", "acc,none": 0.25925925925925924, "acc_stderr,none": 0.04236511258094631}, "mmlu_logical_fallacies": {"alias": "  - logical_fallacies", "acc,none": 0.22085889570552147, "acc_stderr,none": 0.03259177392742177}, "mmlu_moral_disputes": {"alias": "  - moral_disputes", "acc,none": 0.24855491329479767, "acc_stderr,none": 0.023267528432100174}, "mmlu_moral_scenarios": {"alias": "  - moral_scenarios", "acc,none": 0.23798882681564246, "acc_stderr,none": 0.014242630070574885}, "mmlu_philosophy": {"alias": "  - philosophy", "acc,none": 0.1864951768488746, "acc_stderr,none": 0.02212243977248078}, "mmlu_prehistory": {"alias": "  - prehistory", "acc,none": 0.21604938271604937, "acc_stderr,none": 0.02289916291844581}, "mmlu_professional_law": {"alias": "  - professional_law", "acc,none": 0.2457627118644068, "acc_stderr,none": 0.01099615663514269}, "mmlu_world_religions": {"alias": "  - world_religions", "acc,none": 0.3157894736842105, "acc_stderr,none": 0.035650796707083106}, "mmlu_other": {"acc,none": 0.23817186997103315, "acc_stderr,none": 0.007625915052007232, "alias": " - other"}, "mmlu_business_ethics": {"alias": "  - business_ethics", "acc,none": 0.3, "acc_stderr,none": 0.04605661864718381}, "mmlu_clinical_knowledge": {"alias": "  - clinical_knowledge", "acc,none": 0.21132075471698114, "acc_stderr,none": 0.02512576648482785}, "mmlu_college_medicine": {"alias": "  - college_medicine", "acc,none": 0.20809248554913296, "acc_stderr,none": 0.0309528902177499}, "mmlu_global_facts": {"alias": "  - global_facts", "acc,none": 0.18, "acc_stderr,none": 0.03861229196653694}, "mmlu_human_aging": {"alias": "  - human_aging", "acc,none": 0.3094170403587444, "acc_stderr,none": 0.03102441174057221}, "mmlu_management": {"alias": "  - management", "acc,none": 0.17475728155339806, "acc_stderr,none": 0.03760178006026621}, "mmlu_marketing": {"alias": "  - marketing", "acc,none": 0.2905982905982906, "acc_stderr,none": 0.029745048572674043}, "mmlu_medical_genetics": {"alias": "  - medical_genetics", "acc,none": 0.3, "acc_stderr,none": 0.046056618647183814}, "mmlu_miscellaneous": {"alias": "  - miscellaneous", "acc,none": 0.23371647509578544, "acc_stderr,none": 0.015133383278988844}, "mmlu_nutrition": {"alias": "  - nutrition", "acc,none": 0.21895424836601307, "acc_stderr,none": 0.02367908986180772}, "mmlu_professional_accounting": {"alias": "  - professional_accounting", "acc,none": 0.23404255319148937, "acc_stderr,none": 0.025257861359432417}, "mmlu_professional_medicine": {"alias": "  - professional_medicine", "acc,none": 0.19117647058823528, "acc_stderr,none": 0.023886881922440338}, "mmlu_virology": {"alias": "  - virology", "acc,none": 0.28313253012048195, "acc_stderr,none": 0.03507295431370519}, "mmlu_social_sciences": {"acc,none": 0.2170945726356841, "acc_stderr,none": 0.007428659190753481, "alias": " - social sciences"}, "mmlu_econometrics": {"alias": "  - econometrics", "acc,none": 0.22807017543859648, "acc_stderr,none": 0.03947152782669415}, "mmlu_high_school_geography": {"alias": "  - high_school_geography", "acc,none": 0.17676767676767677, "acc_stderr,none": 0.027178752639044915}, "mmlu_high_school_government_and_politics": {"alias": "  - high_school_government_and_politics", "acc,none": 0.19689119170984457, "acc_stderr,none": 0.02869787397186069}, "mmlu_high_school_macroeconomics": {"alias": "  - high_school_macroeconomics", "acc,none": 0.20256410256410257, "acc_stderr,none": 0.020377660970371383}, "mmlu_high_school_microeconomics": {"alias": "  - high_school_microeconomics", "acc,none": 0.21008403361344538, "acc_stderr,none": 0.026461398717471874}, "mmlu_high_school_psychology": {"alias": "  - high_school_psychology", "acc,none": 0.1926605504587156, "acc_stderr,none": 0.016909276884936094}, "mmlu_human_sexuality": {"alias": "  - human_sexuality", "acc,none": 0.25190839694656486, "acc_stderr,none": 0.03807387116306086}, "mmlu_professional_psychology": {"alias": "  - professional_psychology", "acc,none": 0.25163398692810457, "acc_stderr,none": 0.01755581809132227}, "mmlu_public_relations": {"alias": "  - public_relations", "acc,none": 0.21818181818181817, "acc_stderr,none": 0.03955932861795833}, "mmlu_security_studies": {"alias": "  - security_studies", "acc,none": 0.18775510204081633, "acc_stderr,none": 0.02500025603954622}, "mmlu_sociology": {"alias": "  - sociology", "acc,none": 0.24875621890547264, "acc_stderr,none": 0.030567675938916714}, "mmlu_us_foreign_policy": {"alias": "  - us_foreign_policy", "acc,none": 0.28, "acc_stderr,none": 0.04512608598542129}, "mmlu_stem": {"acc,none": 0.2134475103076435, "acc_stderr,none": 0.007284429375156566, "alias": " - stem"}, "mmlu_abstract_algebra": {"alias": "  - abstract_algebra", "acc,none": 0.22, "acc_stderr,none": 0.041633319989322695}, "mmlu_anatomy": {"alias": "  - anatomy", "acc,none": 0.1925925925925926, "acc_stderr,none": 0.03406542058502651}, "mmlu_astronomy": {"alias": "  - astronomy", "acc,none": 0.17763157894736842, "acc_stderr,none": 0.031103182383123387}, "mmlu_college_biology": {"alias": "  - college_biology", "acc,none": 0.2638888888888889, "acc_stderr,none": 0.03685651095897532}, "mmlu_college_chemistry": {"alias": "  - college_chemistry", "acc,none": 0.2, "acc_stderr,none": 0.04020151261036845}, "mmlu_college_computer_science": {"alias": "  - college_computer_science", "acc,none": 0.26, "acc_stderr,none": 0.044084400227680794}, "mmlu_college_mathematics": {"alias": "  - college_mathematics", "acc,none": 0.21, "acc_stderr,none": 0.040936018074033256}, "mmlu_college_physics": {"alias": "  - college_physics", "acc,none": 0.21568627450980393, "acc_stderr,none": 0.04092563958237654}, "mmlu_computer_security": {"alias": "  - computer_security", "acc,none": 0.29, "acc_stderr,none": 0.045604802157206845}, "mmlu_conceptual_physics": {"alias": "  - conceptual_physics", "acc,none": 0.25957446808510637, "acc_stderr,none": 0.028659179374292323}, "mmlu_electrical_engineering": {"alias": "  - electrical_engineering", "acc,none": 0.23448275862068965, "acc_stderr,none": 0.035306258743465914}, "mmlu_elementary_mathematics": {"alias": "  - elementary_mathematics", "acc,none": 0.20899470899470898, "acc_stderr,none": 0.02094048156533485}, "mmlu_high_school_biology": {"alias": "  - high_school_biology", "acc,none": 0.1774193548387097, "acc_stderr,none": 0.021732540689329272}, "mmlu_high_school_chemistry": {"alias": "  - high_school_chemistry", "acc,none": 0.1625615763546798, "acc_stderr,none": 0.02596030006460557}, "mmlu_high_school_computer_science": {"alias": "  - high_school_computer_science", "acc,none": 0.25, "acc_stderr,none": 0.04351941398892446}, "mmlu_high_school_mathematics": {"alias": "  - high_school_mathematics", "acc,none": 0.2111111111111111, "acc_stderr,none": 0.024882116857655092}, "mmlu_high_school_physics": {"alias": "  - high_school_physics", "acc,none": 0.1986754966887417, "acc_stderr,none": 0.032578473844367746}, "mmlu_high_school_statistics": {"alias": "  - high_school_statistics", "acc,none": 0.1527777777777778, "acc_stderr,none": 0.02453632602613422}, "mmlu_machine_learning": {"alias": "  - machine_learning", "acc,none": 0.3125, "acc_stderr,none": 0.043994650575715215}, "truthfulqa_mc1": {"alias": "truthfulqa_mc1", "acc,none": 0.24357405140758873, "acc_stderr,none": 0.01502635482491078}, "winogrande": {"alias": "winogrande", "acc,none": 0.5011838989739542, "acc_stderr,none": 0.014052446290529019}, "xwinograd": {"acc,none": 0.5684423465947404, "acc_stderr,none": 0.007378392242336655, "alias": "xwinograd"}, "xwinograd_en": {"alias": " - xwinograd_en", "acc,none": 0.6223655913978494, "acc_stderr,none": 0.010056352879478859}, "xwinograd_fr": {"alias": " - xwinograd_fr", "acc,none": 0.5180722891566265, "acc_stderr,none": 0.055179683470109306}, "xwinograd_jp": {"alias": " - xwinograd_jp", "acc,none": 0.5203336809176226, "acc_stderr,none": 0.016140902975763375}, "xwinograd_pt": {"alias": " - xwinograd_pt", "acc,none": 0.4714828897338403, "acc_stderr,none": 0.030839820992717412}, "xwinograd_ru": {"alias": " - xwinograd_ru", "acc,none": 0.5428571428571428, "acc_stderr,none": 0.028112788378274866}, "xwinograd_zh": {"alias": " - xwinograd_zh", "acc,none": 0.4861111111111111, "acc_stderr,none": 0.022285295402658736}}, "groups": {"mmlu": {"acc,none": 0.22931206380857427, "acc_stderr,none": 0.003542461179876229, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.24208289054197663, "acc_stderr,none": 0.00624337585685226, "alias": " - humanities"}, "mmlu_other": {"acc,none": 0.23817186997103315, "acc_stderr,none": 0.007625915052007232, "alias": " - other"}, "mmlu_social_sciences": {"acc,none": 0.2170945726356841, "acc_stderr,none": 0.007428659190753481, "alias": " - social sciences"}, "mmlu_stem": {"acc,none": 0.2134475103076435, "acc_stderr,none": 0.007284429375156566, "alias": " - stem"}, "xwinograd": {"acc,none": 0.5684423465947404, "acc_stderr,none": 0.007378392242336655, "alias": "xwinograd"}}, "group_subtasks": {"mmlu_humanities": ["mmlu_professional_law", "mmlu_philosophy", "mmlu_prehistory", "mmlu_jurisprudence", "mmlu_high_school_us_history", "mmlu_formal_logic", "mmlu_high_school_european_history", "mmlu_moral_scenarios", "mmlu_moral_disputes", "mmlu_international_law", "mmlu_logical_fallacies", "mmlu_high_school_world_history", "mmlu_world_religions"], "mmlu_social_sciences": ["mmlu_security_studies", "mmlu_high_school_macroeconomics", "mmlu_professional_psychology", "mmlu_high_school_psychology", "mmlu_sociology", "mmlu_high_school_microeconomics", "mmlu_high_school_government_and_politics", "mmlu_econometrics", "mmlu_human_sexuality", "mmlu_us_foreign_policy", "mmlu_high_school_geography", "mmlu_public_relations"], "mmlu_other": ["mmlu_human_aging", "mmlu_marketing", "mmlu_miscellaneous", "mmlu_nutrition", "mmlu_management", "mmlu_professional_medicine", "mmlu_college_medicine", "mmlu_clinical_knowledge", "mmlu_virology", "mmlu_medical_genetics", "mmlu_global_facts", "mmlu_business_ethics", "mmlu_professional_accounting"], "mmlu_stem": ["mmlu_anatomy", "mmlu_abstract_algebra", "mmlu_high_school_physics", "mmlu_high_school_computer_science", "mmlu_electrical_engineering", "mmlu_astronomy", "mmlu_high_school_statistics", "mmlu_conceptual_physics", "mmlu_high_school_biology", "mmlu_elementary_mathematics", "mmlu_college_biology", "mmlu_college_mathematics", "mmlu_college_computer_science", "mmlu_college_chemistry", "mmlu_machine_learning", "mmlu_computer_security", "mmlu_college_physics", "mmlu_high_school_mathematics", "mmlu_high_school_chemistry"], "mmlu": ["mmlu_stem", "mmlu_other", "mmlu_social_sciences", "mmlu_humanities"], "hellaswag": [], "xwinograd": ["xwinograd_en", "xwinograd_fr", "xwinograd_jp", "xwinograd_pt", "xwinograd_ru", "xwinograd_zh"], "winogrande": [], "truthfulqa_mc1": [], "arc_challenge": [], "gsm8k": []}}