{"results": {"arc_challenge": {"alias": "arc_challenge", "acc,none": 0.3310580204778157, "acc_stderr,none": 0.013752062419817818, "acc_norm,none": 0.35238907849829354, "acc_norm_stderr,none": 0.013960142600598684}, "gsm8k": {"alias": "gsm8k", "exact_match,strict-match": 0.016679302501895376, "exact_match_stderr,strict-match": 0.0035275958887224365, "exact_match,flexible-extract": 0.024260803639120546, "exact_match_stderr,flexible-extract": 0.004238007900001387}, "hellaswag": {"alias": "hellaswag", "acc,none": 0.4958175662218682, "acc_stderr,none": 0.004989606838371073, "acc_norm,none": 0.658832901812388, "acc_norm_stderr,none": 0.004731324409133256}, "mmlu": {"acc,none": 0.26613018088591367, "acc_stderr,none": 0.0037267511693033404, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.2558979808714134, "acc_stderr,none": 0.006366351261713128, "alias": " - humanities"}, "mmlu_formal_logic": {"alias": "  - formal_logic", "acc,none": 0.2619047619047619, "acc_stderr,none": 0.03932537680392871}, "mmlu_high_school_european_history": {"alias": "  - high_school_european_history", "acc,none": 0.22424242424242424, "acc_stderr,none": 0.032568666616811015}, "mmlu_high_school_us_history": {"alias": "  - high_school_us_history", "acc,none": 0.24019607843137256, "acc_stderr,none": 0.02998373305591361}, "mmlu_high_school_world_history": {"alias": "  - high_school_world_history", "acc,none": 0.2911392405063291, "acc_stderr,none": 0.029571601065753378}, "mmlu_international_law": {"alias": "  - international_law", "acc,none": 0.23140495867768596, "acc_stderr,none": 0.03849856098794088}, "mmlu_jurisprudence": {"alias": "  - jurisprudence", "acc,none": 0.24074074074074073, "acc_stderr,none": 0.04133119440243838}, "mmlu_logical_fallacies": {"alias": "  - logical_fallacies", "acc,none": 0.22699386503067484, "acc_stderr,none": 0.032910995786157686}, "mmlu_moral_disputes": {"alias": "  - moral_disputes", "acc,none": 0.2514450867052023, "acc_stderr,none": 0.023357365785874037}, "mmlu_moral_scenarios": {"alias": "  - moral_scenarios", "acc,none": 0.26033519553072626, "acc_stderr,none": 0.014676252009319466}, "mmlu_philosophy": {"alias": "  - philosophy", "acc,none": 0.2797427652733119, "acc_stderr,none": 0.025494259350694905}, "mmlu_prehistory": {"alias": "  - prehistory", "acc,none": 0.25925925925925924, "acc_stderr,none": 0.02438366553103545}, "mmlu_professional_law": {"alias": "  - professional_law", "acc,none": 0.258148631029987, "acc_stderr,none": 0.011176923719313394}, "mmlu_world_religions": {"alias": "  - world_religions", "acc,none": 0.2222222222222222, "acc_stderr,none": 0.03188578017686398}, "mmlu_other": {"acc,none": 0.2729320888316704, "acc_stderr,none": 0.007980908868427438, "alias": " - other"}, "mmlu_business_ethics": {"alias": "  - business_ethics", "acc,none": 0.28, "acc_stderr,none": 0.045126085985421296}, "mmlu_clinical_knowledge": {"alias": "  - clinical_knowledge", "acc,none": 0.2528301886792453, "acc_stderr,none": 0.026749899771241235}, "mmlu_college_medicine": {"alias": "  - college_medicine", "acc,none": 0.24277456647398843, "acc_stderr,none": 0.0326926380614177}, "mmlu_global_facts": {"alias": "  - global_facts", "acc,none": 0.29, "acc_stderr,none": 0.045604802157206845}, "mmlu_human_aging": {"alias": "  - human_aging", "acc,none": 0.3452914798206278, "acc_stderr,none": 0.031911001928357954}, "mmlu_management": {"alias": "  - management", "acc,none": 0.34951456310679613, "acc_stderr,none": 0.047211885060971716}, "mmlu_marketing": {"alias": "  - marketing", "acc,none": 0.2564102564102564, "acc_stderr,none": 0.028605953702004253}, "mmlu_medical_genetics": {"alias": "  - medical_genetics", "acc,none": 0.3, "acc_stderr,none": 0.046056618647183814}, "mmlu_miscellaneous": {"alias": "  - miscellaneous", "acc,none": 0.2515964240102171, "acc_stderr,none": 0.015517322365529614}, "mmlu_nutrition": {"alias": "  - nutrition", "acc,none": 0.26143790849673204, "acc_stderr,none": 0.025160998214292456}, "mmlu_professional_accounting": {"alias": "  - professional_accounting", "acc,none": 0.22695035460992907, "acc_stderr,none": 0.024987106365642973}, "mmlu_professional_medicine": {"alias": "  - professional_medicine", "acc,none": 0.33455882352941174, "acc_stderr,none": 0.028661996202335307}, "mmlu_virology": {"alias": "  - virology", "acc,none": 0.28313253012048195, "acc_stderr,none": 0.03507295431370519}, "mmlu_social_sciences": {"acc,none": 0.26259343516412087, "acc_stderr,none": 0.007914134950850311, "alias": " - social sciences"}, "mmlu_econometrics": {"alias": "  - econometrics", "acc,none": 0.20175438596491227, "acc_stderr,none": 0.037752050135836386}, "mmlu_high_school_geography": {"alias": "  - high_school_geography", "acc,none": 0.23232323232323232, "acc_stderr,none": 0.030088629490217487}, "mmlu_high_school_government_and_politics": {"alias": "  - high_school_government_and_politics", "acc,none": 0.24352331606217617, "acc_stderr,none": 0.03097543638684542}, "mmlu_high_school_macroeconomics": {"alias": "  - high_school_macroeconomics", "acc,none": 0.33076923076923076, "acc_stderr,none": 0.023854795680971114}, "mmlu_high_school_microeconomics": {"alias": "  - high_school_microeconomics", "acc,none": 0.3319327731092437, "acc_stderr,none": 0.030588697013783663}, "mmlu_high_school_psychology": {"alias": "  - high_school_psychology", "acc,none": 0.26788990825688075, "acc_stderr,none": 0.018987462257978652}, "mmlu_human_sexuality": {"alias": "  - human_sexuality", "acc,none": 0.20610687022900764, "acc_stderr,none": 0.03547771004159462}, "mmlu_professional_psychology": {"alias": "  - professional_psychology", "acc,none": 0.24509803921568626, "acc_stderr,none": 0.017401816711427657}, "mmlu_public_relations": {"alias": "  - public_relations", "acc,none": 0.2818181818181818, "acc_stderr,none": 0.04309118709946458}, "mmlu_security_studies": {"alias": "  - security_studies", "acc,none": 0.23265306122448978, "acc_stderr,none": 0.02704925791589618}, "mmlu_sociology": {"alias": "  - sociology", "acc,none": 0.21393034825870647, "acc_stderr,none": 0.028996909693328927}, "mmlu_us_foreign_policy": {"alias": "  - us_foreign_policy", "acc,none": 0.3, "acc_stderr,none": 0.046056618647183814}, "mmlu_stem": {"acc,none": 0.2781477957500793, "acc_stderr,none": 0.007982658140189034, "alias": " - stem"}, "mmlu_abstract_algebra": {"alias": "  - abstract_algebra", "acc,none": 0.3, "acc_stderr,none": 0.046056618647183814}, "mmlu_anatomy": {"alias": "  - anatomy", "acc,none": 0.28888888888888886, "acc_stderr,none": 0.0391545063041425}, "mmlu_astronomy": {"alias": "  - astronomy", "acc,none": 0.26973684210526316, "acc_stderr,none": 0.036117805602848975}, "mmlu_college_biology": {"alias": "  - college_biology", "acc,none": 0.2013888888888889, "acc_stderr,none": 0.033536474697138406}, "mmlu_college_chemistry": {"alias": "  - college_chemistry", "acc,none": 0.24, "acc_stderr,none": 0.042923469599092816}, "mmlu_college_computer_science": {"alias": "  - college_computer_science", "acc,none": 0.28, "acc_stderr,none": 0.045126085985421296}, "mmlu_college_mathematics": {"alias": "  - college_mathematics", "acc,none": 0.26, "acc_stderr,none": 0.04408440022768078}, "mmlu_college_physics": {"alias": "  - college_physics", "acc,none": 0.30392156862745096, "acc_stderr,none": 0.045766654032077636}, "mmlu_computer_security": {"alias": "  - computer_security", "acc,none": 0.25, "acc_stderr,none": 0.04351941398892446}, "mmlu_conceptual_physics": {"alias": "  - conceptual_physics", "acc,none": 0.3021276595744681, "acc_stderr,none": 0.030017554471880557}, "mmlu_electrical_engineering": {"alias": "  - electrical_engineering", "acc,none": 0.22758620689655173, "acc_stderr,none": 0.03493950380131184}, "mmlu_elementary_mathematics": {"alias": "  - elementary_mathematics", "acc,none": 0.30423280423280424, "acc_stderr,none": 0.023695415009463087}, "mmlu_high_school_biology": {"alias": "  - high_school_biology", "acc,none": 0.3096774193548387, "acc_stderr,none": 0.026302774983517418}, "mmlu_high_school_chemistry": {"alias": "  - high_school_chemistry", "acc,none": 0.2857142857142857, "acc_stderr,none": 0.03178529710642749}, "mmlu_high_school_computer_science": {"alias": "  - high_school_computer_science", "acc,none": 0.21, "acc_stderr,none": 0.040936018074033256}, "mmlu_high_school_mathematics": {"alias": "  - high_school_mathematics", "acc,none": 0.3074074074074074, "acc_stderr,none": 0.028133252578815653}, "mmlu_high_school_physics": {"alias": "  - high_school_physics", "acc,none": 0.23178807947019867, "acc_stderr,none": 0.03445406271987054}, "mmlu_high_school_statistics": {"alias": "  - high_school_statistics", "acc,none": 0.27314814814814814, "acc_stderr,none": 0.03038805130167812}, "mmlu_machine_learning": {"alias": "  - machine_learning", "acc,none": 0.29464285714285715, "acc_stderr,none": 0.04327040932578729}, "truthfulqa_mc1": {"alias": "truthfulqa_mc1", "acc,none": 0.21664626682986537, "acc_stderr,none": 0.014421468452506978}, "winogrande": {"alias": "winogrande", "acc,none": 0.6250986582478295, "acc_stderr,none": 0.013605544523788008}, "xwinograd": {"acc,none": 0.7309507754551584, "acc_stderr,none": 0.0064292179648647585, "alias": "xwinograd"}, "xwinograd_en": {"alias": " - xwinograd_en", "acc,none": 0.8365591397849462, "acc_stderr,none": 0.0076702687690417375}, "xwinograd_fr": {"alias": " - xwinograd_fr", "acc,none": 0.6867469879518072, "acc_stderr,none": 0.051219942106581456}, "xwinograd_jp": {"alias": " - xwinograd_jp", "acc,none": 0.5922836287799792, "acc_stderr,none": 0.015876734592302297}, "xwinograd_pt": {"alias": " - xwinograd_pt", "acc,none": 0.6425855513307985, "acc_stderr,none": 0.029607441848169134}, "xwinograd_ru": {"alias": " - xwinograd_ru", "acc,none": 0.5587301587301587, "acc_stderr,none": 0.02802130493237513}, "xwinograd_zh": {"alias": " - xwinograd_zh", "acc,none": 0.6686507936507936, "acc_stderr,none": 0.020987400830976527}}, "groups": {"mmlu": {"acc,none": 0.26613018088591367, "acc_stderr,none": 0.0037267511693033404, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.2558979808714134, "acc_stderr,none": 0.006366351261713128, "alias": " - humanities"}, "mmlu_other": {"acc,none": 0.2729320888316704, "acc_stderr,none": 0.007980908868427438, "alias": " - other"}, "mmlu_social_sciences": {"acc,none": 0.26259343516412087, "acc_stderr,none": 0.007914134950850311, "alias": " - social sciences"}, "mmlu_stem": {"acc,none": 0.2781477957500793, "acc_stderr,none": 0.007982658140189034, "alias": " - stem"}, "xwinograd": {"acc,none": 0.7309507754551584, "acc_stderr,none": 0.0064292179648647585, "alias": "xwinograd"}}, "group_subtasks": {"mmlu_humanities": ["mmlu_professional_law", "mmlu_philosophy", "mmlu_prehistory", "mmlu_jurisprudence", "mmlu_high_school_us_history", "mmlu_formal_logic", "mmlu_high_school_european_history", "mmlu_moral_scenarios", "mmlu_moral_disputes", "mmlu_international_law", "mmlu_logical_fallacies", "mmlu_high_school_world_history", "mmlu_world_religions"], "mmlu_social_sciences": ["mmlu_security_studies", "mmlu_high_school_macroeconomics", "mmlu_professional_psychology", "mmlu_high_school_psychology", "mmlu_sociology", "mmlu_high_school_microeconomics", "mmlu_high_school_government_and_politics", "mmlu_econometrics", "mmlu_human_sexuality", "mmlu_us_foreign_policy", "mmlu_high_school_geography", "mmlu_public_relations"], "mmlu_other": ["mmlu_human_aging", "mmlu_marketing", "mmlu_miscellaneous", "mmlu_nutrition", "mmlu_management", "mmlu_professional_medicine", "mmlu_college_medicine", "mmlu_clinical_knowledge", "mmlu_virology", "mmlu_medical_genetics", "mmlu_global_facts", "mmlu_business_ethics", "mmlu_professional_accounting"], "mmlu_stem": ["mmlu_anatomy", "mmlu_abstract_algebra", "mmlu_high_school_physics", "mmlu_high_school_computer_science", "mmlu_electrical_engineering", "mmlu_astronomy", "mmlu_high_school_statistics", "mmlu_conceptual_physics", "mmlu_high_school_biology", "mmlu_elementary_mathematics", "mmlu_college_biology", "mmlu_college_mathematics", "mmlu_college_computer_science", "mmlu_college_chemistry", "mmlu_machine_learning", "mmlu_computer_security", "mmlu_college_physics", "mmlu_high_school_mathematics", "mmlu_high_school_chemistry"], "mmlu": ["mmlu_stem", "mmlu_other", "mmlu_social_sciences", "mmlu_humanities"], "hellaswag": [], "xwinograd": ["xwinograd_en", "xwinograd_fr", "xwinograd_jp", "xwinograd_pt", "xwinograd_ru", "xwinograd_zh"], "winogrande": [], "truthfulqa_mc1": [], "arc_challenge": [], "gsm8k": []}}