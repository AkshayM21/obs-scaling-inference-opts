{"results": {"arc_challenge": {"alias": "arc_challenge", "acc,none": 0.29180887372013653, "acc_stderr,none": 0.013284525292403506, "acc_norm,none": 0.30716723549488056, "acc_norm_stderr,none": 0.013481034054980945}, "gsm8k": {"alias": "gsm8k", "exact_match,strict-match": 0.01288855193328279, "exact_match_stderr,strict-match": 0.0031069012664996466, "exact_match,flexible-extract": 0.027293404094010616, "exact_match_stderr,flexible-extract": 0.004488095380209762}, "hellaswag": {"alias": "hellaswag", "acc,none": 0.44761999601672975, "acc_stderr,none": 0.004962325297840982, "acc_norm,none": 0.604959171479785, "acc_norm_stderr,none": 0.00487860369968604}, "mmlu": {"acc,none": 0.23686084603332858, "acc_stderr,none": 0.003581925305335357, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.2514346439957492, "acc_stderr,none": 0.006322827286186005, "alias": " - humanities"}, "mmlu_formal_logic": {"alias": "  - formal_logic", "acc,none": 0.23809523809523808, "acc_stderr,none": 0.03809523809523811}, "mmlu_high_school_european_history": {"alias": "  - high_school_european_history", "acc,none": 0.23636363636363636, "acc_stderr,none": 0.033175059300091805}, "mmlu_high_school_us_history": {"alias": "  - high_school_us_history", "acc,none": 0.30392156862745096, "acc_stderr,none": 0.03228210387037893}, "mmlu_high_school_world_history": {"alias": "  - high_school_world_history", "acc,none": 0.24050632911392406, "acc_stderr,none": 0.027820781981149675}, "mmlu_international_law": {"alias": "  - international_law", "acc,none": 0.21487603305785125, "acc_stderr,none": 0.03749492448709695}, "mmlu_jurisprudence": {"alias": "  - jurisprudence", "acc,none": 0.26851851851851855, "acc_stderr,none": 0.04284467968052191}, "mmlu_logical_fallacies": {"alias": "  - logical_fallacies", "acc,none": 0.2331288343558282, "acc_stderr,none": 0.033220157957767414}, "mmlu_moral_disputes": {"alias": "  - moral_disputes", "acc,none": 0.23121387283236994, "acc_stderr,none": 0.022698657167855716}, "mmlu_moral_scenarios": {"alias": "  - moral_scenarios", "acc,none": 0.23798882681564246, "acc_stderr,none": 0.01424263007057488}, "mmlu_philosophy": {"alias": "  - philosophy", "acc,none": 0.2282958199356913, "acc_stderr,none": 0.023839303311398222}, "mmlu_prehistory": {"alias": "  - prehistory", "acc,none": 0.29012345679012347, "acc_stderr,none": 0.025251173936495015}, "mmlu_professional_law": {"alias": "  - professional_law", "acc,none": 0.2522816166883963, "acc_stderr,none": 0.011092789056875232}, "mmlu_world_religions": {"alias": "  - world_religions", "acc,none": 0.3333333333333333, "acc_stderr,none": 0.03615507630310936}, "mmlu_other": {"acc,none": 0.24267782426778242, "acc_stderr,none": 0.007681047459198825, "alias": " - other"}, "mmlu_business_ethics": {"alias": "  - business_ethics", "acc,none": 0.27, "acc_stderr,none": 0.044619604333847394}, "mmlu_clinical_knowledge": {"alias": "  - clinical_knowledge", "acc,none": 0.18867924528301888, "acc_stderr,none": 0.024079995130062235}, "mmlu_college_medicine": {"alias": "  - college_medicine", "acc,none": 0.2543352601156069, "acc_stderr,none": 0.0332055644308557}, "mmlu_global_facts": {"alias": "  - global_facts", "acc,none": 0.29, "acc_stderr,none": 0.04560480215720684}, "mmlu_human_aging": {"alias": "  - human_aging", "acc,none": 0.3004484304932735, "acc_stderr,none": 0.030769352008229143}, "mmlu_management": {"alias": "  - management", "acc,none": 0.21359223300970873, "acc_stderr,none": 0.04058042015646034}, "mmlu_marketing": {"alias": "  - marketing", "acc,none": 0.2863247863247863, "acc_stderr,none": 0.029614323690456648}, "mmlu_medical_genetics": {"alias": "  - medical_genetics", "acc,none": 0.32, "acc_stderr,none": 0.046882617226215034}, "mmlu_miscellaneous": {"alias": "  - miscellaneous", "acc,none": 0.24265644955300128, "acc_stderr,none": 0.01532988894089987}, "mmlu_nutrition": {"alias": "  - nutrition", "acc,none": 0.22875816993464052, "acc_stderr,none": 0.024051029739912258}, "mmlu_professional_accounting": {"alias": "  - professional_accounting", "acc,none": 0.21631205673758866, "acc_stderr,none": 0.024561720560562793}, "mmlu_professional_medicine": {"alias": "  - professional_medicine", "acc,none": 0.1948529411764706, "acc_stderr,none": 0.02406059942348742}, "mmlu_virology": {"alias": "  - virology", "acc,none": 0.25301204819277107, "acc_stderr,none": 0.03384429155233135}, "mmlu_social_sciences": {"acc,none": 0.216769580760481, "acc_stderr,none": 0.007422201691815596, "alias": " - social sciences"}, "mmlu_econometrics": {"alias": "  - econometrics", "acc,none": 0.2631578947368421, "acc_stderr,none": 0.04142439719489361}, "mmlu_high_school_geography": {"alias": "  - high_school_geography", "acc,none": 0.16666666666666666, "acc_stderr,none": 0.026552207828215286}, "mmlu_high_school_government_and_politics": {"alias": "  - high_school_government_and_politics", "acc,none": 0.19170984455958548, "acc_stderr,none": 0.02840895362624528}, "mmlu_high_school_macroeconomics": {"alias": "  - high_school_macroeconomics", "acc,none": 0.19743589743589743, "acc_stderr,none": 0.02018264696867484}, "mmlu_high_school_microeconomics": {"alias": "  - high_school_microeconomics", "acc,none": 0.19747899159663865, "acc_stderr,none": 0.025859164122051463}, "mmlu_high_school_psychology": {"alias": "  - high_school_psychology", "acc,none": 0.1981651376146789, "acc_stderr,none": 0.0170905738042179}, "mmlu_human_sexuality": {"alias": "  - human_sexuality", "acc,none": 0.25190839694656486, "acc_stderr,none": 0.03807387116306085}, "mmlu_professional_psychology": {"alias": "  - professional_psychology", "acc,none": 0.25, "acc_stderr,none": 0.01751781884501444}, "mmlu_public_relations": {"alias": "  - public_relations", "acc,none": 0.22727272727272727, "acc_stderr,none": 0.040139645540727735}, "mmlu_security_studies": {"alias": "  - security_studies", "acc,none": 0.20408163265306123, "acc_stderr,none": 0.0258012834750905}, "mmlu_sociology": {"alias": "  - sociology", "acc,none": 0.21890547263681592, "acc_stderr,none": 0.029239174636647}, "mmlu_us_foreign_policy": {"alias": "  - us_foreign_policy", "acc,none": 0.3, "acc_stderr,none": 0.046056618647183814}, "mmlu_stem": {"acc,none": 0.228988265144307, "acc_stderr,none": 0.007462605142420058, "alias": " - stem"}, "mmlu_abstract_algebra": {"alias": "  - abstract_algebra", "acc,none": 0.21, "acc_stderr,none": 0.040936018074033256}, "mmlu_anatomy": {"alias": "  - anatomy", "acc,none": 0.23703703703703705, "acc_stderr,none": 0.03673731683969506}, "mmlu_astronomy": {"alias": "  - astronomy", "acc,none": 0.19078947368421054, "acc_stderr,none": 0.031975658210325}, "mmlu_college_biology": {"alias": "  - college_biology", "acc,none": 0.2638888888888889, "acc_stderr,none": 0.03685651095897532}, "mmlu_college_chemistry": {"alias": "  - college_chemistry", "acc,none": 0.18, "acc_stderr,none": 0.038612291966536955}, "mmlu_college_computer_science": {"alias": "  - college_computer_science", "acc,none": 0.28, "acc_stderr,none": 0.04512608598542128}, "mmlu_college_mathematics": {"alias": "  - college_mathematics", "acc,none": 0.25, "acc_stderr,none": 0.04351941398892446}, "mmlu_college_physics": {"alias": "  - college_physics", "acc,none": 0.14705882352941177, "acc_stderr,none": 0.03524068951567449}, "mmlu_computer_security": {"alias": "  - computer_security", "acc,none": 0.32, "acc_stderr,none": 0.046882617226215034}, "mmlu_conceptual_physics": {"alias": "  - conceptual_physics", "acc,none": 0.2170212765957447, "acc_stderr,none": 0.026947483121496207}, "mmlu_electrical_engineering": {"alias": "  - electrical_engineering", "acc,none": 0.2620689655172414, "acc_stderr,none": 0.03664666337225256}, "mmlu_elementary_mathematics": {"alias": "  - elementary_mathematics", "acc,none": 0.2804232804232804, "acc_stderr,none": 0.02313528797432562}, "mmlu_high_school_biology": {"alias": "  - high_school_biology", "acc,none": 0.1774193548387097, "acc_stderr,none": 0.02173254068932927}, "mmlu_high_school_chemistry": {"alias": "  - high_school_chemistry", "acc,none": 0.16748768472906403, "acc_stderr,none": 0.02627308604753542}, "mmlu_high_school_computer_science": {"alias": "  - high_school_computer_science", "acc,none": 0.29, "acc_stderr,none": 0.045604802157206845}, "mmlu_high_school_mathematics": {"alias": "  - high_school_mathematics", "acc,none": 0.24074074074074073, "acc_stderr,none": 0.026067159222275798}, "mmlu_high_school_physics": {"alias": "  - high_school_physics", "acc,none": 0.23178807947019867, "acc_stderr,none": 0.034454062719870546}, "mmlu_high_school_statistics": {"alias": "  - high_school_statistics", "acc,none": 0.18518518518518517, "acc_stderr,none": 0.026491914727355147}, "mmlu_machine_learning": {"alias": "  - machine_learning", "acc,none": 0.2767857142857143, "acc_stderr,none": 0.042466243366976256}, "truthfulqa_mc1": {"alias": "truthfulqa_mc1", "acc,none": 0.22888616891064872, "acc_stderr,none": 0.014706994909055025}, "winogrande": {"alias": "winogrande", "acc,none": 0.5808997632202052, "acc_stderr,none": 0.01386732519221012}, "xwinograd": {"acc,none": 0.6862216228365925, "acc_stderr,none": 0.006689489821572014, "alias": "xwinograd"}, "xwinograd_en": {"alias": " - xwinograd_en", "acc,none": 0.8073118279569892, "acc_stderr,none": 0.008181447054319967}, "xwinograd_fr": {"alias": " - xwinograd_fr", "acc,none": 0.4939759036144578, "acc_stderr,none": 0.055211755360913765}, "xwinograd_jp": {"alias": " - xwinograd_jp", "acc,none": 0.5391032325338895, "acc_stderr,none": 0.01610478919696588}, "xwinograd_pt": {"alias": " - xwinograd_pt", "acc,none": 0.5285171102661597, "acc_stderr,none": 0.030839820992717433}, "xwinograd_ru": {"alias": " - xwinograd_ru", "acc,none": 0.5523809523809524, "acc_stderr,none": 0.028061365638353715}, "xwinograd_zh": {"alias": " - xwinograd_zh", "acc,none": 0.6051587301587301, "acc_stderr,none": 0.021795253713508083}}, "groups": {"mmlu": {"acc,none": 0.23686084603332858, "acc_stderr,none": 0.003581925305335357, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.2514346439957492, "acc_stderr,none": 0.006322827286186005, "alias": " - humanities"}, "mmlu_other": {"acc,none": 0.24267782426778242, "acc_stderr,none": 0.007681047459198825, "alias": " - other"}, "mmlu_social_sciences": {"acc,none": 0.216769580760481, "acc_stderr,none": 0.007422201691815596, "alias": " - social sciences"}, "mmlu_stem": {"acc,none": 0.228988265144307, "acc_stderr,none": 0.007462605142420058, "alias": " - stem"}, "xwinograd": {"acc,none": 0.6862216228365925, "acc_stderr,none": 0.006689489821572014, "alias": "xwinograd"}}, "group_subtasks": {"mmlu_humanities": ["mmlu_high_school_world_history", "mmlu_high_school_european_history", "mmlu_prehistory", "mmlu_philosophy", "mmlu_high_school_us_history", "mmlu_logical_fallacies", "mmlu_moral_disputes", "mmlu_moral_scenarios", "mmlu_formal_logic", "mmlu_international_law", "mmlu_world_religions", "mmlu_jurisprudence", "mmlu_professional_law"], "mmlu_social_sciences": ["mmlu_security_studies", "mmlu_sociology", "mmlu_high_school_government_and_politics", "mmlu_human_sexuality", "mmlu_econometrics", "mmlu_public_relations", "mmlu_high_school_microeconomics", "mmlu_high_school_psychology", "mmlu_us_foreign_policy", "mmlu_professional_psychology", "mmlu_high_school_geography", "mmlu_high_school_macroeconomics"], "mmlu_other": ["mmlu_nutrition", "mmlu_professional_medicine", "mmlu_global_facts", "mmlu_virology", "mmlu_human_aging", "mmlu_clinical_knowledge", "mmlu_professional_accounting", "mmlu_miscellaneous", "mmlu_college_medicine", "mmlu_marketing", "mmlu_management", "mmlu_medical_genetics", "mmlu_business_ethics"], "mmlu_stem": ["mmlu_college_physics", "mmlu_college_mathematics", "mmlu_college_chemistry", "mmlu_abstract_algebra", "mmlu_high_school_chemistry", "mmlu_anatomy", "mmlu_electrical_engineering", "mmlu_high_school_computer_science", "mmlu_elementary_mathematics", "mmlu_college_biology", "mmlu_conceptual_physics", "mmlu_high_school_physics", "mmlu_computer_security", "mmlu_high_school_statistics", "mmlu_machine_learning", "mmlu_high_school_mathematics", "mmlu_astronomy", "mmlu_college_computer_science", "mmlu_high_school_biology"], "mmlu": ["mmlu_stem", "mmlu_other", "mmlu_social_sciences", "mmlu_humanities"], "hellaswag": [], "xwinograd": ["xwinograd_en", "xwinograd_fr", "xwinograd_jp", "xwinograd_pt", "xwinograd_ru", "xwinograd_zh"], "winogrande": [], "truthfulqa_mc1": [], "arc_challenge": [], "gsm8k": []}}