{"results": {"arc_challenge": {"alias": "arc_challenge", "acc,none": 0.1962457337883959, "acc_stderr,none": 0.011606019881416286, "acc_norm,none": 0.23378839590443687, "acc_norm_stderr,none": 0.012368225378507146}, "gsm8k": {"alias": "gsm8k", "exact_match,strict-match": 0.002274450341167551, "exact_match_stderr,strict-match": 0.0013121578148674185, "exact_match,flexible-extract": 0.014404852160727824, "exact_match_stderr,flexible-extract": 0.003282055917136938}, "hellaswag": {"alias": "hellaswag", "acc,none": 0.2819159529974109, "acc_stderr,none": 0.004490130691020433, "acc_norm,none": 0.2951603266281617, "acc_norm_stderr,none": 0.00455182627297806}, "mmlu": {"acc,none": 0.2435550491382994, "acc_stderr,none": 0.0036195322891953855, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.25696068012752393, "acc_stderr,none": 0.006367077529320058, "alias": " - humanities"}, "mmlu_formal_logic": {"alias": "  - formal_logic", "acc,none": 0.23809523809523808, "acc_stderr,none": 0.0380952380952381}, "mmlu_high_school_european_history": {"alias": "  - high_school_european_history", "acc,none": 0.2909090909090909, "acc_stderr,none": 0.03546563019624336}, "mmlu_high_school_us_history": {"alias": "  - high_school_us_history", "acc,none": 0.29901960784313725, "acc_stderr,none": 0.03213325717373617}, "mmlu_high_school_world_history": {"alias": "  - high_school_world_history", "acc,none": 0.22362869198312235, "acc_stderr,none": 0.027123298205229972}, "mmlu_international_law": {"alias": "  - international_law", "acc,none": 0.2396694214876033, "acc_stderr,none": 0.03896878985070417}, "mmlu_jurisprudence": {"alias": "  - jurisprudence", "acc,none": 0.23148148148148148, "acc_stderr,none": 0.04077494709252626}, "mmlu_logical_fallacies": {"alias": "  - logical_fallacies", "acc,none": 0.3006134969325153, "acc_stderr,none": 0.0360251131880677}, "mmlu_moral_disputes": {"alias": "  - moral_disputes", "acc,none": 0.29190751445086704, "acc_stderr,none": 0.02447699407624733}, "mmlu_moral_scenarios": {"alias": "  - moral_scenarios", "acc,none": 0.23687150837988827, "acc_stderr,none": 0.014219570788103982}, "mmlu_philosophy": {"alias": "  - philosophy", "acc,none": 0.19935691318327975, "acc_stderr,none": 0.022691033780549656}, "mmlu_prehistory": {"alias": "  - prehistory", "acc,none": 0.24691358024691357, "acc_stderr,none": 0.023993501709042107}, "mmlu_professional_law": {"alias": "  - professional_law", "acc,none": 0.2685788787483703, "acc_stderr,none": 0.011320056629121727}, "mmlu_world_religions": {"alias": "  - world_religions", "acc,none": 0.27485380116959063, "acc_stderr,none": 0.03424042924691584}, "mmlu_other": {"acc,none": 0.251689732861281, "acc_stderr,none": 0.0077862543473576785, "alias": " - other"}, "mmlu_business_ethics": {"alias": "  - business_ethics", "acc,none": 0.28, "acc_stderr,none": 0.045126085985421276}, "mmlu_clinical_knowledge": {"alias": "  - clinical_knowledge", "acc,none": 0.19622641509433963, "acc_stderr,none": 0.02444238813110083}, "mmlu_college_medicine": {"alias": "  - college_medicine", "acc,none": 0.2254335260115607, "acc_stderr,none": 0.03186209851641143}, "mmlu_global_facts": {"alias": "  - global_facts", "acc,none": 0.27, "acc_stderr,none": 0.04461960433384739}, "mmlu_human_aging": {"alias": "  - human_aging", "acc,none": 0.2600896860986547, "acc_stderr,none": 0.029442495585857466}, "mmlu_management": {"alias": "  - management", "acc,none": 0.21359223300970873, "acc_stderr,none": 0.04058042015646034}, "mmlu_marketing": {"alias": "  - marketing", "acc,none": 0.3076923076923077, "acc_stderr,none": 0.030236389942173092}, "mmlu_medical_genetics": {"alias": "  - medical_genetics", "acc,none": 0.26, "acc_stderr,none": 0.0440844002276808}, "mmlu_miscellaneous": {"alias": "  - miscellaneous", "acc,none": 0.2554278416347382, "acc_stderr,none": 0.015594955384455773}, "mmlu_nutrition": {"alias": "  - nutrition", "acc,none": 0.22549019607843138, "acc_stderr,none": 0.02392915551735128}, "mmlu_professional_accounting": {"alias": "  - professional_accounting", "acc,none": 0.2695035460992908, "acc_stderr,none": 0.026469036818590627}, "mmlu_professional_medicine": {"alias": "  - professional_medicine", "acc,none": 0.24632352941176472, "acc_stderr,none": 0.02617343857052}, "mmlu_virology": {"alias": "  - virology", "acc,none": 0.27710843373493976, "acc_stderr,none": 0.03484331592680589}, "mmlu_social_sciences": {"acc,none": 0.22944426389340267, "acc_stderr,none": 0.007576182738658268, "alias": " - social sciences"}, "mmlu_econometrics": {"alias": "  - econometrics", "acc,none": 0.24561403508771928, "acc_stderr,none": 0.04049339297748141}, "mmlu_high_school_geography": {"alias": "  - high_school_geography", "acc,none": 0.22727272727272727, "acc_stderr,none": 0.02985751567338641}, "mmlu_high_school_government_and_politics": {"alias": "  - high_school_government_and_politics", "acc,none": 0.22279792746113988, "acc_stderr,none": 0.030031147977641545}, "mmlu_high_school_macroeconomics": {"alias": "  - high_school_macroeconomics", "acc,none": 0.23076923076923078, "acc_stderr,none": 0.021362027725222724}, "mmlu_high_school_microeconomics": {"alias": "  - high_school_microeconomics", "acc,none": 0.23949579831932774, "acc_stderr,none": 0.027722065493361262}, "mmlu_high_school_psychology": {"alias": "  - high_school_psychology", "acc,none": 0.23302752293577983, "acc_stderr,none": 0.0181256691808615}, "mmlu_human_sexuality": {"alias": "  - human_sexuality", "acc,none": 0.1984732824427481, "acc_stderr,none": 0.0349814938546247}, "mmlu_professional_psychology": {"alias": "  - professional_psychology", "acc,none": 0.2630718954248366, "acc_stderr,none": 0.01781267654232066}, "mmlu_public_relations": {"alias": "  - public_relations", "acc,none": 0.18181818181818182, "acc_stderr,none": 0.03694284335337801}, "mmlu_security_studies": {"alias": "  - security_studies", "acc,none": 0.1510204081632653, "acc_stderr,none": 0.022923004094736844}, "mmlu_sociology": {"alias": "  - sociology", "acc,none": 0.24378109452736318, "acc_stderr,none": 0.030360490154014645}, "mmlu_us_foreign_policy": {"alias": "  - us_foreign_policy", "acc,none": 0.23, "acc_stderr,none": 0.04229525846816506}, "mmlu_stem": {"acc,none": 0.22930542340627974, "acc_stderr,none": 0.007485379921657768, "alias": " - stem"}, "mmlu_abstract_algebra": {"alias": "  - abstract_algebra", "acc,none": 0.18, "acc_stderr,none": 0.03861229196653694}, "mmlu_anatomy": {"alias": "  - anatomy", "acc,none": 0.2518518518518518, "acc_stderr,none": 0.03749850709174022}, "mmlu_astronomy": {"alias": "  - astronomy", "acc,none": 0.17105263157894737, "acc_stderr,none": 0.030643607071677088}, "mmlu_college_biology": {"alias": "  - college_biology", "acc,none": 0.1875, "acc_stderr,none": 0.032639560491693344}, "mmlu_college_chemistry": {"alias": "  - college_chemistry", "acc,none": 0.24, "acc_stderr,none": 0.04292346959909284}, "mmlu_college_computer_science": {"alias": "  - college_computer_science", "acc,none": 0.24, "acc_stderr,none": 0.04292346959909284}, "mmlu_college_mathematics": {"alias": "  - college_mathematics", "acc,none": 0.22, "acc_stderr,none": 0.04163331998932269}, "mmlu_college_physics": {"alias": "  - college_physics", "acc,none": 0.23529411764705882, "acc_stderr,none": 0.04220773659171451}, "mmlu_computer_security": {"alias": "  - computer_security", "acc,none": 0.32, "acc_stderr,none": 0.046882617226215034}, "mmlu_conceptual_physics": {"alias": "  - conceptual_physics", "acc,none": 0.2553191489361702, "acc_stderr,none": 0.028504856470514206}, "mmlu_electrical_engineering": {"alias": "  - electrical_engineering", "acc,none": 0.23448275862068965, "acc_stderr,none": 0.035306258743465914}, "mmlu_elementary_mathematics": {"alias": "  - elementary_mathematics", "acc,none": 0.21164021164021163, "acc_stderr,none": 0.021037331505262893}, "mmlu_high_school_biology": {"alias": "  - high_school_biology", "acc,none": 0.2, "acc_stderr,none": 0.022755204959542936}, "mmlu_high_school_chemistry": {"alias": "  - high_school_chemistry", "acc,none": 0.21674876847290642, "acc_stderr,none": 0.02899033125251624}, "mmlu_high_school_computer_science": {"alias": "  - high_school_computer_science", "acc,none": 0.28, "acc_stderr,none": 0.045126085985421276}, "mmlu_high_school_mathematics": {"alias": "  - high_school_mathematics", "acc,none": 0.24444444444444444, "acc_stderr,none": 0.02620276653465215}, "mmlu_high_school_physics": {"alias": "  - high_school_physics", "acc,none": 0.2781456953642384, "acc_stderr,none": 0.03658603262763743}, "mmlu_high_school_statistics": {"alias": "  - high_school_statistics", "acc,none": 0.20833333333333334, "acc_stderr,none": 0.027696910713093936}, "mmlu_machine_learning": {"alias": "  - machine_learning", "acc,none": 0.2767857142857143, "acc_stderr,none": 0.042466243366976256}, "truthfulqa_mc1": {"alias": "truthfulqa_mc1", "acc,none": 0.23378212974296206, "acc_stderr,none": 0.014816195991931602}, "winogrande": {"alias": "winogrande", "acc,none": 0.5114443567482242, "acc_stderr,none": 0.014048804199859322}, "xwinograd": {"acc,none": 0.5279838165879973, "acc_stderr,none": 0.007472460009884372, "alias": "xwinograd"}, "xwinograd_en": {"alias": " - xwinograd_en", "acc,none": 0.5578494623655914, "acc_stderr,none": 0.010302094428535519}, "xwinograd_fr": {"alias": " - xwinograd_fr", "acc,none": 0.5421686746987951, "acc_stderr,none": 0.05501904358494247}, "xwinograd_jp": {"alias": " - xwinograd_jp", "acc,none": 0.5015641293013556, "acc_stderr,none": 0.016154187700745243}, "xwinograd_pt": {"alias": " - xwinograd_pt", "acc,none": 0.5019011406844106, "acc_stderr,none": 0.030889879865535985}, "xwinograd_ru": {"alias": " - xwinograd_ru", "acc,none": 0.5047619047619047, "acc_stderr,none": 0.028215352704267855}, "xwinograd_zh": {"alias": " - xwinograd_zh", "acc,none": 0.4662698412698413, "acc_stderr,none": 0.022243111668199027}}, "groups": {"mmlu": {"acc,none": 0.2435550491382994, "acc_stderr,none": 0.0036195322891953855, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.25696068012752393, "acc_stderr,none": 0.006367077529320058, "alias": " - humanities"}, "mmlu_other": {"acc,none": 0.251689732861281, "acc_stderr,none": 0.0077862543473576785, "alias": " - other"}, "mmlu_social_sciences": {"acc,none": 0.22944426389340267, "acc_stderr,none": 0.007576182738658268, "alias": " - social sciences"}, "mmlu_stem": {"acc,none": 0.22930542340627974, "acc_stderr,none": 0.007485379921657768, "alias": " - stem"}, "xwinograd": {"acc,none": 0.5279838165879973, "acc_stderr,none": 0.007472460009884372, "alias": "xwinograd"}}, "group_subtasks": {"mmlu_humanities": ["mmlu_high_school_world_history", "mmlu_high_school_european_history", "mmlu_prehistory", "mmlu_philosophy", "mmlu_high_school_us_history", "mmlu_logical_fallacies", "mmlu_moral_disputes", "mmlu_moral_scenarios", "mmlu_formal_logic", "mmlu_international_law", "mmlu_world_religions", "mmlu_jurisprudence", "mmlu_professional_law"], "mmlu_social_sciences": ["mmlu_security_studies", "mmlu_sociology", "mmlu_high_school_government_and_politics", "mmlu_human_sexuality", "mmlu_econometrics", "mmlu_public_relations", "mmlu_high_school_microeconomics", "mmlu_high_school_psychology", "mmlu_us_foreign_policy", "mmlu_professional_psychology", "mmlu_high_school_geography", "mmlu_high_school_macroeconomics"], "mmlu_other": ["mmlu_nutrition", "mmlu_professional_medicine", "mmlu_global_facts", "mmlu_virology", "mmlu_human_aging", "mmlu_clinical_knowledge", "mmlu_professional_accounting", "mmlu_miscellaneous", "mmlu_college_medicine", "mmlu_marketing", "mmlu_management", "mmlu_medical_genetics", "mmlu_business_ethics"], "mmlu_stem": ["mmlu_college_physics", "mmlu_college_mathematics", "mmlu_college_chemistry", "mmlu_abstract_algebra", "mmlu_high_school_chemistry", "mmlu_anatomy", "mmlu_electrical_engineering", "mmlu_high_school_computer_science", "mmlu_elementary_mathematics", "mmlu_college_biology", "mmlu_conceptual_physics", "mmlu_high_school_physics", "mmlu_computer_security", "mmlu_high_school_statistics", "mmlu_machine_learning", "mmlu_high_school_mathematics", "mmlu_astronomy", "mmlu_college_computer_science", "mmlu_high_school_biology"], "mmlu": ["mmlu_stem", "mmlu_other", "mmlu_social_sciences", "mmlu_humanities"], "hellaswag": [], "xwinograd": ["xwinograd_en", "xwinograd_fr", "xwinograd_jp", "xwinograd_pt", "xwinograd_ru", "xwinograd_zh"], "winogrande": [], "truthfulqa_mc1": [], "arc_challenge": [], "gsm8k": []}}