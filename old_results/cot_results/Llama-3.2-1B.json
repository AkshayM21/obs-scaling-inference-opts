{"results": {"arc_challenge_cot": {"alias": "arc_challenge_cot", "exact_match,strict-match": 0.005119453924914676, "exact_match_stderr,strict-match": 0.0020855415236791844, "exact_match,flexible-extract": 0.008532423208191127, "exact_match_stderr,flexible-extract": 0.0026878003416689637}, "gsm8k_cot_zeroshot": {"alias": "gsm8k_cot_zeroshot", "exact_match,strict-match": 0.000758150113722517, "exact_match_stderr,strict-match": 0.0007581501137225328, "exact_match,flexible-extract": 0.04397270659590599, "exact_match_stderr,flexible-extract": 0.0056476664491264626}, "hellaswag_cot": {"alias": "hellaswag_cot", "exact_match,strict-match": 0.004879506074487154, "exact_match_stderr,strict-match": 0.0006954041580471108, "exact_match,flexible-extract": 0.020912168890659232, "exact_match_stderr,flexible-extract": 0.0014279792006347098}, "mmlu_flan_cot_zeroshot": {"alias": "mmlu (flan style, zeroshot cot)"}, "humanities": {"alias": " - humanities"}, "mmlu_flan_cot_zeroshot_formal_logic": {"alias": "  - mmlu_flan_cot_zeroshot_formal_logic", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_high_school_european_history": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_european_history", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_high_school_us_history": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_us_history", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_high_school_world_history": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_world_history", "exact_match,strict-match": 0.038461538461538464, "exact_match_stderr,strict-match": 0.03846153846153844, "exact_match,flexible-extract": 0.038461538461538464, "exact_match_stderr,flexible-extract": 0.03846153846153844}, "mmlu_flan_cot_zeroshot_international_law": {"alias": "  - mmlu_flan_cot_zeroshot_international_law", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_jurisprudence": {"alias": "  - mmlu_flan_cot_zeroshot_jurisprudence", "exact_match,strict-match": 0.09090909090909091, "exact_match_stderr,strict-match": 0.0909090909090909, "exact_match,flexible-extract": 0.09090909090909091, "exact_match_stderr,flexible-extract": 0.0909090909090909}, "mmlu_flan_cot_zeroshot_logical_fallacies": {"alias": "  - mmlu_flan_cot_zeroshot_logical_fallacies", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.05555555555555555, "exact_match_stderr,flexible-extract": 0.055555555555555566}, "mmlu_flan_cot_zeroshot_moral_disputes": {"alias": "  - mmlu_flan_cot_zeroshot_moral_disputes", "exact_match,strict-match": 0.02631578947368421, "exact_match_stderr,strict-match": 0.026315789473684213, "exact_match,flexible-extract": 0.07894736842105263, "exact_match_stderr,flexible-extract": 0.0443312718129372}, "mmlu_flan_cot_zeroshot_moral_scenarios": {"alias": "  - mmlu_flan_cot_zeroshot_moral_scenarios", "exact_match,strict-match": 0.06, "exact_match_stderr,strict-match": 0.023868325657594208, "exact_match,flexible-extract": 0.06, "exact_match_stderr,flexible-extract": 0.023868325657594208}, "mmlu_flan_cot_zeroshot_philosophy": {"alias": "  - mmlu_flan_cot_zeroshot_philosophy", "exact_match,strict-match": 0.058823529411764705, "exact_match_stderr,strict-match": 0.04095944846016421, "exact_match,flexible-extract": 0.058823529411764705, "exact_match_stderr,flexible-extract": 0.04095944846016421}, "mmlu_flan_cot_zeroshot_prehistory": {"alias": "  - mmlu_flan_cot_zeroshot_prehistory", "exact_match,strict-match": 0.05714285714285714, "exact_match_stderr,strict-match": 0.039807459772527774, "exact_match,flexible-extract": 0.08571428571428572, "exact_match_stderr,flexible-extract": 0.04800960288096035}, "mmlu_flan_cot_zeroshot_professional_law": {"alias": "  - mmlu_flan_cot_zeroshot_professional_law", "exact_match,strict-match": 0.01764705882352941, "exact_match_stderr,strict-match": 0.01012806755004498, "exact_match,flexible-extract": 0.029411764705882353, "exact_match_stderr,flexible-extract": 0.012996748069090581}, "mmlu_flan_cot_zeroshot_world_religions": {"alias": "  - mmlu_flan_cot_zeroshot_world_religions", "exact_match,strict-match": 0.10526315789473684, "exact_match_stderr,strict-match": 0.0723351864143449, "exact_match,flexible-extract": 0.05263157894736842, "exact_match_stderr,flexible-extract": 0.052631578947368404}, "other": {"alias": " - other"}, "mmlu_flan_cot_zeroshot_business_ethics": {"alias": "  - mmlu_flan_cot_zeroshot_business_ethics", "exact_match,strict-match": 0.09090909090909091, "exact_match_stderr,strict-match": 0.0909090909090909, "exact_match,flexible-extract": 0.18181818181818182, "exact_match_stderr,flexible-extract": 0.12196734422726124}, "mmlu_flan_cot_zeroshot_clinical_knowledge": {"alias": "  - mmlu_flan_cot_zeroshot_clinical_knowledge", "exact_match,strict-match": 0.06896551724137931, "exact_match_stderr,strict-match": 0.04788724653995955, "exact_match,flexible-extract": 0.10344827586206896, "exact_match_stderr,flexible-extract": 0.05755330761353655}, "mmlu_flan_cot_zeroshot_college_medicine": {"alias": "  - mmlu_flan_cot_zeroshot_college_medicine", "exact_match,strict-match": 0.13636363636363635, "exact_match_stderr,strict-match": 0.07488677009526491, "exact_match,flexible-extract": 0.09090909090909091, "exact_match_stderr,flexible-extract": 0.06273323266748673}, "mmlu_flan_cot_zeroshot_global_facts": {"alias": "  - mmlu_flan_cot_zeroshot_global_facts", "exact_match,strict-match": 0.1, "exact_match_stderr,strict-match": 0.09999999999999999, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_human_aging": {"alias": "  - mmlu_flan_cot_zeroshot_human_aging", "exact_match,strict-match": 0.08695652173913043, "exact_match_stderr,strict-match": 0.060073850409370216, "exact_match,flexible-extract": 0.043478260869565216, "exact_match_stderr,flexible-extract": 0.04347826086956522}, "mmlu_flan_cot_zeroshot_management": {"alias": "  - mmlu_flan_cot_zeroshot_management", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.09090909090909091, "exact_match_stderr,flexible-extract": 0.09090909090909091}, "mmlu_flan_cot_zeroshot_marketing": {"alias": "  - mmlu_flan_cot_zeroshot_marketing", "exact_match,strict-match": 0.12, "exact_match_stderr,strict-match": 0.066332495807108, "exact_match,flexible-extract": 0.04, "exact_match_stderr,flexible-extract": 0.04000000000000002}, "mmlu_flan_cot_zeroshot_medical_genetics": {"alias": "  - mmlu_flan_cot_zeroshot_medical_genetics", "exact_match,strict-match": 0.09090909090909091, "exact_match_stderr,strict-match": 0.09090909090909091, "exact_match,flexible-extract": 0.09090909090909091, "exact_match_stderr,flexible-extract": 0.0909090909090909}, "mmlu_flan_cot_zeroshot_miscellaneous": {"alias": "  - mmlu_flan_cot_zeroshot_miscellaneous", "exact_match,strict-match": 0.06976744186046512, "exact_match_stderr,strict-match": 0.027632024455163016, "exact_match,flexible-extract": 0.09302325581395349, "exact_match_stderr,flexible-extract": 0.031505355229515274}, "mmlu_flan_cot_zeroshot_nutrition": {"alias": "  - mmlu_flan_cot_zeroshot_nutrition", "exact_match,strict-match": 0.12121212121212122, "exact_match_stderr,strict-match": 0.057695250801999304, "exact_match,flexible-extract": 0.06060606060606061, "exact_match_stderr,flexible-extract": 0.04218003305174259}, "mmlu_flan_cot_zeroshot_professional_accounting": {"alias": "  - mmlu_flan_cot_zeroshot_professional_accounting", "exact_match,strict-match": 0.06451612903225806, "exact_match_stderr,strict-match": 0.04485301852605206, "exact_match,flexible-extract": 0.03225806451612903, "exact_match_stderr,flexible-extract": 0.03225806451612906}, "mmlu_flan_cot_zeroshot_professional_medicine": {"alias": "  - mmlu_flan_cot_zeroshot_professional_medicine", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_virology": {"alias": "  - mmlu_flan_cot_zeroshot_virology", "exact_match,strict-match": 0.1111111111111111, "exact_match_stderr,strict-match": 0.07622159339667062, "exact_match,flexible-extract": 0.05555555555555555, "exact_match_stderr,flexible-extract": 0.05555555555555556}, "social sciences": {"alias": " - social sciences"}, "mmlu_flan_cot_zeroshot_econometrics": {"alias": "  - mmlu_flan_cot_zeroshot_econometrics", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.08333333333333333, "exact_match_stderr,flexible-extract": 0.08333333333333331}, "mmlu_flan_cot_zeroshot_high_school_geography": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_geography", "exact_match,strict-match": 0.045454545454545456, "exact_match_stderr,strict-match": 0.04545454545454547, "exact_match,flexible-extract": 0.045454545454545456, "exact_match_stderr,flexible-extract": 0.04545454545454546}, "mmlu_flan_cot_zeroshot_high_school_government_and_politics": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_government_and_politics", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.09523809523809523, "exact_match_stderr,flexible-extract": 0.06563832739090582}, "mmlu_flan_cot_zeroshot_high_school_macroeconomics": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_macroeconomics", "exact_match,strict-match": 0.023255813953488372, "exact_match_stderr,strict-match": 0.02325581395348838, "exact_match,flexible-extract": 0.046511627906976744, "exact_match_stderr,flexible-extract": 0.032494796790966146}, "mmlu_flan_cot_zeroshot_high_school_microeconomics": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_microeconomics", "exact_match,strict-match": 0.038461538461538464, "exact_match_stderr,strict-match": 0.038461538461538464, "exact_match,flexible-extract": 0.07692307692307693, "exact_match_stderr,flexible-extract": 0.05329387100211932}, "mmlu_flan_cot_zeroshot_high_school_psychology": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_psychology", "exact_match,strict-match": 0.08333333333333333, "exact_match_stderr,strict-match": 0.0359823141323644, "exact_match,flexible-extract": 0.1, "exact_match_stderr,flexible-extract": 0.03905667329424715}, "mmlu_flan_cot_zeroshot_human_sexuality": {"alias": "  - mmlu_flan_cot_zeroshot_human_sexuality", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.08333333333333333, "exact_match_stderr,flexible-extract": 0.08333333333333331}, "mmlu_flan_cot_zeroshot_professional_psychology": {"alias": "  - mmlu_flan_cot_zeroshot_professional_psychology", "exact_match,strict-match": 0.028985507246376812, "exact_match_stderr,strict-match": 0.02034458578649911, "exact_match,flexible-extract": 0.057971014492753624, "exact_match_stderr,flexible-extract": 0.02833890901721172}, "mmlu_flan_cot_zeroshot_public_relations": {"alias": "  - mmlu_flan_cot_zeroshot_public_relations", "exact_match,strict-match": 0.08333333333333333, "exact_match_stderr,strict-match": 0.08333333333333333, "exact_match,flexible-extract": 0.08333333333333333, "exact_match_stderr,flexible-extract": 0.08333333333333333}, "mmlu_flan_cot_zeroshot_security_studies": {"alias": "  - mmlu_flan_cot_zeroshot_security_studies", "exact_match,strict-match": 0.1111111111111111, "exact_match_stderr,strict-match": 0.06163335513613659, "exact_match,flexible-extract": 0.07407407407407407, "exact_match_stderr,flexible-extract": 0.05136112928011382}, "mmlu_flan_cot_zeroshot_sociology": {"alias": "  - mmlu_flan_cot_zeroshot_sociology", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_us_foreign_policy": {"alias": "  - mmlu_flan_cot_zeroshot_us_foreign_policy", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "stem": {"alias": " - stem"}, "mmlu_flan_cot_zeroshot_abstract_algebra": {"alias": "  - mmlu_flan_cot_zeroshot_abstract_algebra", "exact_match,strict-match": 0.09090909090909091, "exact_match_stderr,strict-match": 0.0909090909090909, "exact_match,flexible-extract": 0.09090909090909091, "exact_match_stderr,flexible-extract": 0.0909090909090909}, "mmlu_flan_cot_zeroshot_anatomy": {"alias": "  - mmlu_flan_cot_zeroshot_anatomy", "exact_match,strict-match": 0.21428571428571427, "exact_match_stderr,strict-match": 0.11380392954509883, "exact_match,flexible-extract": 0.14285714285714285, "exact_match_stderr,flexible-extract": 0.09705231721239393}, "mmlu_flan_cot_zeroshot_astronomy": {"alias": "  - mmlu_flan_cot_zeroshot_astronomy", "exact_match,strict-match": 0.0625, "exact_match_stderr,strict-match": 0.0625, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_college_biology": {"alias": "  - mmlu_flan_cot_zeroshot_college_biology", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_college_chemistry": {"alias": "  - mmlu_flan_cot_zeroshot_college_chemistry", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_college_computer_science": {"alias": "  - mmlu_flan_cot_zeroshot_college_computer_science", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.09090909090909091, "exact_match_stderr,flexible-extract": 0.09090909090909091}, "mmlu_flan_cot_zeroshot_college_mathematics": {"alias": "  - mmlu_flan_cot_zeroshot_college_mathematics", "exact_match,strict-match": 0.09090909090909091, "exact_match_stderr,strict-match": 0.0909090909090909, "exact_match,flexible-extract": 0.09090909090909091, "exact_match_stderr,flexible-extract": 0.0909090909090909}, "mmlu_flan_cot_zeroshot_college_physics": {"alias": "  - mmlu_flan_cot_zeroshot_college_physics", "exact_match,strict-match": 0.09090909090909091, "exact_match_stderr,strict-match": 0.09090909090909091, "exact_match,flexible-extract": 0.09090909090909091, "exact_match_stderr,flexible-extract": 0.0909090909090909}, "mmlu_flan_cot_zeroshot_computer_security": {"alias": "  - mmlu_flan_cot_zeroshot_computer_security", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_conceptual_physics": {"alias": "  - mmlu_flan_cot_zeroshot_conceptual_physics", "exact_match,strict-match": 0.038461538461538464, "exact_match_stderr,strict-match": 0.03846153846153844, "exact_match,flexible-extract": 0.11538461538461539, "exact_match_stderr,flexible-extract": 0.06389710663783137}, "mmlu_flan_cot_zeroshot_electrical_engineering": {"alias": "  - mmlu_flan_cot_zeroshot_electrical_engineering", "exact_match,strict-match": 0.125, "exact_match_stderr,strict-match": 0.08539125638299665, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_elementary_mathematics": {"alias": "  - mmlu_flan_cot_zeroshot_elementary_mathematics", "exact_match,strict-match": 0.07317073170731707, "exact_match_stderr,strict-match": 0.04117547077105887, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_high_school_biology": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_biology", "exact_match,strict-match": 0.09375, "exact_match_stderr,strict-match": 0.0523514603733822, "exact_match,flexible-extract": 0.0625, "exact_match_stderr,flexible-extract": 0.04347552147751577}, "mmlu_flan_cot_zeroshot_high_school_chemistry": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_chemistry", "exact_match,strict-match": 0.09090909090909091, "exact_match_stderr,strict-match": 0.06273323266748675, "exact_match,flexible-extract": 0.18181818181818182, "exact_match_stderr,flexible-extract": 0.08416546361568647}, "mmlu_flan_cot_zeroshot_high_school_computer_science": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_computer_science", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_high_school_mathematics": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_mathematics", "exact_match,strict-match": 0.10344827586206896, "exact_match_stderr,strict-match": 0.05755330761353655, "exact_match,flexible-extract": 0.10344827586206896, "exact_match_stderr,flexible-extract": 0.05755330761353655}, "mmlu_flan_cot_zeroshot_high_school_physics": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_physics", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.058823529411764705, "exact_match_stderr,flexible-extract": 0.05882352941176472}, "mmlu_flan_cot_zeroshot_high_school_statistics": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_statistics", "exact_match,strict-match": 0.043478260869565216, "exact_match_stderr,strict-match": 0.04347826086956523, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_machine_learning": {"alias": "  - mmlu_flan_cot_zeroshot_machine_learning", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "truthfulqa_cot": {"alias": "truthfulqa_cot", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "winogrande_cot": {"alias": "winogrande_cot", "exact_match,strict-match": 0.007103393843725336, "exact_match_stderr,strict-match": 0.002360304897638305, "exact_match,flexible-extract": 0.009471191791633781, "exact_match_stderr,flexible-extract": 0.0027221936601942377}, "xwinograd_cot": {"alias": "xwinograd_cot"}, "xwinograd_cot_en": {"alias": " - xwinograd_cot_en", "exact_match,strict-match": 0.02021505376344086, "exact_match_stderr,strict-match": 0.002919340617436115, "exact_match,flexible-extract": 0.03139784946236559, "exact_match_stderr,flexible-extract": 0.0036174672171163805}, "xwinograd_cot_fr": {"alias": " - xwinograd_cot_fr", "exact_match,strict-match": 0.012048192771084338, "exact_match_stderr,strict-match": 0.012048192771084335, "exact_match,flexible-extract": 0.024096385542168676, "exact_match_stderr,flexible-extract": 0.016934504301174148}, "xwinograd_cot_jp": {"alias": " - xwinograd_cot_jp", "exact_match,strict-match": 0.011470281543274244, "exact_match_stderr,strict-match": 0.0034403224506276654, "exact_match,flexible-extract": 0.040667361835245046, "exact_match_stderr,flexible-extract": 0.006381530591446522}, "xwinograd_cot_pt": {"alias": " - xwinograd_cot_pt", "exact_match,strict-match": 0.0038022813688212928, "exact_match_stderr,strict-match": 0.0038022813688213023, "exact_match,flexible-extract": 0.0076045627376425855, "exact_match_stderr,flexible-extract": 0.005366966164769959}, "xwinograd_cot_ru": {"alias": " - xwinograd_cot_ru", "exact_match,strict-match": 0.006349206349206349, "exact_match_stderr,strict-match": 0.004482412171899724, "exact_match,flexible-extract": 0.03492063492063492, "exact_match_stderr,flexible-extract": 0.010359952317933505}, "xwinograd_cot_zh": {"alias": " - xwinograd_cot_zh", "exact_match,strict-match": 0.005952380952380952, "exact_match_stderr,strict-match": 0.003429769716019982, "exact_match,flexible-extract": 0.015873015873015872, "exact_match_stderr,flexible-extract": 0.005572772355824598}}, "groups": {"mmlu_flan_cot_zeroshot": {"alias": "mmlu (flan style, zeroshot cot)"}, "humanities": {"alias": " - humanities"}, "other": {"alias": " - other"}, "social sciences": {"alias": " - social sciences"}, "stem": {"alias": " - stem"}, "xwinograd_cot": {"alias": "xwinograd_cot"}}, "group_subtasks": {"hellaswag_cot": [], "arc_challenge_cot": [], "truthfulqa_cot": [], "xwinograd_cot": ["xwinograd_cot_en", "xwinograd_cot_fr", "xwinograd_cot_jp", "xwinograd_cot_pt", "xwinograd_cot_ru", "xwinograd_cot_zh"], "winogrande_cot": [], "gsm8k_cot_zeroshot": [], "humanities": ["mmlu_flan_cot_zeroshot_professional_law", "mmlu_flan_cot_zeroshot_philosophy", "mmlu_flan_cot_zeroshot_prehistory", "mmlu_flan_cot_zeroshot_jurisprudence", "mmlu_flan_cot_zeroshot_high_school_us_history", "mmlu_flan_cot_zeroshot_formal_logic", "mmlu_flan_cot_zeroshot_high_school_european_history", "mmlu_flan_cot_zeroshot_moral_scenarios", "mmlu_flan_cot_zeroshot_moral_disputes", "mmlu_flan_cot_zeroshot_international_law", "mmlu_flan_cot_zeroshot_logical_fallacies", "mmlu_flan_cot_zeroshot_high_school_world_history", "mmlu_flan_cot_zeroshot_world_religions"], "social sciences": ["mmlu_flan_cot_zeroshot_security_studies", "mmlu_flan_cot_zeroshot_high_school_macroeconomics", "mmlu_flan_cot_zeroshot_professional_psychology", "mmlu_flan_cot_zeroshot_high_school_psychology", "mmlu_flan_cot_zeroshot_sociology", "mmlu_flan_cot_zeroshot_high_school_microeconomics", "mmlu_flan_cot_zeroshot_high_school_government_and_politics", "mmlu_flan_cot_zeroshot_econometrics", "mmlu_flan_cot_zeroshot_human_sexuality", "mmlu_flan_cot_zeroshot_us_foreign_policy", "mmlu_flan_cot_zeroshot_high_school_geography", "mmlu_flan_cot_zeroshot_public_relations"], "other": ["mmlu_flan_cot_zeroshot_human_aging", "mmlu_flan_cot_zeroshot_marketing", "mmlu_flan_cot_zeroshot_miscellaneous", "mmlu_flan_cot_zeroshot_nutrition", "mmlu_flan_cot_zeroshot_management", "mmlu_flan_cot_zeroshot_professional_medicine", "mmlu_flan_cot_zeroshot_college_medicine", "mmlu_flan_cot_zeroshot_clinical_knowledge", "mmlu_flan_cot_zeroshot_virology", "mmlu_flan_cot_zeroshot_medical_genetics", "mmlu_flan_cot_zeroshot_global_facts", "mmlu_flan_cot_zeroshot_business_ethics", "mmlu_flan_cot_zeroshot_professional_accounting"], "stem": ["mmlu_flan_cot_zeroshot_anatomy", "mmlu_flan_cot_zeroshot_abstract_algebra", "mmlu_flan_cot_zeroshot_high_school_physics", "mmlu_flan_cot_zeroshot_high_school_computer_science", "mmlu_flan_cot_zeroshot_electrical_engineering", "mmlu_flan_cot_zeroshot_astronomy", "mmlu_flan_cot_zeroshot_high_school_statistics", "mmlu_flan_cot_zeroshot_conceptual_physics", "mmlu_flan_cot_zeroshot_high_school_biology", "mmlu_flan_cot_zeroshot_elementary_mathematics", "mmlu_flan_cot_zeroshot_college_biology", "mmlu_flan_cot_zeroshot_college_mathematics", "mmlu_flan_cot_zeroshot_college_computer_science", "mmlu_flan_cot_zeroshot_college_chemistry", "mmlu_flan_cot_zeroshot_machine_learning", "mmlu_flan_cot_zeroshot_computer_security", "mmlu_flan_cot_zeroshot_college_physics", "mmlu_flan_cot_zeroshot_high_school_mathematics", "mmlu_flan_cot_zeroshot_high_school_chemistry"], "mmlu_flan_cot_zeroshot": ["stem", "other", "social sciences", "humanities"]}}