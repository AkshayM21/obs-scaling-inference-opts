{"results": {"arc_challenge": {"alias": "arc_challenge", "acc,none": 0.22781569965870307, "acc_stderr,none": 0.012256708602326923, "acc_norm,none": 0.24573378839590443, "acc_norm_stderr,none": 0.012581033453730104}, "gsm8k": {"alias": "gsm8k", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.003032600454890068, "exact_match_stderr,flexible-extract": 0.0015145735612245494}, "hellaswag": {"alias": "hellaswag", "acc,none": 0.2811192989444334, "acc_stderr,none": 0.004486268470666294, "acc_norm,none": 0.29067914758016333, "acc_norm_stderr,none": 0.00453147740758966}, "mmlu": {"acc,none": 0.241917105825381, "acc_stderr,none": 0.003612722551674195, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.24484590860786398, "acc_stderr,none": 0.006272807466303847, "alias": " - humanities"}, "mmlu_formal_logic": {"alias": "  - formal_logic", "acc,none": 0.2619047619047619, "acc_stderr,none": 0.03932537680392871}, "mmlu_high_school_european_history": {"alias": "  - high_school_european_history", "acc,none": 0.21818181818181817, "acc_stderr,none": 0.03225078108306289}, "mmlu_high_school_us_history": {"alias": "  - high_school_us_history", "acc,none": 0.24509803921568626, "acc_stderr,none": 0.030190282453501967}, "mmlu_high_school_world_history": {"alias": "  - high_school_world_history", "acc,none": 0.25316455696202533, "acc_stderr,none": 0.02830465794303529}, "mmlu_international_law": {"alias": "  - international_law", "acc,none": 0.23140495867768596, "acc_stderr,none": 0.03849856098794089}, "mmlu_jurisprudence": {"alias": "  - jurisprudence", "acc,none": 0.25925925925925924, "acc_stderr,none": 0.042365112580946336}, "mmlu_logical_fallacies": {"alias": "  - logical_fallacies", "acc,none": 0.20245398773006135, "acc_stderr,none": 0.031570650789119026}, "mmlu_moral_disputes": {"alias": "  - moral_disputes", "acc,none": 0.2832369942196532, "acc_stderr,none": 0.024257901705323378}, "mmlu_moral_scenarios": {"alias": "  - moral_scenarios", "acc,none": 0.25251396648044694, "acc_stderr,none": 0.014530330201468654}, "mmlu_philosophy": {"alias": "  - philosophy", "acc,none": 0.22186495176848875, "acc_stderr,none": 0.02359885829286305}, "mmlu_prehistory": {"alias": "  - prehistory", "acc,none": 0.23148148148148148, "acc_stderr,none": 0.023468429832451163}, "mmlu_professional_law": {"alias": "  - professional_law", "acc,none": 0.2438070404172099, "acc_stderr,none": 0.010966507972178477}, "mmlu_world_religions": {"alias": "  - world_religions", "acc,none": 0.24561403508771928, "acc_stderr,none": 0.033014059469872487}, "mmlu_other": {"acc,none": 0.25040231734792406, "acc_stderr,none": 0.007764488883472086, "alias": " - other"}, "mmlu_business_ethics": {"alias": "  - business_ethics", "acc,none": 0.26, "acc_stderr,none": 0.04408440022768078}, "mmlu_clinical_knowledge": {"alias": "  - clinical_knowledge", "acc,none": 0.22641509433962265, "acc_stderr,none": 0.025757559893106737}, "mmlu_college_medicine": {"alias": "  - college_medicine", "acc,none": 0.2138728323699422, "acc_stderr,none": 0.03126511206173042}, "mmlu_global_facts": {"alias": "  - global_facts", "acc,none": 0.25, "acc_stderr,none": 0.04351941398892446}, "mmlu_human_aging": {"alias": "  - human_aging", "acc,none": 0.2825112107623318, "acc_stderr,none": 0.030216831011508773}, "mmlu_management": {"alias": "  - management", "acc,none": 0.24271844660194175, "acc_stderr,none": 0.042450224863844935}, "mmlu_marketing": {"alias": "  - marketing", "acc,none": 0.31196581196581197, "acc_stderr,none": 0.03035152732334493}, "mmlu_medical_genetics": {"alias": "  - medical_genetics", "acc,none": 0.31, "acc_stderr,none": 0.04648231987117316}, "mmlu_miscellaneous": {"alias": "  - miscellaneous", "acc,none": 0.24904214559386972, "acc_stderr,none": 0.015464676163395976}, "mmlu_nutrition": {"alias": "  - nutrition", "acc,none": 0.2679738562091503, "acc_stderr,none": 0.025360603796242557}, "mmlu_professional_accounting": {"alias": "  - professional_accounting", "acc,none": 0.24822695035460993, "acc_stderr,none": 0.025770015644290382}, "mmlu_professional_medicine": {"alias": "  - professional_medicine", "acc,none": 0.17279411764705882, "acc_stderr,none": 0.022966067585581774}, "mmlu_virology": {"alias": "  - virology", "acc,none": 0.26506024096385544, "acc_stderr,none": 0.03436024037944967}, "mmlu_social_sciences": {"acc,none": 0.2378940526486838, "acc_stderr,none": 0.007672235725845307, "alias": " - social sciences"}, "mmlu_econometrics": {"alias": "  - econometrics", "acc,none": 0.2631578947368421, "acc_stderr,none": 0.041424397194893596}, "mmlu_high_school_geography": {"alias": "  - high_school_geography", "acc,none": 0.2222222222222222, "acc_stderr,none": 0.02962022787479048}, "mmlu_high_school_government_and_politics": {"alias": "  - high_school_government_and_politics", "acc,none": 0.20725388601036268, "acc_stderr,none": 0.029252823291803624}, "mmlu_high_school_macroeconomics": {"alias": "  - high_school_macroeconomics", "acc,none": 0.2282051282051282, "acc_stderr,none": 0.021278393863586282}, "mmlu_high_school_microeconomics": {"alias": "  - high_school_microeconomics", "acc,none": 0.23109243697478993, "acc_stderr,none": 0.02738140692786896}, "mmlu_high_school_psychology": {"alias": "  - high_school_psychology", "acc,none": 0.22752293577981653, "acc_stderr,none": 0.017974463578776502}, "mmlu_human_sexuality": {"alias": "  - human_sexuality", "acc,none": 0.2824427480916031, "acc_stderr,none": 0.03948406125768361}, "mmlu_professional_psychology": {"alias": "  - professional_psychology", "acc,none": 0.2581699346405229, "acc_stderr,none": 0.017704531653250075}, "mmlu_public_relations": {"alias": "  - public_relations", "acc,none": 0.14545454545454545, "acc_stderr,none": 0.03376898319833081}, "mmlu_security_studies": {"alias": "  - security_studies", "acc,none": 0.23265306122448978, "acc_stderr,none": 0.02704925791589618}, "mmlu_sociology": {"alias": "  - sociology", "acc,none": 0.24378109452736318, "acc_stderr,none": 0.03036049015401466}, "mmlu_us_foreign_policy": {"alias": "  - us_foreign_policy", "acc,none": 0.33, "acc_stderr,none": 0.04725815626252606}, "mmlu_stem": {"acc,none": 0.23311132254995243, "acc_stderr,none": 0.007526325402992013, "alias": " - stem"}, "mmlu_abstract_algebra": {"alias": "  - abstract_algebra", "acc,none": 0.28, "acc_stderr,none": 0.04512608598542127}, "mmlu_anatomy": {"alias": "  - anatomy", "acc,none": 0.2, "acc_stderr,none": 0.03455473702325438}, "mmlu_astronomy": {"alias": "  - astronomy", "acc,none": 0.23026315789473684, "acc_stderr,none": 0.03426059424403164}, "mmlu_college_biology": {"alias": "  - college_biology", "acc,none": 0.2152777777777778, "acc_stderr,none": 0.03437079344106136}, "mmlu_college_chemistry": {"alias": "  - college_chemistry", "acc,none": 0.22, "acc_stderr,none": 0.041633319989322695}, "mmlu_college_computer_science": {"alias": "  - college_computer_science", "acc,none": 0.3, "acc_stderr,none": 0.046056618647183814}, "mmlu_college_mathematics": {"alias": "  - college_mathematics", "acc,none": 0.18, "acc_stderr,none": 0.038612291966536955}, "mmlu_college_physics": {"alias": "  - college_physics", "acc,none": 0.3137254901960784, "acc_stderr,none": 0.04617034827006718}, "mmlu_computer_security": {"alias": "  - computer_security", "acc,none": 0.25, "acc_stderr,none": 0.04351941398892446}, "mmlu_conceptual_physics": {"alias": "  - conceptual_physics", "acc,none": 0.251063829787234, "acc_stderr,none": 0.028346963777162466}, "mmlu_electrical_engineering": {"alias": "  - electrical_engineering", "acc,none": 0.31724137931034485, "acc_stderr,none": 0.038783523721386215}, "mmlu_elementary_mathematics": {"alias": "  - elementary_mathematics", "acc,none": 0.21693121693121692, "acc_stderr,none": 0.02122708244944504}, "mmlu_high_school_biology": {"alias": "  - high_school_biology", "acc,none": 0.23870967741935484, "acc_stderr,none": 0.024251071262208834}, "mmlu_high_school_chemistry": {"alias": "  - high_school_chemistry", "acc,none": 0.21182266009852216, "acc_stderr,none": 0.028748983689941072}, "mmlu_high_school_computer_science": {"alias": "  - high_school_computer_science", "acc,none": 0.19, "acc_stderr,none": 0.03942772444036623}, "mmlu_high_school_mathematics": {"alias": "  - high_school_mathematics", "acc,none": 0.24814814814814815, "acc_stderr,none": 0.026335739404055803}, "mmlu_high_school_physics": {"alias": "  - high_school_physics", "acc,none": 0.18543046357615894, "acc_stderr,none": 0.03173284384294285}, "mmlu_high_school_statistics": {"alias": "  - high_school_statistics", "acc,none": 0.21296296296296297, "acc_stderr,none": 0.027920963147993666}, "mmlu_machine_learning": {"alias": "  - machine_learning", "acc,none": 0.20535714285714285, "acc_stderr,none": 0.038342410214190714}, "truthfulqa_mc1": {"alias": "truthfulqa_mc1", "acc,none": 0.2484700122399021, "acc_stderr,none": 0.015127427096520688}, "winogrande": {"alias": "winogrande", "acc,none": 0.49013417521704816, "acc_stderr,none": 0.014049749833367596}, "xwinograd": {"acc,none": 0.5093279388626658, "acc_stderr,none": 0.00749733703073405, "alias": "xwinograd"}, "xwinograd_en": {"alias": " - xwinograd_en", "acc,none": 0.5109677419354839, "acc_stderr,none": 0.01036925210700036}, "xwinograd_fr": {"alias": " - xwinograd_fr", "acc,none": 0.5301204819277109, "acc_stderr,none": 0.05511548370029596}, "xwinograd_jp": {"alias": " - xwinograd_jp", "acc,none": 0.5182481751824818, "acc_stderr,none": 0.016143504549715065}, "xwinograd_pt": {"alias": " - xwinograd_pt", "acc,none": 0.5171102661596958, "acc_stderr,none": 0.030872011014694032}, "xwinograd_ru": {"alias": " - xwinograd_ru", "acc,none": 0.5142857142857142, "acc_stderr,none": 0.0282051130549725}, "xwinograd_zh": {"alias": " - xwinograd_zh", "acc,none": 0.4742063492063492, "acc_stderr,none": 0.022264213526057265}}, "groups": {"mmlu": {"acc,none": 0.241917105825381, "acc_stderr,none": 0.003612722551674195, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.24484590860786398, "acc_stderr,none": 0.006272807466303847, "alias": " - humanities"}, "mmlu_other": {"acc,none": 0.25040231734792406, "acc_stderr,none": 0.007764488883472086, "alias": " - other"}, "mmlu_social_sciences": {"acc,none": 0.2378940526486838, "acc_stderr,none": 0.007672235725845307, "alias": " - social sciences"}, "mmlu_stem": {"acc,none": 0.23311132254995243, "acc_stderr,none": 0.007526325402992013, "alias": " - stem"}, "xwinograd": {"acc,none": 0.5093279388626658, "acc_stderr,none": 0.00749733703073405, "alias": "xwinograd"}}, "group_subtasks": {"mmlu_humanities": ["mmlu_moral_disputes", "mmlu_world_religions", "mmlu_high_school_european_history", "mmlu_moral_scenarios", "mmlu_international_law", "mmlu_logical_fallacies", "mmlu_high_school_us_history", "mmlu_high_school_world_history", "mmlu_prehistory", "mmlu_jurisprudence", "mmlu_formal_logic", "mmlu_professional_law", "mmlu_philosophy"], "mmlu_social_sciences": ["mmlu_public_relations", "mmlu_us_foreign_policy", "mmlu_high_school_geography", "mmlu_high_school_macroeconomics", "mmlu_security_studies", "mmlu_sociology", "mmlu_econometrics", "mmlu_high_school_government_and_politics", "mmlu_professional_psychology", "mmlu_high_school_psychology", "mmlu_high_school_microeconomics", "mmlu_human_sexuality"], "mmlu_other": ["mmlu_management", "mmlu_miscellaneous", "mmlu_human_aging", "mmlu_clinical_knowledge", "mmlu_marketing", "mmlu_college_medicine", "mmlu_nutrition", "mmlu_professional_medicine", "mmlu_professional_accounting", "mmlu_business_ethics", "mmlu_medical_genetics", "mmlu_virology", "mmlu_global_facts"], "mmlu_stem": ["mmlu_college_chemistry", "mmlu_college_mathematics", "mmlu_machine_learning", "mmlu_elementary_mathematics", "mmlu_anatomy", "mmlu_conceptual_physics", "mmlu_college_biology", "mmlu_abstract_algebra", "mmlu_electrical_engineering", "mmlu_high_school_statistics", "mmlu_high_school_chemistry", "mmlu_computer_security", "mmlu_high_school_biology", "mmlu_college_physics", "mmlu_high_school_physics", "mmlu_astronomy", "mmlu_college_computer_science", "mmlu_high_school_mathematics", "mmlu_high_school_computer_science"], "mmlu": ["mmlu_stem", "mmlu_other", "mmlu_social_sciences", "mmlu_humanities"], "hellaswag": [], "xwinograd": ["xwinograd_en", "xwinograd_fr", "xwinograd_jp", "xwinograd_pt", "xwinograd_ru", "xwinograd_zh"], "winogrande": [], "truthfulqa_mc1": [], "arc_challenge": [], "gsm8k": []}}