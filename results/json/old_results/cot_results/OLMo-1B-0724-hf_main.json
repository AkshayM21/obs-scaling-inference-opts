{"results": {"arc_challenge_cot": {"alias": "arc_challenge_cot", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "gsm8k_cot_zeroshot": {"alias": "gsm8k_cot_zeroshot", "exact_match,strict-match": 0.058377558756633814, "exact_match_stderr,strict-match": 0.006458083557832457, "exact_match,flexible-extract": 0.04169825625473844, "exact_match_stderr,flexible-extract": 0.005506205058175763}, "hellaswag_cot": {"alias": "hellaswag_cot", "exact_match,strict-match": 0.024497112129057957, "exact_match_stderr,strict-match": 0.0015427062578427736, "exact_match,flexible-extract": 0.058852818163712406, "exact_match_stderr,flexible-extract": 0.0023486803885506553}, "mmlu_flan_cot_zeroshot": {"alias": "mmlu (flan style, zeroshot cot)"}, "humanities": {"alias": " - humanities"}, "mmlu_flan_cot_zeroshot_formal_logic": {"alias": "  - mmlu_flan_cot_zeroshot_formal_logic", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_high_school_european_history": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_european_history", "exact_match,strict-match": 0.1111111111111111, "exact_match_stderr,strict-match": 0.07622159339667062, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_high_school_us_history": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_us_history", "exact_match,strict-match": 0.18181818181818182, "exact_match_stderr,strict-match": 0.08416546361568647, "exact_match,flexible-extract": 0.22727272727272727, "exact_match_stderr,flexible-extract": 0.09144861547306321}, "mmlu_flan_cot_zeroshot_high_school_world_history": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_world_history", "exact_match,strict-match": 0.11538461538461539, "exact_match_stderr,strict-match": 0.06389710663783135, "exact_match,flexible-extract": 0.11538461538461539, "exact_match_stderr,flexible-extract": 0.06389710663783135}, "mmlu_flan_cot_zeroshot_international_law": {"alias": "  - mmlu_flan_cot_zeroshot_international_law", "exact_match,strict-match": 0.23076923076923078, "exact_match_stderr,strict-match": 0.12162606385262997, "exact_match,flexible-extract": 0.23076923076923078, "exact_match_stderr,flexible-extract": 0.12162606385262997}, "mmlu_flan_cot_zeroshot_jurisprudence": {"alias": "  - mmlu_flan_cot_zeroshot_jurisprudence", "exact_match,strict-match": 0.18181818181818182, "exact_match_stderr,strict-match": 0.12196734422726126, "exact_match,flexible-extract": 0.18181818181818182, "exact_match_stderr,flexible-extract": 0.12196734422726126}, "mmlu_flan_cot_zeroshot_logical_fallacies": {"alias": "  - mmlu_flan_cot_zeroshot_logical_fallacies", "exact_match,strict-match": 0.16666666666666666, "exact_match_stderr,strict-match": 0.0903876907577734, "exact_match,flexible-extract": 0.1111111111111111, "exact_match_stderr,flexible-extract": 0.0762215933966706}, "mmlu_flan_cot_zeroshot_moral_disputes": {"alias": "  - mmlu_flan_cot_zeroshot_moral_disputes", "exact_match,strict-match": 0.07894736842105263, "exact_match_stderr,strict-match": 0.04433127181293721, "exact_match,flexible-extract": 0.10526315789473684, "exact_match_stderr,flexible-extract": 0.05045276771256955}, "mmlu_flan_cot_zeroshot_moral_scenarios": {"alias": "  - mmlu_flan_cot_zeroshot_moral_scenarios", "exact_match,strict-match": 0.11, "exact_match_stderr,strict-match": 0.031446603773522035, "exact_match,flexible-extract": 0.17, "exact_match_stderr,flexible-extract": 0.03775251680686371}, "mmlu_flan_cot_zeroshot_philosophy": {"alias": "  - mmlu_flan_cot_zeroshot_philosophy", "exact_match,strict-match": 0.20588235294117646, "exact_match_stderr,strict-match": 0.07038741487212932, "exact_match,flexible-extract": 0.17647058823529413, "exact_match_stderr,flexible-extract": 0.0663618911550313}, "mmlu_flan_cot_zeroshot_prehistory": {"alias": "  - mmlu_flan_cot_zeroshot_prehistory", "exact_match,strict-match": 0.14285714285714285, "exact_match_stderr,strict-match": 0.06001200360120041, "exact_match,flexible-extract": 0.2, "exact_match_stderr,flexible-extract": 0.06859943405700356}, "mmlu_flan_cot_zeroshot_professional_law": {"alias": "  - mmlu_flan_cot_zeroshot_professional_law", "exact_match,strict-match": 0.12352941176470589, "exact_match_stderr,strict-match": 0.02531107017365972, "exact_match,flexible-extract": 0.1588235294117647, "exact_match_stderr,flexible-extract": 0.0281162649464362}, "mmlu_flan_cot_zeroshot_world_religions": {"alias": "  - mmlu_flan_cot_zeroshot_world_religions", "exact_match,strict-match": 0.21052631578947367, "exact_match_stderr,strict-match": 0.0960916767552923, "exact_match,flexible-extract": 0.2631578947368421, "exact_match_stderr,flexible-extract": 0.10379087338771256}, "other": {"alias": " - other"}, "mmlu_flan_cot_zeroshot_business_ethics": {"alias": "  - mmlu_flan_cot_zeroshot_business_ethics", "exact_match,strict-match": 0.18181818181818182, "exact_match_stderr,strict-match": 0.12196734422726124, "exact_match,flexible-extract": 0.2727272727272727, "exact_match_stderr,flexible-extract": 0.14083575804390605}, "mmlu_flan_cot_zeroshot_clinical_knowledge": {"alias": "  - mmlu_flan_cot_zeroshot_clinical_knowledge", "exact_match,strict-match": 0.13793103448275862, "exact_match_stderr,strict-match": 0.06516628844986677, "exact_match,flexible-extract": 0.2413793103448276, "exact_match_stderr,flexible-extract": 0.080869237238335}, "mmlu_flan_cot_zeroshot_college_medicine": {"alias": "  - mmlu_flan_cot_zeroshot_college_medicine", "exact_match,strict-match": 0.22727272727272727, "exact_match_stderr,strict-match": 0.09144861547306321, "exact_match,flexible-extract": 0.2727272727272727, "exact_match_stderr,flexible-extract": 0.09718590614997252}, "mmlu_flan_cot_zeroshot_global_facts": {"alias": "  - mmlu_flan_cot_zeroshot_global_facts", "exact_match,strict-match": 0.1, "exact_match_stderr,strict-match": 0.09999999999999999, "exact_match,flexible-extract": 0.2, "exact_match_stderr,flexible-extract": 0.13333333333333333}, "mmlu_flan_cot_zeroshot_human_aging": {"alias": "  - mmlu_flan_cot_zeroshot_human_aging", "exact_match,strict-match": 0.17391304347826086, "exact_match_stderr,strict-match": 0.08081046758996392, "exact_match,flexible-extract": 0.21739130434782608, "exact_match_stderr,flexible-extract": 0.08793911249520547}, "mmlu_flan_cot_zeroshot_management": {"alias": "  - mmlu_flan_cot_zeroshot_management", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.2727272727272727, "exact_match_stderr,flexible-extract": 0.14083575804390605}, "mmlu_flan_cot_zeroshot_marketing": {"alias": "  - mmlu_flan_cot_zeroshot_marketing", "exact_match,strict-match": 0.2, "exact_match_stderr,strict-match": 0.08164965809277261, "exact_match,flexible-extract": 0.32, "exact_match_stderr,flexible-extract": 0.09521904571390466}, "mmlu_flan_cot_zeroshot_medical_genetics": {"alias": "  - mmlu_flan_cot_zeroshot_medical_genetics", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.18181818181818182, "exact_match_stderr,flexible-extract": 0.12196734422726124}, "mmlu_flan_cot_zeroshot_miscellaneous": {"alias": "  - mmlu_flan_cot_zeroshot_miscellaneous", "exact_match,strict-match": 0.22093023255813954, "exact_match_stderr,strict-match": 0.04499935488425769, "exact_match,flexible-extract": 0.1744186046511628, "exact_match_stderr,flexible-extract": 0.04115919667121857}, "mmlu_flan_cot_zeroshot_nutrition": {"alias": "  - mmlu_flan_cot_zeroshot_nutrition", "exact_match,strict-match": 0.15151515151515152, "exact_match_stderr,strict-match": 0.06338333534349057, "exact_match,flexible-extract": 0.30303030303030304, "exact_match_stderr,flexible-extract": 0.08124094920275461}, "mmlu_flan_cot_zeroshot_professional_accounting": {"alias": "  - mmlu_flan_cot_zeroshot_professional_accounting", "exact_match,strict-match": 0.03225806451612903, "exact_match_stderr,strict-match": 0.03225806451612904, "exact_match,flexible-extract": 0.06451612903225806, "exact_match_stderr,flexible-extract": 0.04485301852605206}, "mmlu_flan_cot_zeroshot_professional_medicine": {"alias": "  - mmlu_flan_cot_zeroshot_professional_medicine", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_virology": {"alias": "  - mmlu_flan_cot_zeroshot_virology", "exact_match,strict-match": 0.05555555555555555, "exact_match_stderr,strict-match": 0.05555555555555556, "exact_match,flexible-extract": 0.05555555555555555, "exact_match_stderr,flexible-extract": 0.05555555555555556}, "social sciences": {"alias": " - social sciences"}, "mmlu_flan_cot_zeroshot_econometrics": {"alias": "  - mmlu_flan_cot_zeroshot_econometrics", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_high_school_geography": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_geography", "exact_match,strict-match": 0.045454545454545456, "exact_match_stderr,strict-match": 0.04545454545454547, "exact_match,flexible-extract": 0.09090909090909091, "exact_match_stderr,flexible-extract": 0.06273323266748675}, "mmlu_flan_cot_zeroshot_high_school_government_and_politics": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_government_and_politics", "exact_match,strict-match": 0.23809523809523808, "exact_match_stderr,strict-match": 0.09523809523809523, "exact_match,flexible-extract": 0.2857142857142857, "exact_match_stderr,flexible-extract": 0.10101525445522108}, "mmlu_flan_cot_zeroshot_high_school_macroeconomics": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_macroeconomics", "exact_match,strict-match": 0.06976744186046512, "exact_match_stderr,strict-match": 0.03930950021993103, "exact_match,flexible-extract": 0.13953488372093023, "exact_match_stderr,flexible-extract": 0.053466700795452085}, "mmlu_flan_cot_zeroshot_high_school_microeconomics": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_microeconomics", "exact_match,strict-match": 0.038461538461538464, "exact_match_stderr,strict-match": 0.03846153846153845, "exact_match,flexible-extract": 0.11538461538461539, "exact_match_stderr,flexible-extract": 0.06389710663783135}, "mmlu_flan_cot_zeroshot_high_school_psychology": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_psychology", "exact_match,strict-match": 0.21666666666666667, "exact_match_stderr,strict-match": 0.05363439040648222, "exact_match,flexible-extract": 0.21666666666666667, "exact_match_stderr,flexible-extract": 0.0536343904064822}, "mmlu_flan_cot_zeroshot_human_sexuality": {"alias": "  - mmlu_flan_cot_zeroshot_human_sexuality", "exact_match,strict-match": 0.08333333333333333, "exact_match_stderr,strict-match": 0.08333333333333333, "exact_match,flexible-extract": 0.25, "exact_match_stderr,flexible-extract": 0.1305582419667734}, "mmlu_flan_cot_zeroshot_professional_psychology": {"alias": "  - mmlu_flan_cot_zeroshot_professional_psychology", "exact_match,strict-match": 0.11594202898550725, "exact_match_stderr,strict-match": 0.03882454402978922, "exact_match,flexible-extract": 0.2028985507246377, "exact_match_stderr,flexible-extract": 0.0487687714747266}, "mmlu_flan_cot_zeroshot_public_relations": {"alias": "  - mmlu_flan_cot_zeroshot_public_relations", "exact_match,strict-match": 0.16666666666666666, "exact_match_stderr,strict-match": 0.11236664374387369, "exact_match,flexible-extract": 0.16666666666666666, "exact_match_stderr,flexible-extract": 0.11236664374387369}, "mmlu_flan_cot_zeroshot_security_studies": {"alias": "  - mmlu_flan_cot_zeroshot_security_studies", "exact_match,strict-match": 0.07407407407407407, "exact_match_stderr,strict-match": 0.051361129280113826, "exact_match,flexible-extract": 0.14814814814814814, "exact_match_stderr,flexible-extract": 0.06966962541673782}, "mmlu_flan_cot_zeroshot_sociology": {"alias": "  - mmlu_flan_cot_zeroshot_sociology", "exact_match,strict-match": 0.3181818181818182, "exact_match_stderr,strict-match": 0.10163945352271771, "exact_match,flexible-extract": 0.2727272727272727, "exact_match_stderr,flexible-extract": 0.0971859061499725}, "mmlu_flan_cot_zeroshot_us_foreign_policy": {"alias": "  - mmlu_flan_cot_zeroshot_us_foreign_policy", "exact_match,strict-match": 0.09090909090909091, "exact_match_stderr,strict-match": 0.0909090909090909, "exact_match,flexible-extract": 0.18181818181818182, "exact_match_stderr,flexible-extract": 0.12196734422726124}, "stem": {"alias": " - stem"}, "mmlu_flan_cot_zeroshot_abstract_algebra": {"alias": "  - mmlu_flan_cot_zeroshot_abstract_algebra", "exact_match,strict-match": 0.09090909090909091, "exact_match_stderr,strict-match": 0.0909090909090909, "exact_match,flexible-extract": 0.09090909090909091, "exact_match_stderr,flexible-extract": 0.0909090909090909}, "mmlu_flan_cot_zeroshot_anatomy": {"alias": "  - mmlu_flan_cot_zeroshot_anatomy", "exact_match,strict-match": 0.07142857142857142, "exact_match_stderr,strict-match": 0.07142857142857142, "exact_match,flexible-extract": 0.07142857142857142, "exact_match_stderr,flexible-extract": 0.07142857142857142}, "mmlu_flan_cot_zeroshot_astronomy": {"alias": "  - mmlu_flan_cot_zeroshot_astronomy", "exact_match,strict-match": 0.0625, "exact_match_stderr,strict-match": 0.0625, "exact_match,flexible-extract": 0.0625, "exact_match_stderr,flexible-extract": 0.0625}, "mmlu_flan_cot_zeroshot_college_biology": {"alias": "  - mmlu_flan_cot_zeroshot_college_biology", "exact_match,strict-match": 0.25, "exact_match_stderr,strict-match": 0.11180339887498948, "exact_match,flexible-extract": 0.375, "exact_match_stderr,flexible-extract": 0.125}, "mmlu_flan_cot_zeroshot_college_chemistry": {"alias": "  - mmlu_flan_cot_zeroshot_college_chemistry", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_college_computer_science": {"alias": "  - mmlu_flan_cot_zeroshot_college_computer_science", "exact_match,strict-match": 0.09090909090909091, "exact_match_stderr,strict-match": 0.0909090909090909, "exact_match,flexible-extract": 0.18181818181818182, "exact_match_stderr,flexible-extract": 0.12196734422726123}, "mmlu_flan_cot_zeroshot_college_mathematics": {"alias": "  - mmlu_flan_cot_zeroshot_college_mathematics", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_college_physics": {"alias": "  - mmlu_flan_cot_zeroshot_college_physics", "exact_match,strict-match": 0.18181818181818182, "exact_match_stderr,strict-match": 0.12196734422726123, "exact_match,flexible-extract": 0.18181818181818182, "exact_match_stderr,flexible-extract": 0.12196734422726123}, "mmlu_flan_cot_zeroshot_computer_security": {"alias": "  - mmlu_flan_cot_zeroshot_computer_security", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.18181818181818182, "exact_match_stderr,flexible-extract": 0.12196734422726124}, "mmlu_flan_cot_zeroshot_conceptual_physics": {"alias": "  - mmlu_flan_cot_zeroshot_conceptual_physics", "exact_match,strict-match": 0.15384615384615385, "exact_match_stderr,strict-match": 0.072160242458822, "exact_match,flexible-extract": 0.19230769230769232, "exact_match_stderr,flexible-extract": 0.0788226981996892}, "mmlu_flan_cot_zeroshot_electrical_engineering": {"alias": "  - mmlu_flan_cot_zeroshot_electrical_engineering", "exact_match,strict-match": 0.1875, "exact_match_stderr,strict-match": 0.10077822185373188, "exact_match,flexible-extract": 0.3125, "exact_match_stderr,flexible-extract": 0.11967838846954226}, "mmlu_flan_cot_zeroshot_elementary_mathematics": {"alias": "  - mmlu_flan_cot_zeroshot_elementary_mathematics", "exact_match,strict-match": 0.0975609756097561, "exact_match_stderr,strict-match": 0.04691557088212527, "exact_match,flexible-extract": 0.17073170731707318, "exact_match_stderr,flexible-extract": 0.059494199598294974}, "mmlu_flan_cot_zeroshot_high_school_biology": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_biology", "exact_match,strict-match": 0.09375, "exact_match_stderr,strict-match": 0.0523514603733822, "exact_match,flexible-extract": 0.09375, "exact_match_stderr,flexible-extract": 0.0523514603733822}, "mmlu_flan_cot_zeroshot_high_school_chemistry": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_chemistry", "exact_match,strict-match": 0.045454545454545456, "exact_match_stderr,strict-match": 0.045454545454545456, "exact_match,flexible-extract": 0.13636363636363635, "exact_match_stderr,flexible-extract": 0.07488677009526491}, "mmlu_flan_cot_zeroshot_high_school_computer_science": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_computer_science", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_high_school_mathematics": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_mathematics", "exact_match,strict-match": 0.10344827586206896, "exact_match_stderr,strict-match": 0.05755330761353655, "exact_match,flexible-extract": 0.20689655172413793, "exact_match_stderr,flexible-extract": 0.07655305550699533}, "mmlu_flan_cot_zeroshot_high_school_physics": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_physics", "exact_match,strict-match": 0.11764705882352941, "exact_match_stderr,strict-match": 0.08054743492723031, "exact_match,flexible-extract": 0.17647058823529413, "exact_match_stderr,flexible-extract": 0.0953050102707038}, "mmlu_flan_cot_zeroshot_high_school_statistics": {"alias": "  - mmlu_flan_cot_zeroshot_high_school_statistics", "exact_match,strict-match": 0.043478260869565216, "exact_match_stderr,strict-match": 0.04347826086956522, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "mmlu_flan_cot_zeroshot_machine_learning": {"alias": "  - mmlu_flan_cot_zeroshot_machine_learning", "exact_match,strict-match": 0.09090909090909091, "exact_match_stderr,strict-match": 0.0909090909090909, "exact_match,flexible-extract": 0.18181818181818182, "exact_match_stderr,flexible-extract": 0.12196734422726124}, "truthfulqa_cot": {"alias": "truthfulqa_cot", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.0, "exact_match_stderr,flexible-extract": 0.0}, "winogrande_cot": {"alias": "winogrande_cot", "exact_match,strict-match": 0.07419100236779795, "exact_match_stderr,strict-match": 0.007365792243042235, "exact_match,flexible-extract": 0.07576953433307025, "exact_match_stderr,flexible-extract": 0.007437390699079782}, "xwinograd_cot": {"alias": "xwinograd_cot"}, "xwinograd_cot_en": {"alias": " - xwinograd_cot_en", "exact_match,strict-match": 0.0989247311827957, "exact_match_stderr,strict-match": 0.006193197254269946, "exact_match,flexible-extract": 0.11010752688172043, "exact_match_stderr,flexible-extract": 0.006493206412900214}, "xwinograd_cot_fr": {"alias": " - xwinograd_cot_fr", "exact_match,strict-match": 0.14457831325301204, "exact_match_stderr,strict-match": 0.038836025610803654, "exact_match,flexible-extract": 0.14457831325301204, "exact_match_stderr,flexible-extract": 0.038836025610803654}, "xwinograd_cot_jp": {"alias": " - xwinograd_cot_jp", "exact_match,strict-match": 0.056308654848800835, "exact_match_stderr,strict-match": 0.007447659412763028, "exact_match,flexible-extract": 0.07716371220020855, "exact_match_stderr,flexible-extract": 0.00862156553021239}, "xwinograd_cot_pt": {"alias": " - xwinograd_cot_pt", "exact_match,strict-match": 0.12547528517110265, "exact_match_stderr,strict-match": 0.02046512934277563, "exact_match,flexible-extract": 0.13688212927756654, "exact_match_stderr,flexible-extract": 0.02123526988003422}, "xwinograd_cot_ru": {"alias": " - xwinograd_cot_ru", "exact_match,strict-match": 0.08888888888888889, "exact_match_stderr,strict-match": 0.01605996469455663, "exact_match,flexible-extract": 0.10158730158730159, "exact_match_stderr,flexible-extract": 0.01704876125499024}, "xwinograd_cot_zh": {"alias": " - xwinograd_cot_zh", "exact_match,strict-match": 0.001984126984126984, "exact_match_stderr,strict-match": 0.0019841269841269923, "exact_match,flexible-extract": 0.003968253968253968, "exact_match_stderr,flexible-extract": 0.0028031886589115033}}, "groups": {"mmlu_flan_cot_zeroshot": {"alias": "mmlu (flan style, zeroshot cot)"}, "humanities": {"alias": " - humanities"}, "other": {"alias": " - other"}, "social sciences": {"alias": " - social sciences"}, "stem": {"alias": " - stem"}, "xwinograd_cot": {"alias": "xwinograd_cot"}}, "group_subtasks": {"hellaswag_cot": [], "arc_challenge_cot": [], "truthfulqa_cot": [], "xwinograd_cot": ["xwinograd_cot_en", "xwinograd_cot_fr", "xwinograd_cot_jp", "xwinograd_cot_pt", "xwinograd_cot_ru", "xwinograd_cot_zh"], "winogrande_cot": [], "gsm8k_cot_zeroshot": [], "humanities": ["mmlu_flan_cot_zeroshot_high_school_european_history", "mmlu_flan_cot_zeroshot_moral_scenarios", "mmlu_flan_cot_zeroshot_jurisprudence", "mmlu_flan_cot_zeroshot_logical_fallacies", "mmlu_flan_cot_zeroshot_philosophy", "mmlu_flan_cot_zeroshot_moral_disputes", "mmlu_flan_cot_zeroshot_world_religions", "mmlu_flan_cot_zeroshot_international_law", "mmlu_flan_cot_zeroshot_high_school_world_history", "mmlu_flan_cot_zeroshot_high_school_us_history", "mmlu_flan_cot_zeroshot_prehistory", "mmlu_flan_cot_zeroshot_professional_law", "mmlu_flan_cot_zeroshot_formal_logic"], "social sciences": ["mmlu_flan_cot_zeroshot_public_relations", "mmlu_flan_cot_zeroshot_professional_psychology", "mmlu_flan_cot_zeroshot_high_school_microeconomics", "mmlu_flan_cot_zeroshot_human_sexuality", "mmlu_flan_cot_zeroshot_high_school_macroeconomics", "mmlu_flan_cot_zeroshot_security_studies", "mmlu_flan_cot_zeroshot_high_school_geography", "mmlu_flan_cot_zeroshot_econometrics", "mmlu_flan_cot_zeroshot_high_school_government_and_politics", "mmlu_flan_cot_zeroshot_high_school_psychology", "mmlu_flan_cot_zeroshot_sociology", "mmlu_flan_cot_zeroshot_us_foreign_policy"], "other": ["mmlu_flan_cot_zeroshot_management", "mmlu_flan_cot_zeroshot_college_medicine", "mmlu_flan_cot_zeroshot_professional_medicine", "mmlu_flan_cot_zeroshot_global_facts", "mmlu_flan_cot_zeroshot_miscellaneous", "mmlu_flan_cot_zeroshot_clinical_knowledge", "mmlu_flan_cot_zeroshot_virology", "mmlu_flan_cot_zeroshot_business_ethics", "mmlu_flan_cot_zeroshot_nutrition", "mmlu_flan_cot_zeroshot_human_aging", "mmlu_flan_cot_zeroshot_medical_genetics", "mmlu_flan_cot_zeroshot_professional_accounting", "mmlu_flan_cot_zeroshot_marketing"], "stem": ["mmlu_flan_cot_zeroshot_high_school_physics", "mmlu_flan_cot_zeroshot_computer_security", "mmlu_flan_cot_zeroshot_high_school_computer_science", "mmlu_flan_cot_zeroshot_college_computer_science", "mmlu_flan_cot_zeroshot_electrical_engineering", "mmlu_flan_cot_zeroshot_high_school_chemistry", "mmlu_flan_cot_zeroshot_astronomy", "mmlu_flan_cot_zeroshot_machine_learning", "mmlu_flan_cot_zeroshot_high_school_mathematics", "mmlu_flan_cot_zeroshot_college_physics", "mmlu_flan_cot_zeroshot_college_chemistry", "mmlu_flan_cot_zeroshot_elementary_mathematics", "mmlu_flan_cot_zeroshot_abstract_algebra", "mmlu_flan_cot_zeroshot_anatomy", "mmlu_flan_cot_zeroshot_college_mathematics", "mmlu_flan_cot_zeroshot_college_biology", "mmlu_flan_cot_zeroshot_conceptual_physics", "mmlu_flan_cot_zeroshot_high_school_statistics", "mmlu_flan_cot_zeroshot_high_school_biology"], "mmlu_flan_cot_zeroshot": ["stem", "other", "social sciences", "humanities"]}}