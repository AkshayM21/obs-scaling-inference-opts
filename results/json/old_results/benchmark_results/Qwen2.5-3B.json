{"results": {"arc_challenge": {"alias": "arc_challenge", "acc,none": 0.4453924914675768, "acc_stderr,none": 0.014523987638344074, "acc_norm,none": 0.47013651877133106, "acc_norm_stderr,none": 0.014585305840007102}, "gsm8k": {"alias": "gsm8k", "exact_match,strict-match": 0.6990144048521607, "exact_match_stderr,strict-match": 0.012634504465211187, "exact_match,flexible-extract": 0.7573919636087946, "exact_match_stderr,flexible-extract": 0.011807426004596859}, "hellaswag": {"alias": "hellaswag", "acc,none": 0.5501892053375822, "acc_stderr,none": 0.004964579685712445, "acc_norm,none": 0.7369049990041824, "acc_norm_stderr,none": 0.004394136724173006}, "mmlu": {"acc,none": 0.6514741489816266, "acc_stderr,none": 0.003775763817548032, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.5666312433581296, "acc_stderr,none": 0.00659404435419182, "alias": " - humanities"}, "mmlu_formal_logic": {"alias": "  - formal_logic", "acc,none": 0.48412698412698413, "acc_stderr,none": 0.04469881854072606}, "mmlu_high_school_european_history": {"alias": "  - high_school_european_history", "acc,none": 0.7636363636363637, "acc_stderr,none": 0.033175059300091805}, "mmlu_high_school_us_history": {"alias": "  - high_school_us_history", "acc,none": 0.8333333333333334, "acc_stderr,none": 0.026156867523931055}, "mmlu_high_school_world_history": {"alias": "  - high_school_world_history", "acc,none": 0.8227848101265823, "acc_stderr,none": 0.02485636418450321}, "mmlu_international_law": {"alias": "  - international_law", "acc,none": 0.8264462809917356, "acc_stderr,none": 0.03457272836917669}, "mmlu_jurisprudence": {"alias": "  - jurisprudence", "acc,none": 0.8333333333333334, "acc_stderr,none": 0.036028141763926456}, "mmlu_logical_fallacies": {"alias": "  - logical_fallacies", "acc,none": 0.7975460122699386, "acc_stderr,none": 0.03157065078911899}, "mmlu_moral_disputes": {"alias": "  - moral_disputes", "acc,none": 0.6763005780346821, "acc_stderr,none": 0.025190181327608422}, "mmlu_moral_scenarios": {"alias": "  - moral_scenarios", "acc,none": 0.2435754189944134, "acc_stderr,none": 0.01435591196476787}, "mmlu_philosophy": {"alias": "  - philosophy", "acc,none": 0.7106109324758842, "acc_stderr,none": 0.025755865922632945}, "mmlu_prehistory": {"alias": "  - prehistory", "acc,none": 0.7253086419753086, "acc_stderr,none": 0.024836057868294674}, "mmlu_professional_law": {"alias": "  - professional_law", "acc,none": 0.48565840938722293, "acc_stderr,none": 0.012764981829524272}, "mmlu_world_religions": {"alias": "  - world_religions", "acc,none": 0.8245614035087719, "acc_stderr,none": 0.029170885500727682}, "mmlu_other": {"acc,none": 0.7135500482780818, "acc_stderr,none": 0.007829512899144466, "alias": " - other"}, "mmlu_business_ethics": {"alias": "  - business_ethics", "acc,none": 0.73, "acc_stderr,none": 0.044619604333847394}, "mmlu_clinical_knowledge": {"alias": "  - clinical_knowledge", "acc,none": 0.7283018867924528, "acc_stderr,none": 0.02737770662467071}, "mmlu_college_medicine": {"alias": "  - college_medicine", "acc,none": 0.6647398843930635, "acc_stderr,none": 0.035995863012470784}, "mmlu_global_facts": {"alias": "  - global_facts", "acc,none": 0.36, "acc_stderr,none": 0.04824181513244218}, "mmlu_human_aging": {"alias": "  - human_aging", "acc,none": 0.7399103139013453, "acc_stderr,none": 0.029442495585857483}, "mmlu_management": {"alias": "  - management", "acc,none": 0.8155339805825242, "acc_stderr,none": 0.03840423627288276}, "mmlu_marketing": {"alias": "  - marketing", "acc,none": 0.8846153846153846, "acc_stderr,none": 0.020930193185179333}, "mmlu_medical_genetics": {"alias": "  - medical_genetics", "acc,none": 0.74, "acc_stderr,none": 0.04408440022768079}, "mmlu_miscellaneous": {"alias": "  - miscellaneous", "acc,none": 0.7982120051085568, "acc_stderr,none": 0.014351702181636863}, "mmlu_nutrition": {"alias": "  - nutrition", "acc,none": 0.7679738562091504, "acc_stderr,none": 0.024170840879340863}, "mmlu_professional_accounting": {"alias": "  - professional_accounting", "acc,none": 0.5177304964539007, "acc_stderr,none": 0.02980873964223777}, "mmlu_professional_medicine": {"alias": "  - professional_medicine", "acc,none": 0.6544117647058824, "acc_stderr,none": 0.02888819310398863}, "mmlu_virology": {"alias": "  - virology", "acc,none": 0.5180722891566265, "acc_stderr,none": 0.03889951252827216}, "mmlu_social_sciences": {"acc,none": 0.7630809229769255, "acc_stderr,none": 0.0075435965326657445, "alias": " - social sciences"}, "mmlu_econometrics": {"alias": "  - econometrics", "acc,none": 0.5175438596491229, "acc_stderr,none": 0.04700708033551038}, "mmlu_high_school_geography": {"alias": "  - high_school_geography", "acc,none": 0.7878787878787878, "acc_stderr,none": 0.029126522834586815}, "mmlu_high_school_government_and_politics": {"alias": "  - high_school_government_and_politics", "acc,none": 0.8601036269430051, "acc_stderr,none": 0.025033870583015184}, "mmlu_high_school_macroeconomics": {"alias": "  - high_school_macroeconomics", "acc,none": 0.6820512820512821, "acc_stderr,none": 0.02361088430892786}, "mmlu_high_school_microeconomics": {"alias": "  - high_school_microeconomics", "acc,none": 0.8025210084033614, "acc_stderr,none": 0.025859164122051463}, "mmlu_high_school_psychology": {"alias": "  - high_school_psychology", "acc,none": 0.8495412844036697, "acc_stderr,none": 0.015328563932669235}, "mmlu_human_sexuality": {"alias": "  - human_sexuality", "acc,none": 0.7557251908396947, "acc_stderr,none": 0.03768335959728744}, "mmlu_professional_psychology": {"alias": "  - professional_psychology", "acc,none": 0.7173202614379085, "acc_stderr,none": 0.018217269552053435}, "mmlu_public_relations": {"alias": "  - public_relations", "acc,none": 0.7, "acc_stderr,none": 0.04389311454644287}, "mmlu_security_studies": {"alias": "  - security_studies", "acc,none": 0.7306122448979592, "acc_stderr,none": 0.02840125202902294}, "mmlu_sociology": {"alias": "  - sociology", "acc,none": 0.8407960199004975, "acc_stderr,none": 0.02587064676616913}, "mmlu_us_foreign_policy": {"alias": "  - us_foreign_policy", "acc,none": 0.84, "acc_stderr,none": 0.03684529491774709}, "mmlu_stem": {"acc,none": 0.6079923882017126, "acc_stderr,none": 0.008499003051254892, "alias": " - stem"}, "mmlu_abstract_algebra": {"alias": "  - abstract_algebra", "acc,none": 0.5, "acc_stderr,none": 0.050251890762960605}, "mmlu_anatomy": {"alias": "  - anatomy", "acc,none": 0.6222222222222222, "acc_stderr,none": 0.04188307537595853}, "mmlu_astronomy": {"alias": "  - astronomy", "acc,none": 0.6907894736842105, "acc_stderr,none": 0.037610708698674805}, "mmlu_college_biology": {"alias": "  - college_biology", "acc,none": 0.7083333333333334, "acc_stderr,none": 0.03800968060554858}, "mmlu_college_chemistry": {"alias": "  - college_chemistry", "acc,none": 0.51, "acc_stderr,none": 0.05024183937956912}, "mmlu_college_computer_science": {"alias": "  - college_computer_science", "acc,none": 0.57, "acc_stderr,none": 0.04975698519562428}, "mmlu_college_mathematics": {"alias": "  - college_mathematics", "acc,none": 0.41, "acc_stderr,none": 0.049431107042371025}, "mmlu_college_physics": {"alias": "  - college_physics", "acc,none": 0.5392156862745098, "acc_stderr,none": 0.04959859966384181}, "mmlu_computer_security": {"alias": "  - computer_security", "acc,none": 0.77, "acc_stderr,none": 0.042295258468165044}, "mmlu_conceptual_physics": {"alias": "  - conceptual_physics", "acc,none": 0.6680851063829787, "acc_stderr,none": 0.03078373675774566}, "mmlu_electrical_engineering": {"alias": "  - electrical_engineering", "acc,none": 0.6275862068965518, "acc_stderr,none": 0.04028731532947558}, "mmlu_elementary_mathematics": {"alias": "  - elementary_mathematics", "acc,none": 0.5714285714285714, "acc_stderr,none": 0.025487187147859375}, "mmlu_high_school_biology": {"alias": "  - high_school_biology", "acc,none": 0.8096774193548387, "acc_stderr,none": 0.02233170761182307}, "mmlu_high_school_chemistry": {"alias": "  - high_school_chemistry", "acc,none": 0.5960591133004927, "acc_stderr,none": 0.03452453903822033}, "mmlu_high_school_computer_science": {"alias": "  - high_school_computer_science", "acc,none": 0.74, "acc_stderr,none": 0.0440844002276808}, "mmlu_high_school_mathematics": {"alias": "  - high_school_mathematics", "acc,none": 0.5222222222222223, "acc_stderr,none": 0.030455413985678415}, "mmlu_high_school_physics": {"alias": "  - high_school_physics", "acc,none": 0.41721854304635764, "acc_stderr,none": 0.0402614149763461}, "mmlu_high_school_statistics": {"alias": "  - high_school_statistics", "acc,none": 0.6018518518518519, "acc_stderr,none": 0.033384734032074016}, "mmlu_machine_learning": {"alias": "  - machine_learning", "acc,none": 0.45535714285714285, "acc_stderr,none": 0.04726835553719099}, "truthfulqa_mc1": {"alias": "truthfulqa_mc1", "acc,none": 0.3182374541003672, "acc_stderr,none": 0.016305988648920612}, "winogrande": {"alias": "winogrande", "acc,none": 0.6787687450670876, "acc_stderr,none": 0.013123599324558302}, "xwinograd": {"acc,none": 0.7986064284108788, "acc_stderr,none": 0.005931872481174303, "alias": "xwinograd"}, "xwinograd_en": {"alias": " - xwinograd_en", "acc,none": 0.8559139784946237, "acc_stderr,none": 0.007284637122057424}, "xwinograd_fr": {"alias": " - xwinograd_fr", "acc,none": 0.7108433734939759, "acc_stderr,none": 0.050066428050419214}, "xwinograd_jp": {"alias": " - xwinograd_jp", "acc,none": 0.708029197080292, "acc_stderr,none": 0.01468968696371698}, "xwinograd_pt": {"alias": " - xwinograd_pt", "acc,none": 0.7490494296577946, "acc_stderr,none": 0.026785433946579916}, "xwinograd_ru": {"alias": " - xwinograd_ru", "acc,none": 0.6984126984126984, "acc_stderr,none": 0.025899880794833647}, "xwinograd_zh": {"alias": " - xwinograd_zh", "acc,none": 0.8095238095238095, "acc_stderr,none": 0.01750858984514584}}, "groups": {"mmlu": {"acc,none": 0.6514741489816266, "acc_stderr,none": 0.003775763817548032, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.5666312433581296, "acc_stderr,none": 0.00659404435419182, "alias": " - humanities"}, "mmlu_other": {"acc,none": 0.7135500482780818, "acc_stderr,none": 0.007829512899144466, "alias": " - other"}, "mmlu_social_sciences": {"acc,none": 0.7630809229769255, "acc_stderr,none": 0.0075435965326657445, "alias": " - social sciences"}, "mmlu_stem": {"acc,none": 0.6079923882017126, "acc_stderr,none": 0.008499003051254892, "alias": " - stem"}, "xwinograd": {"acc,none": 0.7986064284108788, "acc_stderr,none": 0.005931872481174303, "alias": "xwinograd"}}, "group_subtasks": {"mmlu_humanities": ["mmlu_professional_law", "mmlu_high_school_us_history", "mmlu_formal_logic", "mmlu_high_school_european_history", "mmlu_international_law", "mmlu_high_school_world_history", "mmlu_logical_fallacies", "mmlu_world_religions", "mmlu_moral_disputes", "mmlu_prehistory", "mmlu_moral_scenarios", "mmlu_jurisprudence", "mmlu_philosophy"], "mmlu_social_sciences": ["mmlu_professional_psychology", "mmlu_high_school_macroeconomics", "mmlu_human_sexuality", "mmlu_public_relations", "mmlu_security_studies", "mmlu_econometrics", "mmlu_high_school_microeconomics", "mmlu_us_foreign_policy", "mmlu_high_school_psychology", "mmlu_high_school_government_and_politics", "mmlu_sociology", "mmlu_high_school_geography"], "mmlu_other": ["mmlu_professional_accounting", "mmlu_marketing", "mmlu_college_medicine", "mmlu_virology", "mmlu_human_aging", "mmlu_professional_medicine", "mmlu_business_ethics", "mmlu_medical_genetics", "mmlu_management", "mmlu_global_facts", "mmlu_miscellaneous", "mmlu_clinical_knowledge", "mmlu_nutrition"], "mmlu_stem": ["mmlu_college_chemistry", "mmlu_high_school_computer_science", "mmlu_electrical_engineering", "mmlu_high_school_physics", "mmlu_astronomy", "mmlu_college_computer_science", "mmlu_high_school_chemistry", "mmlu_machine_learning", "mmlu_computer_security", "mmlu_college_biology", "mmlu_anatomy", "mmlu_elementary_mathematics", "mmlu_conceptual_physics", "mmlu_college_mathematics", "mmlu_high_school_statistics", "mmlu_abstract_algebra", "mmlu_high_school_biology", "mmlu_high_school_mathematics", "mmlu_college_physics"], "mmlu": ["mmlu_stem", "mmlu_other", "mmlu_social_sciences", "mmlu_humanities"], "hellaswag": [], "xwinograd": ["xwinograd_en", "xwinograd_fr", "xwinograd_jp", "xwinograd_pt", "xwinograd_ru", "xwinograd_zh"], "winogrande": [], "truthfulqa_mc1": [], "arc_challenge": [], "gsm8k": []}}