{"results": {"arc_challenge": {"alias": "arc_challenge", "acc,none": 0.3122866894197952, "acc_stderr,none": 0.013542598541688065, "acc_norm,none": 0.36006825938566556, "acc_norm_stderr,none": 0.014027516814585184}, "gsm8k": {"alias": "gsm8k", "exact_match,strict-match": 0.0712661106899166, "exact_match_stderr,strict-match": 0.0070864621279544985, "exact_match,flexible-extract": 0.07505686125852919, "exact_match_stderr,flexible-extract": 0.007257633145486642}, "hellaswag": {"alias": "hellaswag", "acc,none": 0.47689703246365267, "acc_stderr,none": 0.004984452002563918, "acc_norm,none": 0.6378211511651065, "acc_norm_stderr,none": 0.004796478664403822}, "mmlu": {"acc,none": 0.3661871528272326, "acc_stderr,none": 0.004003436111803322, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.34537725823591925, "acc_stderr,none": 0.006820115805867835, "alias": " - humanities"}, "mmlu_formal_logic": {"alias": "  - formal_logic", "acc,none": 0.2619047619047619, "acc_stderr,none": 0.039325376803928724}, "mmlu_high_school_european_history": {"alias": "  - high_school_european_history", "acc,none": 0.47878787878787876, "acc_stderr,none": 0.03900828913737302}, "mmlu_high_school_us_history": {"alias": "  - high_school_us_history", "acc,none": 0.4411764705882353, "acc_stderr,none": 0.034849415144292316}, "mmlu_high_school_world_history": {"alias": "  - high_school_world_history", "acc,none": 0.42616033755274263, "acc_stderr,none": 0.03219035703131774}, "mmlu_international_law": {"alias": "  - international_law", "acc,none": 0.5289256198347108, "acc_stderr,none": 0.04556710331269498}, "mmlu_jurisprudence": {"alias": "  - jurisprudence", "acc,none": 0.48148148148148145, "acc_stderr,none": 0.04830366024635331}, "mmlu_logical_fallacies": {"alias": "  - logical_fallacies", "acc,none": 0.34355828220858897, "acc_stderr,none": 0.03731133519673891}, "mmlu_moral_disputes": {"alias": "  - moral_disputes", "acc,none": 0.36416184971098264, "acc_stderr,none": 0.025906632631016124}, "mmlu_moral_scenarios": {"alias": "  - moral_scenarios", "acc,none": 0.23798882681564246, "acc_stderr,none": 0.01424263007057489}, "mmlu_philosophy": {"alias": "  - philosophy", "acc,none": 0.42443729903536975, "acc_stderr,none": 0.028071928247946205}, "mmlu_prehistory": {"alias": "  - prehistory", "acc,none": 0.4351851851851852, "acc_stderr,none": 0.027586006221607718}, "mmlu_professional_law": {"alias": "  - professional_law", "acc,none": 0.2940026075619296, "acc_stderr,none": 0.011636062953698614}, "mmlu_world_religions": {"alias": "  - world_religions", "acc,none": 0.5087719298245614, "acc_stderr,none": 0.038342347441649924}, "mmlu_other": {"acc,none": 0.407145156099131, "acc_stderr,none": 0.008746727378318686, "alias": " - other"}, "mmlu_business_ethics": {"alias": "  - business_ethics", "acc,none": 0.34, "acc_stderr,none": 0.047609522856952344}, "mmlu_clinical_knowledge": {"alias": "  - clinical_knowledge", "acc,none": 0.3584905660377358, "acc_stderr,none": 0.029514703583981772}, "mmlu_college_medicine": {"alias": "  - college_medicine", "acc,none": 0.32947976878612717, "acc_stderr,none": 0.03583901754736412}, "mmlu_global_facts": {"alias": "  - global_facts", "acc,none": 0.31, "acc_stderr,none": 0.04648231987117316}, "mmlu_human_aging": {"alias": "  - human_aging", "acc,none": 0.42152466367713004, "acc_stderr,none": 0.03314190222110656}, "mmlu_management": {"alias": "  - management", "acc,none": 0.42718446601941745, "acc_stderr,none": 0.048979577377811674}, "mmlu_marketing": {"alias": "  - marketing", "acc,none": 0.5, "acc_stderr,none": 0.03275608910402091}, "mmlu_medical_genetics": {"alias": "  - medical_genetics", "acc,none": 0.45, "acc_stderr,none": 0.05}, "mmlu_miscellaneous": {"alias": "  - miscellaneous", "acc,none": 0.47509578544061304, "acc_stderr,none": 0.017857770704901025}, "mmlu_nutrition": {"alias": "  - nutrition", "acc,none": 0.4215686274509804, "acc_stderr,none": 0.028275490156791434}, "mmlu_professional_accounting": {"alias": "  - professional_accounting", "acc,none": 0.2695035460992908, "acc_stderr,none": 0.026469036818590624}, "mmlu_professional_medicine": {"alias": "  - professional_medicine", "acc,none": 0.3860294117647059, "acc_stderr,none": 0.029573269134411124}, "mmlu_virology": {"alias": "  - virology", "acc,none": 0.39759036144578314, "acc_stderr,none": 0.03809973084540218}, "mmlu_social_sciences": {"acc,none": 0.3997400064998375, "acc_stderr,none": 0.008712787331560813, "alias": " - social sciences"}, "mmlu_econometrics": {"alias": "  - econometrics", "acc,none": 0.23684210526315788, "acc_stderr,none": 0.03999423879281338}, "mmlu_high_school_geography": {"alias": "  - high_school_geography", "acc,none": 0.5050505050505051, "acc_stderr,none": 0.035621707606254015}, "mmlu_high_school_government_and_politics": {"alias": "  - high_school_government_and_politics", "acc,none": 0.45595854922279794, "acc_stderr,none": 0.03594413711272438}, "mmlu_high_school_macroeconomics": {"alias": "  - high_school_macroeconomics", "acc,none": 0.3076923076923077, "acc_stderr,none": 0.023400928918310502}, "mmlu_high_school_microeconomics": {"alias": "  - high_school_microeconomics", "acc,none": 0.29831932773109243, "acc_stderr,none": 0.029719142876342863}, "mmlu_high_school_psychology": {"alias": "  - high_school_psychology", "acc,none": 0.45137614678899085, "acc_stderr,none": 0.021335714711268796}, "mmlu_human_sexuality": {"alias": "  - human_sexuality", "acc,none": 0.48091603053435117, "acc_stderr,none": 0.043820947055509867}, "mmlu_professional_psychology": {"alias": "  - professional_psychology", "acc,none": 0.3545751633986928, "acc_stderr,none": 0.019353360547553686}, "mmlu_public_relations": {"alias": "  - public_relations", "acc,none": 0.36363636363636365, "acc_stderr,none": 0.046075820907199756}, "mmlu_security_studies": {"alias": "  - security_studies", "acc,none": 0.37551020408163266, "acc_stderr,none": 0.03100120903989484}, "mmlu_sociology": {"alias": "  - sociology", "acc,none": 0.5572139303482587, "acc_stderr,none": 0.03512310964123935}, "mmlu_us_foreign_policy": {"alias": "  - us_foreign_policy", "acc,none": 0.54, "acc_stderr,none": 0.05009082659620332}, "mmlu_stem": {"acc,none": 0.3241357437361243, "acc_stderr,none": 0.00822949296691121, "alias": " - stem"}, "mmlu_abstract_algebra": {"alias": "  - abstract_algebra", "acc,none": 0.25, "acc_stderr,none": 0.04351941398892446}, "mmlu_anatomy": {"alias": "  - anatomy", "acc,none": 0.4888888888888889, "acc_stderr,none": 0.04318275491977976}, "mmlu_astronomy": {"alias": "  - astronomy", "acc,none": 0.40131578947368424, "acc_stderr,none": 0.03988903703336285}, "mmlu_college_biology": {"alias": "  - college_biology", "acc,none": 0.375, "acc_stderr,none": 0.04048439222695598}, "mmlu_college_chemistry": {"alias": "  - college_chemistry", "acc,none": 0.29, "acc_stderr,none": 0.045604802157206845}, "mmlu_college_computer_science": {"alias": "  - college_computer_science", "acc,none": 0.38, "acc_stderr,none": 0.04878317312145632}, "mmlu_college_mathematics": {"alias": "  - college_mathematics", "acc,none": 0.29, "acc_stderr,none": 0.04560480215720684}, "mmlu_college_physics": {"alias": "  - college_physics", "acc,none": 0.2549019607843137, "acc_stderr,none": 0.04336432707993177}, "mmlu_computer_security": {"alias": "  - computer_security", "acc,none": 0.52, "acc_stderr,none": 0.050211673156867795}, "mmlu_conceptual_physics": {"alias": "  - conceptual_physics", "acc,none": 0.3574468085106383, "acc_stderr,none": 0.03132941789476425}, "mmlu_electrical_engineering": {"alias": "  - electrical_engineering", "acc,none": 0.4068965517241379, "acc_stderr,none": 0.040937939812662374}, "mmlu_elementary_mathematics": {"alias": "  - elementary_mathematics", "acc,none": 0.2566137566137566, "acc_stderr,none": 0.022494510767503154}, "mmlu_high_school_biology": {"alias": "  - high_school_biology", "acc,none": 0.4, "acc_stderr,none": 0.027869320571664632}, "mmlu_high_school_chemistry": {"alias": "  - high_school_chemistry", "acc,none": 0.28078817733990147, "acc_stderr,none": 0.03161856335358609}, "mmlu_high_school_computer_science": {"alias": "  - high_school_computer_science", "acc,none": 0.34, "acc_stderr,none": 0.047609522856952344}, "mmlu_high_school_mathematics": {"alias": "  - high_school_mathematics", "acc,none": 0.2222222222222222, "acc_stderr,none": 0.02534809746809783}, "mmlu_high_school_physics": {"alias": "  - high_school_physics", "acc,none": 0.2251655629139073, "acc_stderr,none": 0.03410435282008937}, "mmlu_high_school_statistics": {"alias": "  - high_school_statistics", "acc,none": 0.24074074074074073, "acc_stderr,none": 0.029157522184605607}, "mmlu_machine_learning": {"alias": "  - machine_learning", "acc,none": 0.36607142857142855, "acc_stderr,none": 0.0457237235873743}, "truthfulqa_mc1": {"alias": "truthfulqa_mc1", "acc,none": 0.23623011015911874, "acc_stderr,none": 0.014869755015871114}, "winogrande": {"alias": "winogrande", "acc,none": 0.5998421468034728, "acc_stderr,none": 0.013769472660464991}, "xwinograd": {"acc,none": 0.721735221398067, "acc_stderr,none": 0.006565783754306834, "alias": "xwinograd"}, "xwinograd_en": {"alias": " - xwinograd_en", "acc,none": 0.8129032258064516, "acc_stderr,none": 0.008089739024656699}, "xwinograd_fr": {"alias": " - xwinograd_fr", "acc,none": 0.6265060240963856, "acc_stderr,none": 0.05341921480681956}, "xwinograd_jp": {"alias": " - xwinograd_jp", "acc,none": 0.6068821689259646, "acc_stderr,none": 0.015780865040470972}, "xwinograd_pt": {"alias": " - xwinograd_pt", "acc,none": 0.6577946768060836, "acc_stderr,none": 0.02931149111427513}, "xwinograd_ru": {"alias": " - xwinograd_ru", "acc,none": 0.6031746031746031, "acc_stderr,none": 0.02760936569491737}, "xwinograd_zh": {"alias": " - xwinograd_zh", "acc,none": 0.6428571428571429, "acc_stderr,none": 0.021364573561124412}}, "groups": {"mmlu": {"acc,none": 0.3661871528272326, "acc_stderr,none": 0.004003436111803322, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.34537725823591925, "acc_stderr,none": 0.006820115805867835, "alias": " - humanities"}, "mmlu_other": {"acc,none": 0.407145156099131, "acc_stderr,none": 0.008746727378318686, "alias": " - other"}, "mmlu_social_sciences": {"acc,none": 0.3997400064998375, "acc_stderr,none": 0.008712787331560813, "alias": " - social sciences"}, "mmlu_stem": {"acc,none": 0.3241357437361243, "acc_stderr,none": 0.00822949296691121, "alias": " - stem"}, "xwinograd": {"acc,none": 0.721735221398067, "acc_stderr,none": 0.006565783754306834, "alias": "xwinograd"}}, "group_subtasks": {"mmlu_humanities": ["mmlu_high_school_european_history", "mmlu_moral_scenarios", "mmlu_jurisprudence", "mmlu_logical_fallacies", "mmlu_philosophy", "mmlu_moral_disputes", "mmlu_world_religions", "mmlu_international_law", "mmlu_high_school_world_history", "mmlu_high_school_us_history", "mmlu_prehistory", "mmlu_professional_law", "mmlu_formal_logic"], "mmlu_social_sciences": ["mmlu_public_relations", "mmlu_professional_psychology", "mmlu_high_school_microeconomics", "mmlu_human_sexuality", "mmlu_high_school_macroeconomics", "mmlu_security_studies", "mmlu_high_school_geography", "mmlu_econometrics", "mmlu_high_school_government_and_politics", "mmlu_high_school_psychology", "mmlu_sociology", "mmlu_us_foreign_policy"], "mmlu_other": ["mmlu_management", "mmlu_college_medicine", "mmlu_professional_medicine", "mmlu_global_facts", "mmlu_miscellaneous", "mmlu_clinical_knowledge", "mmlu_virology", "mmlu_business_ethics", "mmlu_nutrition", "mmlu_human_aging", "mmlu_medical_genetics", "mmlu_professional_accounting", "mmlu_marketing"], "mmlu_stem": ["mmlu_high_school_physics", "mmlu_computer_security", "mmlu_high_school_computer_science", "mmlu_college_computer_science", "mmlu_electrical_engineering", "mmlu_high_school_chemistry", "mmlu_astronomy", "mmlu_machine_learning", "mmlu_high_school_mathematics", "mmlu_college_physics", "mmlu_college_chemistry", "mmlu_elementary_mathematics", "mmlu_abstract_algebra", "mmlu_anatomy", "mmlu_college_mathematics", "mmlu_college_biology", "mmlu_conceptual_physics", "mmlu_high_school_statistics", "mmlu_high_school_biology"], "mmlu": ["mmlu_stem", "mmlu_other", "mmlu_social_sciences", "mmlu_humanities"], "hellaswag": [], "xwinograd": ["xwinograd_en", "xwinograd_fr", "xwinograd_jp", "xwinograd_pt", "xwinograd_ru", "xwinograd_zh"], "winogrande": [], "truthfulqa_mc1": [], "arc_challenge": [], "gsm8k": []}}