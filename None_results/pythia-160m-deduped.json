{"results": {"arc_challenge": {"alias": "arc_challenge", "acc,none": 0.20819112627986347, "acc_stderr,none": 0.011864866118448069, "acc_norm,none": 0.23976109215017063, "acc_norm_stderr,none": 0.012476304127453939}, "gsm8k": {"alias": "gsm8k", "exact_match,strict-match": 0.0, "exact_match_stderr,strict-match": 0.0, "exact_match,flexible-extract": 0.006823351023502654, "exact_match_stderr,flexible-extract": 0.0022675371022544692}, "hellaswag": {"alias": "hellaswag", "acc,none": 0.28440549691296557, "acc_stderr,none": 0.004502088287470089, "acc_norm,none": 0.3037243576976698, "acc_norm_stderr,none": 0.0045892522436278204}, "mmlu": {"acc,none": 0.22931206380857427, "acc_stderr,none": 0.0035419346049023162, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.24229543039319873, "acc_stderr,none": 0.006244901613088654, "alias": " - humanities"}, "mmlu_formal_logic": {"alias": "  - formal_logic", "acc,none": 0.29365079365079366, "acc_stderr,none": 0.04073524322147127}, "mmlu_high_school_european_history": {"alias": "  - high_school_european_history", "acc,none": 0.22424242424242424, "acc_stderr,none": 0.03256866661681102}, "mmlu_high_school_us_history": {"alias": "  - high_school_us_history", "acc,none": 0.25, "acc_stderr,none": 0.03039153369274154}, "mmlu_high_school_world_history": {"alias": "  - high_school_world_history", "acc,none": 0.270042194092827, "acc_stderr,none": 0.028900721906293426}, "mmlu_international_law": {"alias": "  - international_law", "acc,none": 0.2396694214876033, "acc_stderr,none": 0.03896878985070417}, "mmlu_jurisprudence": {"alias": "  - jurisprudence", "acc,none": 0.25925925925925924, "acc_stderr,none": 0.04236511258094634}, "mmlu_logical_fallacies": {"alias": "  - logical_fallacies", "acc,none": 0.22085889570552147, "acc_stderr,none": 0.032591773927421776}, "mmlu_moral_disputes": {"alias": "  - moral_disputes", "acc,none": 0.24855491329479767, "acc_stderr,none": 0.023267528432100174}, "mmlu_moral_scenarios": {"alias": "  - moral_scenarios", "acc,none": 0.23798882681564246, "acc_stderr,none": 0.014242630070574885}, "mmlu_philosophy": {"alias": "  - philosophy", "acc,none": 0.1864951768488746, "acc_stderr,none": 0.02212243977248077}, "mmlu_prehistory": {"alias": "  - prehistory", "acc,none": 0.21604938271604937, "acc_stderr,none": 0.022899162918445813}, "mmlu_professional_law": {"alias": "  - professional_law", "acc,none": 0.2457627118644068, "acc_stderr,none": 0.01099615663514269}, "mmlu_world_religions": {"alias": "  - world_religions", "acc,none": 0.3157894736842105, "acc_stderr,none": 0.03565079670708313}, "mmlu_other": {"acc,none": 0.23978113936272932, "acc_stderr,none": 0.00764294783394267, "alias": " - other"}, "mmlu_business_ethics": {"alias": "  - business_ethics", "acc,none": 0.3, "acc_stderr,none": 0.046056618647183814}, "mmlu_clinical_knowledge": {"alias": "  - clinical_knowledge", "acc,none": 0.21509433962264152, "acc_stderr,none": 0.025288394502891377}, "mmlu_college_medicine": {"alias": "  - college_medicine", "acc,none": 0.20809248554913296, "acc_stderr,none": 0.03095289021774988}, "mmlu_global_facts": {"alias": "  - global_facts", "acc,none": 0.18, "acc_stderr,none": 0.038612291966536955}, "mmlu_human_aging": {"alias": "  - human_aging", "acc,none": 0.31390134529147984, "acc_stderr,none": 0.03114679648297246}, "mmlu_management": {"alias": "  - management", "acc,none": 0.17475728155339806, "acc_stderr,none": 0.03760178006026621}, "mmlu_marketing": {"alias": "  - marketing", "acc,none": 0.2905982905982906, "acc_stderr,none": 0.029745048572674057}, "mmlu_medical_genetics": {"alias": "  - medical_genetics", "acc,none": 0.3, "acc_stderr,none": 0.046056618647183814}, "mmlu_miscellaneous": {"alias": "  - miscellaneous", "acc,none": 0.23627075351213284, "acc_stderr,none": 0.01519047371703751}, "mmlu_nutrition": {"alias": "  - nutrition", "acc,none": 0.22549019607843138, "acc_stderr,none": 0.023929155517351284}, "mmlu_professional_accounting": {"alias": "  - professional_accounting", "acc,none": 0.23404255319148937, "acc_stderr,none": 0.025257861359432407}, "mmlu_professional_medicine": {"alias": "  - professional_medicine", "acc,none": 0.1875, "acc_stderr,none": 0.023709788253811766}, "mmlu_virology": {"alias": "  - virology", "acc,none": 0.28313253012048195, "acc_stderr,none": 0.03507295431370518}, "mmlu_social_sciences": {"acc,none": 0.2170945726356841, "acc_stderr,none": 0.007428786285788534, "alias": " - social sciences"}, "mmlu_econometrics": {"alias": "  - econometrics", "acc,none": 0.23684210526315788, "acc_stderr,none": 0.039994238792813386}, "mmlu_high_school_geography": {"alias": "  - high_school_geography", "acc,none": 0.17676767676767677, "acc_stderr,none": 0.027178752639044915}, "mmlu_high_school_government_and_politics": {"alias": "  - high_school_government_and_politics", "acc,none": 0.19689119170984457, "acc_stderr,none": 0.02869787397186069}, "mmlu_high_school_macroeconomics": {"alias": "  - high_school_macroeconomics", "acc,none": 0.20256410256410257, "acc_stderr,none": 0.020377660970371397}, "mmlu_high_school_microeconomics": {"alias": "  - high_school_microeconomics", "acc,none": 0.21008403361344538, "acc_stderr,none": 0.026461398717471874}, "mmlu_high_school_psychology": {"alias": "  - high_school_psychology", "acc,none": 0.1926605504587156, "acc_stderr,none": 0.016909276884936073}, "mmlu_human_sexuality": {"alias": "  - human_sexuality", "acc,none": 0.2595419847328244, "acc_stderr,none": 0.03844876139785271}, "mmlu_professional_psychology": {"alias": "  - professional_psychology", "acc,none": 0.25, "acc_stderr,none": 0.01751781884501444}, "mmlu_public_relations": {"alias": "  - public_relations", "acc,none": 0.21818181818181817, "acc_stderr,none": 0.03955932861795833}, "mmlu_security_studies": {"alias": "  - security_studies", "acc,none": 0.18775510204081633, "acc_stderr,none": 0.02500025603954622}, "mmlu_sociology": {"alias": "  - sociology", "acc,none": 0.24378109452736318, "acc_stderr,none": 0.030360490154014652}, "mmlu_us_foreign_policy": {"alias": "  - us_foreign_policy", "acc,none": 0.28, "acc_stderr,none": 0.045126085985421276}, "mmlu_stem": {"acc,none": 0.21154456073580716, "acc_stderr,none": 0.007258884125731186, "alias": " - stem"}, "mmlu_abstract_algebra": {"alias": "  - abstract_algebra", "acc,none": 0.22, "acc_stderr,none": 0.04163331998932269}, "mmlu_anatomy": {"alias": "  - anatomy", "acc,none": 0.18518518518518517, "acc_stderr,none": 0.03355677216313142}, "mmlu_astronomy": {"alias": "  - astronomy", "acc,none": 0.17763157894736842, "acc_stderr,none": 0.031103182383123398}, "mmlu_college_biology": {"alias": "  - college_biology", "acc,none": 0.2569444444444444, "acc_stderr,none": 0.03653946969442099}, "mmlu_college_chemistry": {"alias": "  - college_chemistry", "acc,none": 0.2, "acc_stderr,none": 0.040201512610368445}, "mmlu_college_computer_science": {"alias": "  - college_computer_science", "acc,none": 0.25, "acc_stderr,none": 0.04351941398892446}, "mmlu_college_mathematics": {"alias": "  - college_mathematics", "acc,none": 0.21, "acc_stderr,none": 0.040936018074033256}, "mmlu_college_physics": {"alias": "  - college_physics", "acc,none": 0.21568627450980393, "acc_stderr,none": 0.040925639582376556}, "mmlu_computer_security": {"alias": "  - computer_security", "acc,none": 0.28, "acc_stderr,none": 0.045126085985421276}, "mmlu_conceptual_physics": {"alias": "  - conceptual_physics", "acc,none": 0.26382978723404255, "acc_stderr,none": 0.02880998985410298}, "mmlu_electrical_engineering": {"alias": "  - electrical_engineering", "acc,none": 0.2413793103448276, "acc_stderr,none": 0.03565998174135302}, "mmlu_elementary_mathematics": {"alias": "  - elementary_mathematics", "acc,none": 0.20899470899470898, "acc_stderr,none": 0.020940481565334835}, "mmlu_high_school_biology": {"alias": "  - high_school_biology", "acc,none": 0.1774193548387097, "acc_stderr,none": 0.021732540689329265}, "mmlu_high_school_chemistry": {"alias": "  - high_school_chemistry", "acc,none": 0.1477832512315271, "acc_stderr,none": 0.024969621333521284}, "mmlu_high_school_computer_science": {"alias": "  - high_school_computer_science", "acc,none": 0.25, "acc_stderr,none": 0.04351941398892446}, "mmlu_high_school_mathematics": {"alias": "  - high_school_mathematics", "acc,none": 0.2074074074074074, "acc_stderr,none": 0.024720713193952165}, "mmlu_high_school_physics": {"alias": "  - high_school_physics", "acc,none": 0.1986754966887417, "acc_stderr,none": 0.032578473844367746}, "mmlu_high_school_statistics": {"alias": "  - high_school_statistics", "acc,none": 0.1527777777777778, "acc_stderr,none": 0.02453632602613422}, "mmlu_machine_learning": {"alias": "  - machine_learning", "acc,none": 0.3125, "acc_stderr,none": 0.043994650575715215}, "truthfulqa_mc1": {"alias": "truthfulqa_mc1", "acc,none": 0.23255813953488372, "acc_stderr,none": 0.014789157531080522}, "winogrande": {"alias": "winogrande", "acc,none": 0.5201262825572218, "acc_stderr,none": 0.014041096664344324}, "xwinograd": {"acc,none": 0.5291076646437401, "acc_stderr,none": 0.0074748313133107345, "alias": "xwinograd"}, "xwinograd_en": {"alias": " - xwinograd_en", "acc,none": 0.5556989247311828, "acc_stderr,none": 0.010307192770028716}, "xwinograd_fr": {"alias": " - xwinograd_fr", "acc,none": 0.5783132530120482, "acc_stderr,none": 0.05453428485295111}, "xwinograd_jp": {"alias": " - xwinograd_jp", "acc,none": 0.49530761209593327, "acc_stderr,none": 0.016153555342534955}, "xwinograd_pt": {"alias": " - xwinograd_pt", "acc,none": 0.4866920152091255, "acc_stderr,none": 0.030879159795851503}, "xwinograd_ru": {"alias": " - xwinograd_ru", "acc,none": 0.5079365079365079, "acc_stderr,none": 0.02821307754781505}, "xwinograd_zh": {"alias": " - xwinograd_zh", "acc,none": 0.498015873015873, "acc_stderr,none": 0.022293722571246788}}, "groups": {"mmlu": {"acc,none": 0.22931206380857427, "acc_stderr,none": 0.0035419346049023162, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.24229543039319873, "acc_stderr,none": 0.006244901613088654, "alias": " - humanities"}, "mmlu_other": {"acc,none": 0.23978113936272932, "acc_stderr,none": 0.00764294783394267, "alias": " - other"}, "mmlu_social_sciences": {"acc,none": 0.2170945726356841, "acc_stderr,none": 0.007428786285788534, "alias": " - social sciences"}, "mmlu_stem": {"acc,none": 0.21154456073580716, "acc_stderr,none": 0.007258884125731186, "alias": " - stem"}, "xwinograd": {"acc,none": 0.5291076646437401, "acc_stderr,none": 0.0074748313133107345, "alias": "xwinograd"}}, "group_subtasks": {"mmlu_humanities": ["mmlu_moral_disputes", "mmlu_world_religions", "mmlu_high_school_european_history", "mmlu_moral_scenarios", "mmlu_international_law", "mmlu_logical_fallacies", "mmlu_high_school_us_history", "mmlu_high_school_world_history", "mmlu_prehistory", "mmlu_jurisprudence", "mmlu_formal_logic", "mmlu_professional_law", "mmlu_philosophy"], "mmlu_social_sciences": ["mmlu_public_relations", "mmlu_us_foreign_policy", "mmlu_high_school_geography", "mmlu_high_school_macroeconomics", "mmlu_security_studies", "mmlu_sociology", "mmlu_econometrics", "mmlu_high_school_government_and_politics", "mmlu_professional_psychology", "mmlu_high_school_psychology", "mmlu_high_school_microeconomics", "mmlu_human_sexuality"], "mmlu_other": ["mmlu_management", "mmlu_miscellaneous", "mmlu_human_aging", "mmlu_clinical_knowledge", "mmlu_marketing", "mmlu_college_medicine", "mmlu_nutrition", "mmlu_professional_medicine", "mmlu_professional_accounting", "mmlu_business_ethics", "mmlu_medical_genetics", "mmlu_virology", "mmlu_global_facts"], "mmlu_stem": ["mmlu_college_chemistry", "mmlu_college_mathematics", "mmlu_machine_learning", "mmlu_elementary_mathematics", "mmlu_anatomy", "mmlu_conceptual_physics", "mmlu_college_biology", "mmlu_abstract_algebra", "mmlu_electrical_engineering", "mmlu_high_school_statistics", "mmlu_high_school_chemistry", "mmlu_computer_security", "mmlu_high_school_biology", "mmlu_college_physics", "mmlu_high_school_physics", "mmlu_astronomy", "mmlu_college_computer_science", "mmlu_high_school_mathematics", "mmlu_high_school_computer_science"], "mmlu": ["mmlu_stem", "mmlu_other", "mmlu_social_sciences", "mmlu_humanities"], "hellaswag": [], "xwinograd": ["xwinograd_en", "xwinograd_fr", "xwinograd_jp", "xwinograd_pt", "xwinograd_ru", "xwinograd_zh"], "winogrande": [], "truthfulqa_mc1": [], "arc_challenge": [], "gsm8k": []}}