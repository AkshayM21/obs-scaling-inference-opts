{"results": {"arc_challenge": {"alias": "arc_challenge", "acc,none": 0.26109215017064846, "acc_stderr,none": 0.012835523909473852, "acc_norm,none": 0.2627986348122867, "acc_norm_stderr,none": 0.012862523175351331}, "gsm8k": {"alias": "gsm8k", "exact_match,strict-match": 0.01592115238817286, "exact_match_stderr,strict-match": 0.0034478192723890098, "exact_match,flexible-extract": 0.02350265352539803, "exact_match_stderr,flexible-extract": 0.004172883669643963}, "hellaswag": {"alias": "hellaswag", "acc,none": 0.3604859589723163, "acc_stderr,none": 0.0047916019756127646, "acc_norm,none": 0.4410476000796654, "acc_norm_stderr,none": 0.004954977202585492}, "mmlu": {"acc,none": 0.23529411764705882, "acc_stderr,none": 0.003575402276408048, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.24442082890541977, "acc_stderr,none": 0.006268009266028934, "alias": " - humanities"}, "mmlu_formal_logic": {"alias": "  - formal_logic", "acc,none": 0.2777777777777778, "acc_stderr,none": 0.04006168083848877}, "mmlu_high_school_european_history": {"alias": "  - high_school_european_history", "acc,none": 0.21212121212121213, "acc_stderr,none": 0.031922715695483}, "mmlu_high_school_us_history": {"alias": "  - high_school_us_history", "acc,none": 0.2647058823529412, "acc_stderr,none": 0.03096451792692339}, "mmlu_high_school_world_history": {"alias": "  - high_school_world_history", "acc,none": 0.25738396624472576, "acc_stderr,none": 0.028458820991460295}, "mmlu_international_law": {"alias": "  - international_law", "acc,none": 0.2644628099173554, "acc_stderr,none": 0.04026187527591205}, "mmlu_jurisprudence": {"alias": "  - jurisprudence", "acc,none": 0.28703703703703703, "acc_stderr,none": 0.043733130409147614}, "mmlu_logical_fallacies": {"alias": "  - logical_fallacies", "acc,none": 0.24539877300613497, "acc_stderr,none": 0.03380939813943354}, "mmlu_moral_disputes": {"alias": "  - moral_disputes", "acc,none": 0.2543352601156069, "acc_stderr,none": 0.023445826276545546}, "mmlu_moral_scenarios": {"alias": "  - moral_scenarios", "acc,none": 0.2424581005586592, "acc_stderr,none": 0.014333522059217889}, "mmlu_philosophy": {"alias": "  - philosophy", "acc,none": 0.19614147909967847, "acc_stderr,none": 0.02255244778047803}, "mmlu_prehistory": {"alias": "  - prehistory", "acc,none": 0.23148148148148148, "acc_stderr,none": 0.02346842983245116}, "mmlu_professional_law": {"alias": "  - professional_law", "acc,none": 0.24445893089960888, "acc_stderr,none": 0.010976425013113907}, "mmlu_world_religions": {"alias": "  - world_religions", "acc,none": 0.26900584795321636, "acc_stderr,none": 0.03401052620104088}, "mmlu_other": {"acc,none": 0.24686192468619247, "acc_stderr,none": 0.0077121117984350835, "alias": " - other"}, "mmlu_business_ethics": {"alias": "  - business_ethics", "acc,none": 0.29, "acc_stderr,none": 0.045604802157206845}, "mmlu_clinical_knowledge": {"alias": "  - clinical_knowledge", "acc,none": 0.22264150943396227, "acc_stderr,none": 0.02560423347089909}, "mmlu_college_medicine": {"alias": "  - college_medicine", "acc,none": 0.19653179190751446, "acc_stderr,none": 0.03029957466478814}, "mmlu_global_facts": {"alias": "  - global_facts", "acc,none": 0.23, "acc_stderr,none": 0.04229525846816508}, "mmlu_human_aging": {"alias": "  - human_aging", "acc,none": 0.33183856502242154, "acc_stderr,none": 0.031602951437766785}, "mmlu_management": {"alias": "  - management", "acc,none": 0.14563106796116504, "acc_stderr,none": 0.034926064766237906}, "mmlu_marketing": {"alias": "  - marketing", "acc,none": 0.2905982905982906, "acc_stderr,none": 0.029745048572674054}, "mmlu_medical_genetics": {"alias": "  - medical_genetics", "acc,none": 0.25, "acc_stderr,none": 0.04351941398892446}, "mmlu_miscellaneous": {"alias": "  - miscellaneous", "acc,none": 0.2554278416347382, "acc_stderr,none": 0.015594955384455772}, "mmlu_nutrition": {"alias": "  - nutrition", "acc,none": 0.20915032679738563, "acc_stderr,none": 0.02328768531233481}, "mmlu_professional_accounting": {"alias": "  - professional_accounting", "acc,none": 0.25886524822695034, "acc_stderr,none": 0.026129572527180848}, "mmlu_professional_medicine": {"alias": "  - professional_medicine", "acc,none": 0.19117647058823528, "acc_stderr,none": 0.023886881922440362}, "mmlu_virology": {"alias": "  - virology", "acc,none": 0.3072289156626506, "acc_stderr,none": 0.03591566797824664}, "mmlu_social_sciences": {"acc,none": 0.22619434514137146, "acc_stderr,none": 0.007537755791492467, "alias": " - social sciences"}, "mmlu_econometrics": {"alias": "  - econometrics", "acc,none": 0.24561403508771928, "acc_stderr,none": 0.04049339297748139}, "mmlu_high_school_geography": {"alias": "  - high_school_geography", "acc,none": 0.21717171717171718, "acc_stderr,none": 0.029376616484945616}, "mmlu_high_school_government_and_politics": {"alias": "  - high_school_government_and_politics", "acc,none": 0.17098445595854922, "acc_stderr,none": 0.027171213683164545}, "mmlu_high_school_macroeconomics": {"alias": "  - high_school_macroeconomics", "acc,none": 0.21025641025641026, "acc_stderr,none": 0.020660597485026924}, "mmlu_high_school_microeconomics": {"alias": "  - high_school_microeconomics", "acc,none": 0.21428571428571427, "acc_stderr,none": 0.026653531596715487}, "mmlu_high_school_psychology": {"alias": "  - high_school_psychology", "acc,none": 0.20733944954128442, "acc_stderr,none": 0.017381415563608667}, "mmlu_human_sexuality": {"alias": "  - human_sexuality", "acc,none": 0.25190839694656486, "acc_stderr,none": 0.038073871163060866}, "mmlu_professional_psychology": {"alias": "  - professional_psychology", "acc,none": 0.2647058823529412, "acc_stderr,none": 0.017848089574913226}, "mmlu_public_relations": {"alias": "  - public_relations", "acc,none": 0.21818181818181817, "acc_stderr,none": 0.03955932861795833}, "mmlu_security_studies": {"alias": "  - security_studies", "acc,none": 0.2, "acc_stderr,none": 0.025607375986579153}, "mmlu_sociology": {"alias": "  - sociology", "acc,none": 0.23880597014925373, "acc_stderr,none": 0.030147775935409224}, "mmlu_us_foreign_policy": {"alias": "  - us_foreign_policy", "acc,none": 0.3, "acc_stderr,none": 0.046056618647183814}, "mmlu_stem": {"acc,none": 0.21915635902315256, "acc_stderr,none": 0.007361354586830359, "alias": " - stem"}, "mmlu_abstract_algebra": {"alias": "  - abstract_algebra", "acc,none": 0.22, "acc_stderr,none": 0.041633319989322695}, "mmlu_anatomy": {"alias": "  - anatomy", "acc,none": 0.22962962962962963, "acc_stderr,none": 0.03633384414073465}, "mmlu_astronomy": {"alias": "  - astronomy", "acc,none": 0.20394736842105263, "acc_stderr,none": 0.0327900040631005}, "mmlu_college_biology": {"alias": "  - college_biology", "acc,none": 0.2638888888888889, "acc_stderr,none": 0.03685651095897532}, "mmlu_college_chemistry": {"alias": "  - college_chemistry", "acc,none": 0.21, "acc_stderr,none": 0.04093601807403325}, "mmlu_college_computer_science": {"alias": "  - college_computer_science", "acc,none": 0.25, "acc_stderr,none": 0.04351941398892446}, "mmlu_college_mathematics": {"alias": "  - college_mathematics", "acc,none": 0.23, "acc_stderr,none": 0.04229525846816505}, "mmlu_college_physics": {"alias": "  - college_physics", "acc,none": 0.19607843137254902, "acc_stderr,none": 0.03950581861179961}, "mmlu_computer_security": {"alias": "  - computer_security", "acc,none": 0.27, "acc_stderr,none": 0.044619604333847394}, "mmlu_conceptual_physics": {"alias": "  - conceptual_physics", "acc,none": 0.2723404255319149, "acc_stderr,none": 0.029101290698386694}, "mmlu_electrical_engineering": {"alias": "  - electrical_engineering", "acc,none": 0.2620689655172414, "acc_stderr,none": 0.036646663372252565}, "mmlu_elementary_mathematics": {"alias": "  - elementary_mathematics", "acc,none": 0.19576719576719576, "acc_stderr,none": 0.020435730971541805}, "mmlu_high_school_biology": {"alias": "  - high_school_biology", "acc,none": 0.20967741935483872, "acc_stderr,none": 0.023157879349083532}, "mmlu_high_school_chemistry": {"alias": "  - high_school_chemistry", "acc,none": 0.1724137931034483, "acc_stderr,none": 0.02657767218303657}, "mmlu_high_school_computer_science": {"alias": "  - high_school_computer_science", "acc,none": 0.23, "acc_stderr,none": 0.04229525846816506}, "mmlu_high_school_mathematics": {"alias": "  - high_school_mathematics", "acc,none": 0.21851851851851853, "acc_stderr,none": 0.02519575225182379}, "mmlu_high_school_physics": {"alias": "  - high_school_physics", "acc,none": 0.19205298013245034, "acc_stderr,none": 0.03216298420593613}, "mmlu_high_school_statistics": {"alias": "  - high_school_statistics", "acc,none": 0.1527777777777778, "acc_stderr,none": 0.024536326026134227}, "mmlu_machine_learning": {"alias": "  - machine_learning", "acc,none": 0.29464285714285715, "acc_stderr,none": 0.04327040932578729}, "truthfulqa_mc1": {"alias": "truthfulqa_mc1", "acc,none": 0.2215422276621787, "acc_stderr,none": 0.014537867601301145}, "winogrande": {"alias": "winogrande", "acc,none": 0.5090765588003157, "acc_stderr,none": 0.014050170094497707}, "xwinograd": {"acc,none": 0.5059563946954372, "acc_stderr,none": 0.007491934531923876, "alias": "xwinograd"}, "xwinograd_en": {"alias": " - xwinograd_en", "acc,none": 0.5221505376344086, "acc_stderr,none": 0.010361564952394807}, "xwinograd_fr": {"alias": " - xwinograd_fr", "acc,none": 0.5903614457831325, "acc_stderr,none": 0.054306583295391495}, "xwinograd_jp": {"alias": " - xwinograd_jp", "acc,none": 0.48905109489051096, "acc_stderr,none": 0.01615039318009044}, "xwinograd_pt": {"alias": " - xwinograd_pt", "acc,none": 0.49049429657794674, "acc_stderr,none": 0.03088452029581301}, "xwinograd_ru": {"alias": " - xwinograd_ru", "acc,none": 0.4507936507936508, "acc_stderr,none": 0.028079660068225123}, "xwinograd_zh": {"alias": " - xwinograd_zh", "acc,none": 0.49206349206349204, "acc_stderr,none": 0.02229108942329834}}, "groups": {"mmlu": {"acc,none": 0.23529411764705882, "acc_stderr,none": 0.003575402276408048, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.24442082890541977, "acc_stderr,none": 0.006268009266028934, "alias": " - humanities"}, "mmlu_other": {"acc,none": 0.24686192468619247, "acc_stderr,none": 0.0077121117984350835, "alias": " - other"}, "mmlu_social_sciences": {"acc,none": 0.22619434514137146, "acc_stderr,none": 0.007537755791492467, "alias": " - social sciences"}, "mmlu_stem": {"acc,none": 0.21915635902315256, "acc_stderr,none": 0.007361354586830359, "alias": " - stem"}, "xwinograd": {"acc,none": 0.5059563946954372, "acc_stderr,none": 0.007491934531923876, "alias": "xwinograd"}}, "group_subtasks": {"mmlu_humanities": ["mmlu_moral_disputes", "mmlu_world_religions", "mmlu_high_school_european_history", "mmlu_moral_scenarios", "mmlu_international_law", "mmlu_logical_fallacies", "mmlu_high_school_us_history", "mmlu_high_school_world_history", "mmlu_prehistory", "mmlu_jurisprudence", "mmlu_formal_logic", "mmlu_professional_law", "mmlu_philosophy"], "mmlu_social_sciences": ["mmlu_public_relations", "mmlu_us_foreign_policy", "mmlu_high_school_geography", "mmlu_high_school_macroeconomics", "mmlu_security_studies", "mmlu_sociology", "mmlu_econometrics", "mmlu_high_school_government_and_politics", "mmlu_professional_psychology", "mmlu_high_school_psychology", "mmlu_high_school_microeconomics", "mmlu_human_sexuality"], "mmlu_other": ["mmlu_management", "mmlu_miscellaneous", "mmlu_human_aging", "mmlu_clinical_knowledge", "mmlu_marketing", "mmlu_college_medicine", "mmlu_nutrition", "mmlu_professional_medicine", "mmlu_professional_accounting", "mmlu_business_ethics", "mmlu_medical_genetics", "mmlu_virology", "mmlu_global_facts"], "mmlu_stem": ["mmlu_college_chemistry", "mmlu_college_mathematics", "mmlu_machine_learning", "mmlu_elementary_mathematics", "mmlu_anatomy", "mmlu_conceptual_physics", "mmlu_college_biology", "mmlu_abstract_algebra", "mmlu_electrical_engineering", "mmlu_high_school_statistics", "mmlu_high_school_chemistry", "mmlu_computer_security", "mmlu_high_school_biology", "mmlu_college_physics", "mmlu_high_school_physics", "mmlu_astronomy", "mmlu_college_computer_science", "mmlu_high_school_mathematics", "mmlu_high_school_computer_science"], "mmlu": ["mmlu_stem", "mmlu_other", "mmlu_social_sciences", "mmlu_humanities"], "hellaswag": [], "xwinograd": ["xwinograd_en", "xwinograd_fr", "xwinograd_jp", "xwinograd_pt", "xwinograd_ru", "xwinograd_zh"], "winogrande": [], "truthfulqa_mc1": [], "arc_challenge": [], "gsm8k": []}}