{"results": {"arc_challenge": {"alias": "arc_challenge", "acc,none": 0.25597269624573377, "acc_stderr,none": 0.01275301324124453, "acc_norm,none": 0.30204778156996587, "acc_norm_stderr,none": 0.013417519144716417}, "gsm8k": {"alias": "gsm8k", "exact_match,strict-match": 0.037149355572403335, "exact_match_stderr,strict-match": 0.005209516283073774, "exact_match,flexible-extract": 0.050037907505686124, "exact_match_stderr,flexible-extract": 0.006005442354577732}, "hellaswag": {"alias": "hellaswag", "acc,none": 0.4506074487153953, "acc_stderr,none": 0.004965375341643148, "acc_norm,none": 0.5813582951603267, "acc_norm_stderr,none": 0.004923281841828584}, "mmlu": {"acc,none": 0.2446944879646774, "acc_stderr,none": 0.0036251906084613624, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.24909670563230607, "acc_stderr,none": 0.006306887610761067, "alias": " - humanities"}, "mmlu_formal_logic": {"alias": "  - formal_logic", "acc,none": 0.2619047619047619, "acc_stderr,none": 0.03932537680392871}, "mmlu_high_school_european_history": {"alias": "  - high_school_european_history", "acc,none": 0.24848484848484848, "acc_stderr,none": 0.033744026441394036}, "mmlu_high_school_us_history": {"alias": "  - high_school_us_history", "acc,none": 0.23529411764705882, "acc_stderr,none": 0.029771775228145635}, "mmlu_high_school_world_history": {"alias": "  - high_school_world_history", "acc,none": 0.29957805907172996, "acc_stderr,none": 0.029818024749753102}, "mmlu_international_law": {"alias": "  - international_law", "acc,none": 0.2644628099173554, "acc_stderr,none": 0.04026187527591207}, "mmlu_jurisprudence": {"alias": "  - jurisprudence", "acc,none": 0.26851851851851855, "acc_stderr,none": 0.04284467968052191}, "mmlu_logical_fallacies": {"alias": "  - logical_fallacies", "acc,none": 0.2147239263803681, "acc_stderr,none": 0.03226219377286774}, "mmlu_moral_disputes": {"alias": "  - moral_disputes", "acc,none": 0.2774566473988439, "acc_stderr,none": 0.024105712607754307}, "mmlu_moral_scenarios": {"alias": "  - moral_scenarios", "acc,none": 0.24581005586592178, "acc_stderr,none": 0.014400296429225615}, "mmlu_philosophy": {"alias": "  - philosophy", "acc,none": 0.21221864951768488, "acc_stderr,none": 0.023222756797435126}, "mmlu_prehistory": {"alias": "  - prehistory", "acc,none": 0.23148148148148148, "acc_stderr,none": 0.023468429832451163}, "mmlu_professional_law": {"alias": "  - professional_law", "acc,none": 0.25358539765319427, "acc_stderr,none": 0.011111715336101127}, "mmlu_world_religions": {"alias": "  - world_religions", "acc,none": 0.21637426900584794, "acc_stderr,none": 0.03158149539338734}, "mmlu_other": {"acc,none": 0.2578049565497264, "acc_stderr,none": 0.007847531673213759, "alias": " - other"}, "mmlu_business_ethics": {"alias": "  - business_ethics", "acc,none": 0.29, "acc_stderr,none": 0.04560480215720684}, "mmlu_clinical_knowledge": {"alias": "  - clinical_knowledge", "acc,none": 0.2641509433962264, "acc_stderr,none": 0.02713429162874169}, "mmlu_college_medicine": {"alias": "  - college_medicine", "acc,none": 0.23121387283236994, "acc_stderr,none": 0.032147373020294696}, "mmlu_global_facts": {"alias": "  - global_facts", "acc,none": 0.22, "acc_stderr,none": 0.041633319989322695}, "mmlu_human_aging": {"alias": "  - human_aging", "acc,none": 0.27802690582959644, "acc_stderr,none": 0.030069584874494036}, "mmlu_management": {"alias": "  - management", "acc,none": 0.23300970873786409, "acc_stderr,none": 0.04185832598928315}, "mmlu_marketing": {"alias": "  - marketing", "acc,none": 0.3247863247863248, "acc_stderr,none": 0.03067902276549883}, "mmlu_medical_genetics": {"alias": "  - medical_genetics", "acc,none": 0.3, "acc_stderr,none": 0.046056618647183814}, "mmlu_miscellaneous": {"alias": "  - miscellaneous", "acc,none": 0.2515964240102171, "acc_stderr,none": 0.015517322365529631}, "mmlu_nutrition": {"alias": "  - nutrition", "acc,none": 0.2222222222222222, "acc_stderr,none": 0.02380518652488815}, "mmlu_professional_accounting": {"alias": "  - professional_accounting", "acc,none": 0.2695035460992908, "acc_stderr,none": 0.02646903681859063}, "mmlu_professional_medicine": {"alias": "  - professional_medicine", "acc,none": 0.22794117647058823, "acc_stderr,none": 0.025483081468029807}, "mmlu_virology": {"alias": "  - virology", "acc,none": 0.2710843373493976, "acc_stderr,none": 0.034605799075530276}, "mmlu_social_sciences": {"acc,none": 0.23301917452063697, "acc_stderr,none": 0.007612772172838681, "alias": " - social sciences"}, "mmlu_econometrics": {"alias": "  - econometrics", "acc,none": 0.24561403508771928, "acc_stderr,none": 0.0404933929774814}, "mmlu_high_school_geography": {"alias": "  - high_school_geography", "acc,none": 0.1919191919191919, "acc_stderr,none": 0.028057791672989017}, "mmlu_high_school_government_and_politics": {"alias": "  - high_school_government_and_politics", "acc,none": 0.18134715025906736, "acc_stderr,none": 0.02780703236068609}, "mmlu_high_school_macroeconomics": {"alias": "  - high_school_macroeconomics", "acc,none": 0.2128205128205128, "acc_stderr,none": 0.02075242372212801}, "mmlu_high_school_microeconomics": {"alias": "  - high_school_microeconomics", "acc,none": 0.21008403361344538, "acc_stderr,none": 0.026461398717471874}, "mmlu_high_school_psychology": {"alias": "  - high_school_psychology", "acc,none": 0.22568807339449543, "acc_stderr,none": 0.017923087667803057}, "mmlu_human_sexuality": {"alias": "  - human_sexuality", "acc,none": 0.2824427480916031, "acc_stderr,none": 0.03948406125768362}, "mmlu_professional_psychology": {"alias": "  - professional_psychology", "acc,none": 0.26633986928104575, "acc_stderr,none": 0.01788318813466719}, "mmlu_public_relations": {"alias": "  - public_relations", "acc,none": 0.23636363636363636, "acc_stderr,none": 0.04069306319721376}, "mmlu_security_studies": {"alias": "  - security_studies", "acc,none": 0.19591836734693877, "acc_stderr,none": 0.025409301953225678}, "mmlu_sociology": {"alias": "  - sociology", "acc,none": 0.2835820895522388, "acc_stderr,none": 0.03187187537919796}, "mmlu_us_foreign_policy": {"alias": "  - us_foreign_policy", "acc,none": 0.29, "acc_stderr,none": 0.045604802157206845}, "mmlu_stem": {"acc,none": 0.2366000634316524, "acc_stderr,none": 0.007555622141492223, "alias": " - stem"}, "mmlu_abstract_algebra": {"alias": "  - abstract_algebra", "acc,none": 0.2, "acc_stderr,none": 0.04020151261036846}, "mmlu_anatomy": {"alias": "  - anatomy", "acc,none": 0.22962962962962963, "acc_stderr,none": 0.036333844140734636}, "mmlu_astronomy": {"alias": "  - astronomy", "acc,none": 0.2236842105263158, "acc_stderr,none": 0.03391160934343602}, "mmlu_college_biology": {"alias": "  - college_biology", "acc,none": 0.2847222222222222, "acc_stderr,none": 0.037738099906869355}, "mmlu_college_chemistry": {"alias": "  - college_chemistry", "acc,none": 0.21, "acc_stderr,none": 0.040936018074033256}, "mmlu_college_computer_science": {"alias": "  - college_computer_science", "acc,none": 0.23, "acc_stderr,none": 0.04229525846816506}, "mmlu_college_mathematics": {"alias": "  - college_mathematics", "acc,none": 0.28, "acc_stderr,none": 0.045126085985421255}, "mmlu_college_physics": {"alias": "  - college_physics", "acc,none": 0.22549019607843138, "acc_stderr,none": 0.04158307533083286}, "mmlu_computer_security": {"alias": "  - computer_security", "acc,none": 0.32, "acc_stderr,none": 0.04688261722621504}, "mmlu_conceptual_physics": {"alias": "  - conceptual_physics", "acc,none": 0.2936170212765957, "acc_stderr,none": 0.029771642712491227}, "mmlu_electrical_engineering": {"alias": "  - electrical_engineering", "acc,none": 0.2620689655172414, "acc_stderr,none": 0.036646663372252565}, "mmlu_elementary_mathematics": {"alias": "  - elementary_mathematics", "acc,none": 0.24338624338624337, "acc_stderr,none": 0.022101128787415433}, "mmlu_high_school_biology": {"alias": "  - high_school_biology", "acc,none": 0.2032258064516129, "acc_stderr,none": 0.022891687984554963}, "mmlu_high_school_chemistry": {"alias": "  - high_school_chemistry", "acc,none": 0.19704433497536947, "acc_stderr,none": 0.02798672466673622}, "mmlu_high_school_computer_science": {"alias": "  - high_school_computer_science", "acc,none": 0.35, "acc_stderr,none": 0.047937248544110196}, "mmlu_high_school_mathematics": {"alias": "  - high_school_mathematics", "acc,none": 0.2037037037037037, "acc_stderr,none": 0.024556172219141262}, "mmlu_high_school_physics": {"alias": "  - high_school_physics", "acc,none": 0.1986754966887417, "acc_stderr,none": 0.03257847384436776}, "mmlu_high_school_statistics": {"alias": "  - high_school_statistics", "acc,none": 0.18981481481481483, "acc_stderr,none": 0.026744714834691926}, "mmlu_machine_learning": {"alias": "  - machine_learning", "acc,none": 0.26785714285714285, "acc_stderr,none": 0.04203277291467763}, "truthfulqa_mc1": {"alias": "truthfulqa_mc1", "acc,none": 0.22766217870257038, "acc_stderr,none": 0.014679255032111068}, "winogrande": {"alias": "winogrande", "acc,none": 0.5335438042620363, "acc_stderr,none": 0.014020826677598098}, "xwinograd": {"acc,none": 0.5567543268150146, "acc_stderr,none": 0.007423508372052856, "alias": "xwinograd"}, "xwinograd_en": {"alias": " - xwinograd_en", "acc,none": 0.5948387096774194, "acc_stderr,none": 0.01018346376378726}, "xwinograd_fr": {"alias": " - xwinograd_fr", "acc,none": 0.5903614457831325, "acc_stderr,none": 0.05430658329539148}, "xwinograd_jp": {"alias": " - xwinograd_jp", "acc,none": 0.5318039624608968, "acc_stderr,none": 0.016121553797293343}, "xwinograd_pt": {"alias": " - xwinograd_pt", "acc,none": 0.5247148288973384, "acc_stderr,none": 0.030852343325490784}, "xwinograd_ru": {"alias": " - xwinograd_ru", "acc,none": 0.4793650793650794, "acc_stderr,none": 0.02819259287745827}, "xwinograd_zh": {"alias": " - xwinograd_zh", "acc,none": 0.4880952380952381, "acc_stderr,none": 0.022287578075447477}}, "groups": {"mmlu": {"acc,none": 0.2446944879646774, "acc_stderr,none": 0.0036251906084613624, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.24909670563230607, "acc_stderr,none": 0.006306887610761067, "alias": " - humanities"}, "mmlu_other": {"acc,none": 0.2578049565497264, "acc_stderr,none": 0.007847531673213759, "alias": " - other"}, "mmlu_social_sciences": {"acc,none": 0.23301917452063697, "acc_stderr,none": 0.007612772172838681, "alias": " - social sciences"}, "mmlu_stem": {"acc,none": 0.2366000634316524, "acc_stderr,none": 0.007555622141492223, "alias": " - stem"}, "xwinograd": {"acc,none": 0.5567543268150146, "acc_stderr,none": 0.007423508372052856, "alias": "xwinograd"}}, "group_subtasks": {"mmlu_humanities": ["mmlu_moral_disputes", "mmlu_world_religions", "mmlu_high_school_european_history", "mmlu_moral_scenarios", "mmlu_international_law", "mmlu_logical_fallacies", "mmlu_high_school_us_history", "mmlu_high_school_world_history", "mmlu_prehistory", "mmlu_jurisprudence", "mmlu_formal_logic", "mmlu_professional_law", "mmlu_philosophy"], "mmlu_social_sciences": ["mmlu_public_relations", "mmlu_us_foreign_policy", "mmlu_high_school_geography", "mmlu_high_school_macroeconomics", "mmlu_security_studies", "mmlu_sociology", "mmlu_econometrics", "mmlu_high_school_government_and_politics", "mmlu_professional_psychology", "mmlu_high_school_psychology", "mmlu_high_school_microeconomics", "mmlu_human_sexuality"], "mmlu_other": ["mmlu_management", "mmlu_miscellaneous", "mmlu_human_aging", "mmlu_clinical_knowledge", "mmlu_marketing", "mmlu_college_medicine", "mmlu_nutrition", "mmlu_professional_medicine", "mmlu_professional_accounting", "mmlu_business_ethics", "mmlu_medical_genetics", "mmlu_virology", "mmlu_global_facts"], "mmlu_stem": ["mmlu_college_chemistry", "mmlu_college_mathematics", "mmlu_machine_learning", "mmlu_elementary_mathematics", "mmlu_anatomy", "mmlu_conceptual_physics", "mmlu_college_biology", "mmlu_abstract_algebra", "mmlu_electrical_engineering", "mmlu_high_school_statistics", "mmlu_high_school_chemistry", "mmlu_computer_security", "mmlu_high_school_biology", "mmlu_college_physics", "mmlu_high_school_physics", "mmlu_astronomy", "mmlu_college_computer_science", "mmlu_high_school_mathematics", "mmlu_high_school_computer_science"], "mmlu": ["mmlu_stem", "mmlu_other", "mmlu_social_sciences", "mmlu_humanities"], "hellaswag": [], "xwinograd": ["xwinograd_en", "xwinograd_fr", "xwinograd_jp", "xwinograd_pt", "xwinograd_ru", "xwinograd_zh"], "winogrande": [], "truthfulqa_mc1": [], "arc_challenge": [], "gsm8k": []}}