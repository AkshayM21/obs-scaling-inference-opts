{"results": {"arc_challenge": {"alias": "arc_challenge", "acc,none": 0.2773037542662116, "acc_stderr,none": 0.013082095839059374, "acc_norm,none": 0.3003412969283277, "acc_norm_stderr,none": 0.013395909309956999}, "gsm8k": {"alias": "gsm8k", "exact_match,strict-match": 0.052312357846853674, "exact_match_stderr,strict-match": 0.006133057708959228, "exact_match,flexible-extract": 0.0576194086429113, "exact_match_stderr,flexible-extract": 0.006418593319822862}, "hellaswag": {"alias": "hellaswag", "acc,none": 0.42421828321051586, "acc_stderr,none": 0.004932137126625389, "acc_norm,none": 0.5423222465644294, "acc_norm_stderr,none": 0.004971874159777644}, "mmlu": {"acc,none": 0.26684233015239994, "acc_stderr,none": 0.0037293192164413753, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.2724760892667375, "acc_stderr,none": 0.006481127001708271, "alias": " - humanities"}, "mmlu_formal_logic": {"alias": "  - formal_logic", "acc,none": 0.2619047619047619, "acc_stderr,none": 0.03932537680392871}, "mmlu_high_school_european_history": {"alias": "  - high_school_european_history", "acc,none": 0.32727272727272727, "acc_stderr,none": 0.036639749943912434}, "mmlu_high_school_us_history": {"alias": "  - high_school_us_history", "acc,none": 0.3382352941176471, "acc_stderr,none": 0.0332057461294543}, "mmlu_high_school_world_history": {"alias": "  - high_school_world_history", "acc,none": 0.35864978902953587, "acc_stderr,none": 0.03121956944530185}, "mmlu_international_law": {"alias": "  - international_law", "acc,none": 0.24793388429752067, "acc_stderr,none": 0.039418975265163005}, "mmlu_jurisprudence": {"alias": "  - jurisprudence", "acc,none": 0.2222222222222222, "acc_stderr,none": 0.0401910747255735}, "mmlu_logical_fallacies": {"alias": "  - logical_fallacies", "acc,none": 0.27607361963190186, "acc_stderr,none": 0.0351238528370505}, "mmlu_moral_disputes": {"alias": "  - moral_disputes", "acc,none": 0.2745664739884393, "acc_stderr,none": 0.02402774515526501}, "mmlu_moral_scenarios": {"alias": "  - moral_scenarios", "acc,none": 0.2324022346368715, "acc_stderr,none": 0.014125968754673385}, "mmlu_philosophy": {"alias": "  - philosophy", "acc,none": 0.2861736334405145, "acc_stderr,none": 0.02567025924218896}, "mmlu_prehistory": {"alias": "  - prehistory", "acc,none": 0.2839506172839506, "acc_stderr,none": 0.025089478523765127}, "mmlu_professional_law": {"alias": "  - professional_law", "acc,none": 0.26401564537157757, "acc_stderr,none": 0.011258435537723798}, "mmlu_world_religions": {"alias": "  - world_religions", "acc,none": 0.30994152046783624, "acc_stderr,none": 0.03546976959393162}, "mmlu_other": {"acc,none": 0.2780817508850982, "acc_stderr,none": 0.008036824697936335, "alias": " - other"}, "mmlu_business_ethics": {"alias": "  - business_ethics", "acc,none": 0.22, "acc_stderr,none": 0.041633319989322716}, "mmlu_clinical_knowledge": {"alias": "  - clinical_knowledge", "acc,none": 0.24528301886792453, "acc_stderr,none": 0.026480357179895688}, "mmlu_college_medicine": {"alias": "  - college_medicine", "acc,none": 0.24855491329479767, "acc_stderr,none": 0.03295304696818318}, "mmlu_global_facts": {"alias": "  - global_facts", "acc,none": 0.26, "acc_stderr,none": 0.0440844002276808}, "mmlu_human_aging": {"alias": "  - human_aging", "acc,none": 0.2645739910313901, "acc_stderr,none": 0.02960510321703834}, "mmlu_management": {"alias": "  - management", "acc,none": 0.33980582524271846, "acc_stderr,none": 0.04689765937278135}, "mmlu_marketing": {"alias": "  - marketing", "acc,none": 0.3333333333333333, "acc_stderr,none": 0.030882736974138646}, "mmlu_medical_genetics": {"alias": "  - medical_genetics", "acc,none": 0.32, "acc_stderr,none": 0.046882617226215034}, "mmlu_miscellaneous": {"alias": "  - miscellaneous", "acc,none": 0.2822477650063857, "acc_stderr,none": 0.01609530296987854}, "mmlu_nutrition": {"alias": "  - nutrition", "acc,none": 0.2908496732026144, "acc_stderr,none": 0.026004800363952113}, "mmlu_professional_accounting": {"alias": "  - professional_accounting", "acc,none": 0.2695035460992908, "acc_stderr,none": 0.026469036818590624}, "mmlu_professional_medicine": {"alias": "  - professional_medicine", "acc,none": 0.2977941176470588, "acc_stderr,none": 0.027778298701545443}, "mmlu_virology": {"alias": "  - virology", "acc,none": 0.22289156626506024, "acc_stderr,none": 0.03240004825594687}, "mmlu_social_sciences": {"acc,none": 0.2544686382840429, "acc_stderr,none": 0.007858796967178693, "alias": " - social sciences"}, "mmlu_econometrics": {"alias": "  - econometrics", "acc,none": 0.2719298245614035, "acc_stderr,none": 0.04185774424022057}, "mmlu_high_school_geography": {"alias": "  - high_school_geography", "acc,none": 0.2777777777777778, "acc_stderr,none": 0.03191178226713549}, "mmlu_high_school_government_and_politics": {"alias": "  - high_school_government_and_politics", "acc,none": 0.2849740932642487, "acc_stderr,none": 0.03257714077709662}, "mmlu_high_school_macroeconomics": {"alias": "  - high_school_macroeconomics", "acc,none": 0.2358974358974359, "acc_stderr,none": 0.02152596540740873}, "mmlu_high_school_microeconomics": {"alias": "  - high_school_microeconomics", "acc,none": 0.22268907563025211, "acc_stderr,none": 0.027025433498882378}, "mmlu_high_school_psychology": {"alias": "  - high_school_psychology", "acc,none": 0.25504587155963304, "acc_stderr,none": 0.01868850085653584}, "mmlu_human_sexuality": {"alias": "  - human_sexuality", "acc,none": 0.3053435114503817, "acc_stderr,none": 0.040393149787245626}, "mmlu_professional_psychology": {"alias": "  - professional_psychology", "acc,none": 0.2581699346405229, "acc_stderr,none": 0.017704531653250075}, "mmlu_public_relations": {"alias": "  - public_relations", "acc,none": 0.2545454545454545, "acc_stderr,none": 0.04172343038705383}, "mmlu_security_studies": {"alias": "  - security_studies", "acc,none": 0.22857142857142856, "acc_stderr,none": 0.026882144922307748}, "mmlu_sociology": {"alias": "  - sociology", "acc,none": 0.263681592039801, "acc_stderr,none": 0.03115715086935558}, "mmlu_us_foreign_policy": {"alias": "  - us_foreign_policy", "acc,none": 0.23, "acc_stderr,none": 0.042295258468165044}, "mmlu_stem": {"acc,none": 0.2594354582936885, "acc_stderr,none": 0.00779556246138418, "alias": " - stem"}, "mmlu_abstract_algebra": {"alias": "  - abstract_algebra", "acc,none": 0.27, "acc_stderr,none": 0.04461960433384741}, "mmlu_anatomy": {"alias": "  - anatomy", "acc,none": 0.3037037037037037, "acc_stderr,none": 0.03972552884785137}, "mmlu_astronomy": {"alias": "  - astronomy", "acc,none": 0.2236842105263158, "acc_stderr,none": 0.03391160934343603}, "mmlu_college_biology": {"alias": "  - college_biology", "acc,none": 0.2569444444444444, "acc_stderr,none": 0.03653946969442099}, "mmlu_college_chemistry": {"alias": "  - college_chemistry", "acc,none": 0.19, "acc_stderr,none": 0.039427724440366234}, "mmlu_college_computer_science": {"alias": "  - college_computer_science", "acc,none": 0.22, "acc_stderr,none": 0.04163331998932269}, "mmlu_college_mathematics": {"alias": "  - college_mathematics", "acc,none": 0.28, "acc_stderr,none": 0.04512608598542128}, "mmlu_college_physics": {"alias": "  - college_physics", "acc,none": 0.2549019607843137, "acc_stderr,none": 0.043364327079931785}, "mmlu_computer_security": {"alias": "  - computer_security", "acc,none": 0.28, "acc_stderr,none": 0.04512608598542128}, "mmlu_conceptual_physics": {"alias": "  - conceptual_physics", "acc,none": 0.25957446808510637, "acc_stderr,none": 0.02865917937429232}, "mmlu_electrical_engineering": {"alias": "  - electrical_engineering", "acc,none": 0.25517241379310346, "acc_stderr,none": 0.03632984052707842}, "mmlu_elementary_mathematics": {"alias": "  - elementary_mathematics", "acc,none": 0.2671957671957672, "acc_stderr,none": 0.02278967314577657}, "mmlu_high_school_biology": {"alias": "  - high_school_biology", "acc,none": 0.35161290322580646, "acc_stderr,none": 0.027162537826948458}, "mmlu_high_school_chemistry": {"alias": "  - high_school_chemistry", "acc,none": 0.21674876847290642, "acc_stderr,none": 0.02899033125251624}, "mmlu_high_school_computer_science": {"alias": "  - high_school_computer_science", "acc,none": 0.25, "acc_stderr,none": 0.04351941398892446}, "mmlu_high_school_mathematics": {"alias": "  - high_school_mathematics", "acc,none": 0.25925925925925924, "acc_stderr,none": 0.026719240783712163}, "mmlu_high_school_physics": {"alias": "  - high_school_physics", "acc,none": 0.2251655629139073, "acc_stderr,none": 0.03410435282008936}, "mmlu_high_school_statistics": {"alias": "  - high_school_statistics", "acc,none": 0.19907407407407407, "acc_stderr,none": 0.02723229846269025}, "mmlu_machine_learning": {"alias": "  - machine_learning", "acc,none": 0.2857142857142857, "acc_stderr,none": 0.04287858751340456}, "truthfulqa_mc1": {"alias": "truthfulqa_mc1", "acc,none": 0.24969400244798043, "acc_stderr,none": 0.015152286907148128}, "winogrande": {"alias": "winogrande", "acc,none": 0.4964483030781373, "acc_stderr,none": 0.014052131146915873}, "xwinograd": {"acc,none": 0.5075297819734772, "acc_stderr,none": 0.00749738429641096, "alias": "xwinograd"}, "xwinograd_en": {"alias": " - xwinograd_en", "acc,none": 0.5144086021505376, "acc_stderr,none": 0.010367440264363212}, "xwinograd_fr": {"alias": " - xwinograd_fr", "acc,none": 0.5180722891566265, "acc_stderr,none": 0.05517968347010931}, "xwinograd_jp": {"alias": " - xwinograd_jp", "acc,none": 0.5151199165797706, "acc_stderr,none": 0.016146878939958996}, "xwinograd_pt": {"alias": " - xwinograd_pt", "acc,none": 0.4790874524714829, "acc_stderr,none": 0.030863072709687606}, "xwinograd_ru": {"alias": " - xwinograd_ru", "acc,none": 0.5047619047619047, "acc_stderr,none": 0.02821535270426785}, "xwinograd_zh": {"alias": " - xwinograd_zh", "acc,none": 0.47619047619047616, "acc_stderr,none": 0.022268607229556633}}, "groups": {"mmlu": {"acc,none": 0.26684233015239994, "acc_stderr,none": 0.0037293192164413753, "alias": "mmlu"}, "mmlu_humanities": {"acc,none": 0.2724760892667375, "acc_stderr,none": 0.006481127001708271, "alias": " - humanities"}, "mmlu_other": {"acc,none": 0.2780817508850982, "acc_stderr,none": 0.008036824697936335, "alias": " - other"}, "mmlu_social_sciences": {"acc,none": 0.2544686382840429, "acc_stderr,none": 0.007858796967178693, "alias": " - social sciences"}, "mmlu_stem": {"acc,none": 0.2594354582936885, "acc_stderr,none": 0.00779556246138418, "alias": " - stem"}, "xwinograd": {"acc,none": 0.5075297819734772, "acc_stderr,none": 0.00749738429641096, "alias": "xwinograd"}}, "group_subtasks": {"mmlu_humanities": ["mmlu_moral_disputes", "mmlu_world_religions", "mmlu_high_school_european_history", "mmlu_moral_scenarios", "mmlu_international_law", "mmlu_logical_fallacies", "mmlu_high_school_us_history", "mmlu_high_school_world_history", "mmlu_prehistory", "mmlu_jurisprudence", "mmlu_formal_logic", "mmlu_professional_law", "mmlu_philosophy"], "mmlu_social_sciences": ["mmlu_public_relations", "mmlu_us_foreign_policy", "mmlu_high_school_geography", "mmlu_high_school_macroeconomics", "mmlu_security_studies", "mmlu_sociology", "mmlu_econometrics", "mmlu_high_school_government_and_politics", "mmlu_professional_psychology", "mmlu_high_school_psychology", "mmlu_high_school_microeconomics", "mmlu_human_sexuality"], "mmlu_other": ["mmlu_management", "mmlu_miscellaneous", "mmlu_human_aging", "mmlu_clinical_knowledge", "mmlu_marketing", "mmlu_college_medicine", "mmlu_nutrition", "mmlu_professional_medicine", "mmlu_professional_accounting", "mmlu_business_ethics", "mmlu_medical_genetics", "mmlu_virology", "mmlu_global_facts"], "mmlu_stem": ["mmlu_college_chemistry", "mmlu_college_mathematics", "mmlu_machine_learning", "mmlu_elementary_mathematics", "mmlu_anatomy", "mmlu_conceptual_physics", "mmlu_college_biology", "mmlu_abstract_algebra", "mmlu_electrical_engineering", "mmlu_high_school_statistics", "mmlu_high_school_chemistry", "mmlu_computer_security", "mmlu_high_school_biology", "mmlu_college_physics", "mmlu_high_school_physics", "mmlu_astronomy", "mmlu_college_computer_science", "mmlu_high_school_mathematics", "mmlu_high_school_computer_science"], "mmlu": ["mmlu_stem", "mmlu_other", "mmlu_social_sciences", "mmlu_humanities"], "hellaswag": [], "xwinograd": ["xwinograd_en", "xwinograd_fr", "xwinograd_jp", "xwinograd_pt", "xwinograd_ru", "xwinograd_zh"], "winogrande": [], "truthfulqa_mc1": [], "arc_challenge": [], "gsm8k": []}}